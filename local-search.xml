<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>k8s使用nfs做provisioner</title>
    <link href="/posts/k8s%E4%BD%BF%E7%94%A8nfs%E5%81%9Aprovisioner/"/>
    <url>/posts/k8s%E4%BD%BF%E7%94%A8nfs%E5%81%9Aprovisioner/</url>
    
    <content type="html"><![CDATA[<blockquote><p>nfs-client-provisioner 是一个Kubernetes的简易NFS的外部provisioner，本身不提供NFS，需要现有的NFS服务器提供存储<br>这里的 k8s 版本为 v1.16.9</p></blockquote><h1 id="准备k8s集群"><a href="#准备k8s集群" class="headerlink" title="准备k8s集群"></a>准备k8s集群</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@k8s-master nfs-client]# kubectl get nodes<br>NAME         STATUS   ROLES    AGE   VERSION<br>k8s-master   Ready    master   60d   v1.19.4<br>k8s-node01   Ready    &lt;none&gt;   60d   v1.19.4<br>k8s-node02   Ready    &lt;none&gt;   60d   v1.19.4<br>k8s-node03   Ready    &lt;none&gt;   60d   v1.19.4<br>k8s-node04   Ready    &lt;none&gt;   60d   v1.19.4<br><span class="hljs-meta">#</span><span class="bash"> yum install -y nfs-utils  <span class="hljs-comment">#所有k8s节点需要装此包</span></span><br></code></pre></td></tr></table></figure><h1 id="搭建NFS"><a href="#搭建NFS" class="headerlink" title="搭建NFS"></a>搭建NFS</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@k8s-nfs ~]# mkdir /data/nfs -p<br>[root@k8s-nfs ~]# yum install -y nfs-utils<br>[root@k8s-nfs ~]# vim /etc/exports<br>/data/nfs 10.62.169.0/24(rw,sync,fsid=0)<br>[root@k8s-nfs ~]# systemctl enable rpcbind.service<br>[root@k8s-nfs ~]# systemctl enable nfs-server.service<br>[root@k8s-nfs ~]# systemctl start rpcbind.service <br>[root@k8s-nfs ~]# systemctl start nfs-server.service<br>[root@k8s-nfs ~]# systemctl status rpcbind.service <br>[root@k8s-nfs ~]# systemctl status nfs-server.service<br><br><br>[root@k8s-node1 ~]# showmount -e 10.62.169.145<br>Export list for 10.62.169.145:<br>/data/nfs 10.62.169.0/24<br></code></pre></td></tr></table></figure><h1 id="创建nfs-provisioner的namespaces"><a href="#创建nfs-provisioner的namespaces" class="headerlink" title="创建nfs-provisioner的namespaces"></a>创建nfs-provisioner的namespaces</h1><p>创建用于部署nfs-client-provisioner端的namespaces</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@k8s-master nfs-client]# kubectl create ns nfs<br></code></pre></td></tr></table></figure><h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">[root@k8s-master]#</span><span class="bash"> git <span class="hljs-built_in">clone</span> https://github.com/lijiawang/DevOps.git</span><br>[root@k8s-master ~]# cd DevOps/nfs-client/<br>[root@k8s-master nfs-client]# kubectl apply -f rbac.yaml <br>serviceaccount/nfs-client-provisioner created<br>clusterrole.rbac.authorization.k8s.io/nfs-client-provisioner-runner created<br>clusterrolebinding.rbac.authorization.k8s.io/run-nfs-client-provisioner created<br>role.rbac.authorization.k8s.io/leader-locking-nfs-client-provisioner created<br>rolebinding.rbac.authorization.k8s.io/leader-locking-nfs-client-provisioner created<br>[root@k8s-master nfs-client]# cat deployment.yaml <br>---<br>kind: Deployment<br>apiVersion: apps/v1<br>metadata:<br>  name: nfs-client-provisioner<br>  namespace: nfs<br>spec:<br>  replicas: 1<br>  selector:<br>    matchLabels:<br>      app: nfs-client-provisioner<br>  strategy:<br>    type: Recreate<br>  template:<br>    metadata:<br>      labels:<br>        app: nfs-client-provisioner<br>    spec:<br>      serviceAccountName: nfs-client-provisioner<br>      containers:<br>        - name: nfs-client-provisioner<br>          image: lijiawang/nfs-client-provisioner:latest<br>          volumeMounts:<br>            - name: nfs-client-root<br>              mountPath: /persistentvolumes<br>          env:<br>            - name: PROVISIONER_NAME<br>              value: fuseim.pri/ifs<br>            - name: NFS_SERVER<br>              value: 10.62.169.145   # 这里是NFS 服务器的地址<br>            - name: NFS_PATH<br>              value: /data/nfs       # NFS 路径<br>      volumes:<br>        - name: nfs-client-root<br>          nfs:<br>            server: 10.62.169.145<br>            path: /data/nfs<br>[root@k8s-master nfs-client]# kubectl apply -f deployment.yaml <br>deployment.apps/nfs-client-provisioner created<br>[root@k8s-master nfs-client]# kubectl apply -f class.yaml <br>storageclass.storage.k8s.io/managed-nfs-storage created<br></code></pre></td></tr></table></figure><h1 id="验证nfs-client-provisioner是否安装完成"><a href="#验证nfs-client-provisioner是否安装完成" class="headerlink" title="验证nfs-client-provisioner是否安装完成"></a>验证nfs-client-provisioner是否安装完成</h1><p>``<br>[root@k8s-master nfs-client]# kubectl get pod -n nfs<br>NAME                                      READY   STATUS    RESTARTS   AGE<br>nfs-client-provisioner-64b94d544d-9fsnf   1/1     Running   0          100s<br>[root@k8s-master nfs-client]# kubectl get storageclasses<br>NAME                  PROVISIONER      RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE<br>managed-nfs-storage   fuseim.pri/ifs   Delete          Immediate           false                  5h36m</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-comment"># 使用storageclasses</span><br><span class="hljs-string">```shell</span><br>[<span class="hljs-string">root@k8s-master</span> <span class="hljs-string">nfs-client</span>]<span class="hljs-comment"># cat test-pod.yml </span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">PersistentVolumeClaim</span><br><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">test-claim</span><br>  <span class="hljs-attr">annotations:</span><br>    <span class="hljs-attr">volume.beta.kubernetes.io/storage-class:</span> <span class="hljs-string">&quot;managed-nfs-storage&quot;</span><br><span class="hljs-attr">spec:</span><br>  <span class="hljs-attr">accessModes:</span><br>    <span class="hljs-bullet">-</span> <span class="hljs-string">ReadWriteMany</span><br>  <span class="hljs-attr">resources:</span><br>    <span class="hljs-attr">requests:</span><br>      <span class="hljs-attr">storage:</span> <span class="hljs-string">1Mi</span><br><span class="hljs-meta">---</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">Pod</span><br><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">test-pod</span><br><span class="hljs-attr">spec:</span><br>  <span class="hljs-attr">containers:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">test-pod</span><br>    <span class="hljs-attr">image:</span> <span class="hljs-string">lijiawang/busybox:1.24</span><br>    <span class="hljs-attr">command:</span><br>      <span class="hljs-bullet">-</span> <span class="hljs-string">&quot;/bin/sh&quot;</span><br>    <span class="hljs-attr">args:</span><br>      <span class="hljs-bullet">-</span> <span class="hljs-string">&quot;-c&quot;</span><br>      <span class="hljs-bullet">-</span> <span class="hljs-string">&quot;touch /mnt/SUCCESS &amp;&amp; exit 0 || exit 1 &amp;&amp; sleep 100&quot;</span><br>    <span class="hljs-attr">volumeMounts:</span><br>      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">nfs-pvc</span><br>        <span class="hljs-attr">mountPath:</span> <span class="hljs-string">&quot;/mnt&quot;</span><br>  <span class="hljs-attr">restartPolicy:</span> <span class="hljs-string">&quot;Never&quot;</span><br>  <span class="hljs-attr">volumes:</span><br>    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">nfs-pvc</span><br>      <span class="hljs-attr">persistentVolumeClaim:</span><br>        <span class="hljs-attr">claimName:</span> <span class="hljs-string">test-claim</span><br>[<span class="hljs-string">root@k8s-master</span> <span class="hljs-string">nfs-client</span>]<span class="hljs-comment"># kubectl  apply -f test-pod.yml </span><br><span class="hljs-string">persistentvolumeclaim/test-claim</span> <span class="hljs-string">created</span><br><span class="hljs-string">pod/test-pod</span> <span class="hljs-string">created</span><br><br>[<span class="hljs-string">root@k8s-master</span> <span class="hljs-string">nfs-client</span>]<span class="hljs-comment"># kubectl get pvc</span><br><span class="hljs-string">NAME</span>         <span class="hljs-string">STATUS</span>   <span class="hljs-string">VOLUME</span>                                     <span class="hljs-string">CAPACITY</span>   <span class="hljs-string">ACCESS</span> <span class="hljs-string">MODES</span>   <span class="hljs-string">STORAGECLASS</span>          <span class="hljs-string">AGE</span><br><span class="hljs-string">test-claim</span>   <span class="hljs-string">Bound</span>    <span class="hljs-string">pvc-73ea81a1-b34d-4b0c-a374-69ca9078d378</span>   <span class="hljs-string">1Mi</span>        <span class="hljs-string">RWX</span>            <span class="hljs-string">managed-nfs-storage</span>   <span class="hljs-string">17s</span><br></code></pre></td></tr></table></figure>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>cephrgw与openstack keystone集成</title>
    <link href="/posts/cephrgw%E4%B8%8Eopenstack-keystone%E9%9B%86%E6%88%90/"/>
    <url>/posts/cephrgw%E4%B8%8Eopenstack-keystone%E9%9B%86%E6%88%90/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    <categories>
      
      <category>OpenStack</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ceph</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>使用ceph提供对象存储</title>
    <link href="/posts/%E4%BD%BF%E7%94%A8ceph%E6%8F%90%E4%BE%9B%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8/"/>
    <url>/posts/%E4%BD%BF%E7%94%A8ceph%E6%8F%90%E4%BE%9B%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8/</url>
    
    <content type="html"><![CDATA[<h1 id="查看ceph集群状态检查rgw服务是否正常"><a href="#查看ceph集群状态检查rgw服务是否正常" class="headerlink" title="查看ceph集群状态检查rgw服务是否正常"></a>查看ceph集群状态检查rgw服务是否正常</h1><p>首先先提供一个装有ceph rgw服务的ceph集群，安装方式我就在这里不演示了，详细请见<a href="https://www.lijiawang.org/posts/ceph-deploy-%E5%AE%89%E8%A3%85ceph/">ceph-deploy安装ceph集群</a>这篇文章</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@ceph--1 ~]# ceph -s<br>  cluster:<br>    id:     faa2e2c4-98bc-47c4-a5b4-a478721b7ea2<br>    health: HEALTH_OK<br><br>  services:<br>    mon: 3 daemons, quorum ceph--1,ceph--2,ceph--3 (age 47h)<br>    mgr: ceph--1(active, since 4d), standbys: ceph--2, ceph--3<br>    mds: cephfs:1 &#123;0=ceph--2=up:active&#125; 2 up:standby<br>    osd: 3 osds: 3 up (since 4d), 3 in (since 4d)<br>    rgw: 3 daemons active (ceph--1, ceph--2, ceph--3)<br><br>  task status:<br>    scrub status:<br>        mds.ceph--2: idle<br><br>  data:<br>    pools:   7 pools, 208 pgs<br>    objects: 315 objects, 101 MiB<br>    usage:   3.3 GiB used, 147 GiB / 150 GiB avail<br>    pgs:     208 active+clean<br><br>  io:<br>    client:   4.0 KiB/s rd, 0 B/s wr, 3 op/s rd, 2 op/s wr<br><span class="hljs-meta">#</span><span class="bash"> radosgw端口默认为7480</span><br>[root@ceph--1 ~]# netstat -antupl |grep 7480<br>tcp        0      0 0.0.0.0:7480            0.0.0.0:*               LISTEN      15194/radosgw<br>tcp6       0      0 :::7480                 :::*                    LISTEN      15194/radosgw<br></code></pre></td></tr></table></figure><h1 id="修改rgw的默认端口"><a href="#修改rgw的默认端口" class="headerlink" title="修改rgw的默认端口"></a>修改rgw的默认端口</h1><p>ceph支持修改rgw的默认端口，下面我将rgw默认端口7480 修改为80端口</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@ceph--1 ceph-deploy]# cat ceph.conf <br>[client.rgw.ceph--1]<br>host = ceph--1<br>rgw frontends = &quot;civetweb port=80&quot;<br>[client.rgw.ceph--2]<br>host = ceph--2<br>rgw frontends = &quot;civetweb port=80&quot;<br>[client.rgw.ceph--3]<br>host = ceph--3<br>rgw frontends = &quot;civetweb port=80&quot;<br><br>[root@ceph--1 ceph-deploy]# ceph-deploy  --overwrite-conf  config push ceph--1 ceph--2 ceph--3<br><span class="hljs-meta">#</span><span class="bash"> systemctl start ceph-radosgw@rgw.`hostname -s` <span class="hljs-comment"># 重启所有主机上的ceph rgw服务</span></span><br></code></pre></td></tr></table></figure><h1 id="验证是否修改成功"><a href="#验证是否修改成功" class="headerlink" title="验证是否修改成功"></a>验证是否修改成功</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@ceph--1 ceph-deploy]# netstat -antupl |grep radosgw<br>tcp        0      0 0.0.0.0:80              0.0.0.0:*               LISTEN      35167/radosgw<br>tcp        0      0 10.140.11.8:33980       10.140.11.24:6800       ESTABLISHED 35167/radosgw<br>tcp        0      0 10.140.11.8:36764       10.140.11.8:6802        ESTABLISHED 35167/radosgw<br>tcp        0      0 10.140.11.8:44946       10.140.11.8:6800        ESTABLISHED 35167/radosgw<br>tcp        0      0 10.140.11.8:51116       10.140.11.8:3300        ESTABLISHED 35167/radosgw<br>tcp        0      0 10.140.11.8:42700       10.140.11.6:6800        ESTABLISHED 35167/radosgw<br>tcp        0      0 10.140.11.8:44926       10.140.11.8:6800        ESTABLISHED 35167/radosgw<br>tcp        0      0 10.140.11.8:33960       10.140.11.24:6800       ESTABLISHED 35167/radosgw<br>tcp        0      0 10.140.11.8:42678       10.140.11.6:6800        ESTABLISHED 35167/radosgw<br>tcp        0      0 10.140.11.8:51134       10.140.11.8:3300        ESTABLISHED 35167/radosgw<br>tcp        0      0 10.140.11.8:36782       10.140.11.8:6802        ESTABLISHED 35167/radosgw<br>[root@ceph--1 ceph-deploy]# curl http://ceph--1<br>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;ListAllMyBucketsResult xmlns=&quot;http://s3.amazonaws.com/doc/2006-03-01/&quot;&gt;&lt;Owner&gt;&lt;ID&gt;anonymous&lt;/ID&gt;&lt;DisplayName&gt;&lt;/DisplayName&gt;&lt;/Owner&gt;&lt;Buckets&gt;&lt;/Buckets&gt;&lt;/ListAllMyBucketsResult&gt;<br></code></pre></td></tr></table></figure><h1 id="使用S3访问CEPH-RGW"><a href="#使用S3访问CEPH-RGW" class="headerlink" title="使用S3访问CEPH RGW"></a>使用S3访问CEPH RGW</h1><h2 id="为S3的访问创建账号"><a href="#为S3的访问创建账号" class="headerlink" title="为S3的访问创建账号"></a>为S3的访问创建账号</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@ceph--1 ~]# radosgw-admin  user create --uid ceph-s3-user --display-name &quot;Ceph S3 User Dome&quot;<br>&#123;<br>    &quot;user_id&quot;: &quot;ceph-s3-user&quot;,<br>    &quot;display_name&quot;: &quot;Ceph S3 User Dome&quot;,<br>    &quot;email&quot;: &quot;&quot;,<br>    &quot;suspended&quot;: 0,<br>    &quot;max_buckets&quot;: 1000,<br>    &quot;subusers&quot;: [],<br>    &quot;keys&quot;: [<br>        &#123;<br>            &quot;user&quot;: &quot;ceph-s3-user&quot;,<br>            &quot;access_key&quot;: &quot;9HR6Y9PAGBYFKRW5U6XM&quot;,   <br>            &quot;secret_key&quot;: &quot;51dSEB2WqZATowS71GPJPo8CoGG2VQZM63ndGfKl&quot;<br>        &#125;<br>    ],<br>    &quot;swift_keys&quot;: [],<br>    &quot;caps&quot;: [],<br>    &quot;op_mask&quot;: &quot;read, write, delete&quot;,<br>    &quot;default_placement&quot;: &quot;&quot;,<br>    &quot;default_storage_class&quot;: &quot;&quot;,<br>    &quot;placement_tags&quot;: [],<br>    &quot;bucket_quota&quot;: &#123;<br>        &quot;enabled&quot;: false,<br>        &quot;check_on_raw&quot;: false,<br>        &quot;max_size&quot;: -1,<br>        &quot;max_size_kb&quot;: 0,<br>        &quot;max_objects&quot;: -1<br>    &#125;,<br>    &quot;user_quota&quot;: &#123;<br>        &quot;enabled&quot;: false,<br>        &quot;check_on_raw&quot;: false,<br>        &quot;max_size&quot;: -1,<br>        &quot;max_size_kb&quot;: 0,<br>        &quot;max_objects&quot;: -1<br>    &#125;,<br>    &quot;temp_url_keys&quot;: [],<br>    &quot;type&quot;: &quot;rgw&quot;,<br>    &quot;mfa_ids&quot;: []<br>&#125;<br><br>[root@ceph--1 ~]# radosgw-admin  user list<br>[<br>    &quot;ceph-s3-user&quot;<br>]<br><br>[root@ceph--1 ~]# radosgw-admin  user info --uid ceph-s3-user<br><br><br>&#123;<br>    &quot;user_id&quot;: &quot;ceph-s3-user&quot;,<br>    &quot;display_name&quot;: &quot;Ceph S3 User Dome&quot;,<br>    &quot;email&quot;: &quot;&quot;,<br>    &quot;suspended&quot;: 0,<br>    &quot;max_buckets&quot;: 1000,<br>    &quot;subusers&quot;: [],<br>    &quot;keys&quot;: [<br>        &#123;<br>            &quot;user&quot;: &quot;ceph-s3-user&quot;,<br>            &quot;access_key&quot;: &quot;9HR6Y9PAGBYFKRW5U6XM&quot;,<br>            &quot;secret_key&quot;: &quot;51dSEB2WqZATowS71GPJPo8CoGG2VQZM63ndGfKl&quot;<br>        &#125;<br>    ],<br>    &quot;swift_keys&quot;: [],<br>    &quot;caps&quot;: [],<br>    &quot;op_mask&quot;: &quot;read, write, delete&quot;,<br>    &quot;default_placement&quot;: &quot;&quot;,<br>    &quot;default_storage_class&quot;: &quot;&quot;,<br>    &quot;placement_tags&quot;: [],<br>    &quot;bucket_quota&quot;: &#123;<br>        &quot;enabled&quot;: false,<br>        &quot;check_on_raw&quot;: false,<br>        &quot;max_size&quot;: -1,<br>        &quot;max_size_kb&quot;: 0,<br>        &quot;max_objects&quot;: -1<br>    &#125;,<br>    &quot;user_quota&quot;: &#123;<br>        &quot;enabled&quot;: false,<br>        &quot;check_on_raw&quot;: false,<br>        &quot;max_size&quot;: -1,<br>        &quot;max_size_kb&quot;: 0,<br>        &quot;max_objects&quot;: -1<br>    &#125;,<br>    &quot;temp_url_keys&quot;: [],<br>    &quot;type&quot;: &quot;rgw&quot;,<br>    &quot;mfa_ids&quot;: []<br>&#125;<br>将access_key和secret_key记录下来<br><br></code></pre></td></tr></table></figure><h2 id="安装配置S3"><a href="#安装配置S3" class="headerlink" title="安装配置S3"></a>安装配置S3</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@ceph--1 ~]# yum install s3cmd -y<br>[root@ceph--1 ~]# s3cmd --configure<br><br>Enter new values or accept defaults in brackets with Enter.<br>Refer to user manual for detailed description of all options.<br><br>Access key and Secret key are your identifiers for Amazon S3. Leave them empty for using the env variables.<br>Access Key: 9HR6Y9PAGBYFKRW5U6XM<br>Secret Key: 51dSEB2WqZATowS71GPJPo8CoGG2VQZM63ndGfKl<br>Default Region [US]: <br><br>Use &quot;s3.amazonaws.com&quot; for S3 Endpoint and not modify it to the target Amazon S3.<br>S3 Endpoint [s3.amazonaws.com]: 10.140.11.8:80<br><br>Use &quot;%(bucket)s.s3.amazonaws.com&quot; to the target Amazon S3. &quot;%(bucket)s&quot; and &quot;%(location)s&quot; vars can be used<br>if the target S3 system supports dns based buckets.<br>DNS-style bucket+hostname:port template for accessing a bucket [%(bucket)s.s3.amazonaws.com]: 10.140.11.8:80/%(bucket)s<br><br>Encryption password is used to protect your files from reading<br>by unauthorized persons while in transfer to S3<br>Encryption password:<br>Path to GPG program [/usr/bin/gpg]: <br><br>When using secure HTTPS protocol all communication with Amazon S3<br>servers is protected from 3rd party eavesdropping. This method is<br>slower than plain HTTP, and can only be proxied with Python 2.7 or newer<br>Use HTTPS protocol [Yes]: no<br><br>On some networks all internet access must go through a HTTP proxy.<br>Try setting it here if you can&#x27;t connect to S3 directly<br>HTTP Proxy server name:<br><br>New settings:<br>  Access Key: 9HR6Y9PAGBYFKRW5U6XM<br>  Secret Key: 51dSEB2WqZATowS71GPJPo8CoGG2VQZM63ndGfKl<br>  Default Region: US<br>  S3 Endpoint: 10.140.11.8:80<br>  DNS-style bucket+hostname:port template for accessing a bucket: 10.140.11.8:80/%(bucket)s<br>  Encryption password:<br>  Path to GPG program: /usr/bin/gpg<br>  Use HTTPS protocol: False<br>  HTTP Proxy server name:<br>  HTTP Proxy server port: 0<br><br>Test access with supplied credentials? [Y/n] y<br>Please wait, attempting to list all buckets...<br>Success. Your access key and secret key worked fine :-)<br><br>Now verifying that encryption works...<br>Not configured. Never mind.<br><br>Save settings? [y/N] y<br>Configuration saved to &#x27;/root/.s3cfg&#x27;<br><span class="hljs-meta">#</span><span class="bash"> 在这个交互配置过程中，只配置了其中access_key和secret_key.</span><br></code></pre></td></tr></table></figure><h2 id="使用S3操作ceph-RGW"><a href="#使用S3操作ceph-RGW" class="headerlink" title="使用S3操作ceph RGW"></a>使用S3操作ceph RGW</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@ceph--1 ~]# s3cmd mb s3://s3cmd-demo   # 创建bucket<br>Bucket &#x27;s3://s3cmd-demo/&#x27; created<br>[root@ceph--1 ~]# s3cmd ls<br>2021-01-10 08:00  s3://s3cmd-demo<br>[root@ceph--1 ~]# s3cmd put /etc/fstab s3://s3cmd-demo/fstab-demo   # 上传文件<br>upload: &#x27;/etc/fstab&#x27; -&gt; &#x27;s3://s3cmd-demo/fstab-demo&#x27;  [1 of 1]<br> 42 of 42   100% in    2s    20.23 B/s  done<br>[root@ceph--1 ~]# s3cmd ls s3://s3cmd-demo<br>2021-01-10 08:09           42  s3://s3cmd-demo/fstab-demo<br>[root@ceph--1 ~]# s3cmd get s3://s3cmd-demo/fstab-demo lijiawang123 下载文件<br>download: &#x27;s3://s3cmd-demo/fstab-demo&#x27; -&gt; &#x27;lijiawang123&#x27;  [1 of 1]<br> 42 of 42   100% in    0s     2.83 KB/s  done<br>[root@ceph--1 ~]# cat lijiawang123   #验证文件<br>LABEL=cloudimg-rootfs / ext4 defaults 0 1<br>[root@ceph--1 ~]# cat /etc/fstab <br>LABEL=cloudimg-rootfs / ext4 defaults 0 1<br></code></pre></td></tr></table></figure><h1 id="使用swift访问CEPH-RGW"><a href="#使用swift访问CEPH-RGW" class="headerlink" title="使用swift访问CEPH RGW"></a>使用swift访问CEPH RGW</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> 在已创建S3用户数添加swift_keys</span><br>[root@ceph--1 ~]# radosgw-admin subuser create --uid ceph-s3-user --subuser=ceph-s3-user:swift  --access=full<br>&#123;<br>    &quot;user_id&quot;: &quot;ceph-s3-user&quot;,<br>    &quot;display_name&quot;: &quot;Ceph S3 User Dome&quot;,<br>    &quot;email&quot;: &quot;&quot;,<br>    &quot;suspended&quot;: 0,<br>    &quot;max_buckets&quot;: 1000,<br>    &quot;subusers&quot;: [<br>        &#123;<br>            &quot;id&quot;: &quot;ceph-s3-user:swift&quot;,<br>            &quot;permissions&quot;: &quot;full-control&quot;<br>        &#125;<br>    ],<br>    &quot;keys&quot;: [<br>        &#123;<br>            &quot;user&quot;: &quot;ceph-s3-user&quot;,<br>            &quot;access_key&quot;: &quot;9HR6Y9PAGBYFKRW5U6XM&quot;,<br>            &quot;secret_key&quot;: &quot;51dSEB2WqZATowS71GPJPo8CoGG2VQZM63ndGfKl&quot;<br>        &#125;<br>    ],<br>    &quot;swift_keys&quot;: [<br>        &#123;<br>            &quot;user&quot;: &quot;ceph-s3-user:swift&quot;,<br>            &quot;secret_key&quot;: &quot;wLVnoP1hg6M1wMz4H6oBiBMUhFRSx5IgYvtCh6Ed&quot;<br>        &#125;<br>    ],<br>    &quot;caps&quot;: [],<br>    &quot;op_mask&quot;: &quot;read, write, delete&quot;,<br>    &quot;default_placement&quot;: &quot;&quot;,<br>    &quot;default_storage_class&quot;: &quot;&quot;,<br>    &quot;placement_tags&quot;: [],<br>    &quot;bucket_quota&quot;: &#123;<br>        &quot;enabled&quot;: false,<br>        &quot;check_on_raw&quot;: false,<br>        &quot;max_size&quot;: -1,<br>        &quot;max_size_kb&quot;: 0,<br>        &quot;max_objects&quot;: -1<br>    &#125;,<br>    &quot;user_quota&quot;: &#123;<br>        &quot;enabled&quot;: false,<br>        &quot;check_on_raw&quot;: false,<br>        &quot;max_size&quot;: -1,<br>        &quot;max_size_kb&quot;: 0,<br>        &quot;max_objects&quot;: -1<br>    &#125;,<br>    &quot;temp_url_keys&quot;: [],<br>    &quot;type&quot;: &quot;rgw&quot;,<br>    &quot;mfa_ids&quot;: []<br>&#125;<br><span class="hljs-meta">#</span><span class="bash"> 验证</span><br>[root@ceph--1 ~]# swift -V 1 -A http://10.140.11.8:80/auth -U ceph-s3-user:swift -K wLVnoP1hg6M1wMz4H6oBiBMUhFRSx5IgYvtCh6Ed list<br>s3cmd-demo<br><span class="hljs-meta">#</span><span class="bash"> 设置环境</span><br>[root@ceph--1 ~]# cat swift.rc <br>export ST_AUTH=http://10.140.11.8:80/auth <br>export ST_USER=ceph-s3-user:swift<br>export ST_KEY=wLVnoP1hg6M1wMz4H6oBiBMUhFRSx5IgYvtCh6Ed<br><br>[root@ceph--1 ~]# . swift.rc<br><br>[root@ceph--1 ~]# swift  list<br>s3cmd-demo<br>[root@ceph--1 ~]# cat swift.rc <br>export ST_AUTH=http://10.140.11.8:80/auth <br>export ST_USER=ceph-s3-user:swift<br>export ST_KEY=wLVnoP1hg6M1wMz4H6oBiBMUhFRSx5IgYvtCh6Ed<br><br>[root@ceph--1 ~]# . swift.rc <br>[root@ceph--1 ~]# swift  list<br>s3cmd-demo<br>[root@ceph--1 ~]# swift post swift-demo   # 创建bucket<br>[root@ceph--1 ~]# swift  list<br>s3cmd-demo<br>swift-demo<br>[root@ceph--1 ~]# swift upload  swift-demo /etc/passwd  # 上传文件<br>etc/passwd<br>[root@ceph--1 ~]# swift  list swift-demo<br>etc/passwd<br>[root@ceph--1 ~]# swift download swift-demo etc/passwd  # 下载文件<br>etc/passwd [auth 0.014s, headers 0.017s, total 0.017s, 0.460 MB/s]<br>[root@ceph--1 ~]# ls etc/passwd <br>etc/passwd<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>ceph</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ceph</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>使用ceph提供块存储</title>
    <link href="/posts/%E4%BD%BF%E7%94%A8ceph%E6%8F%90%E4%BE%9B%E5%9D%97%E5%AD%98%E5%82%A8/"/>
    <url>/posts/%E4%BD%BF%E7%94%A8ceph%E6%8F%90%E4%BE%9B%E5%9D%97%E5%AD%98%E5%82%A8/</url>
    
    <content type="html"><![CDATA[<h1 id="ceph的块存储"><a href="#ceph的块存储" class="headerlink" title="ceph的块存储"></a><code>ceph</code>的块存储</h1><h1 id="创建一个POOL"><a href="#创建一个POOL" class="headerlink" title="创建一个POOL"></a>创建一个POOL</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@ceph--1 ~]# ceph osd pool create ceph-demo 16 16<br>pool &#x27;ceph-demo&#x27; created<br>[root@ceph--1 ~]# rbd -p ceph-demo ls<br></code></pre></td></tr></table></figure><h1 id="创建rbd"><a href="#创建rbd" class="headerlink" title="创建rbd"></a>创建rbd</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@ceph--1 ~]# rbd create -p ceph-demo --image rbd-demo.img --size 1G<br>[root@ceph--1 ~]# rbd -p ceph-demo ls<br>rbd-demo.img<br>[root@ceph--1 ~]# rbd create ceph-demo/rbd-demo-1.img --size 1G   # 也可以这么创建<br>[root@ceph--1 ~]# rbd -p ceph-demo ls<br>rbd-demo-1.img<br>rbd-demo.img<br></code></pre></td></tr></table></figure><h1 id="将RBD映射到主机"><a href="#将RBD映射到主机" class="headerlink" title="将RBD映射到主机"></a>将RBD映射到主机</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@ceph--1 ~]# rbd -p ceph-demo ls<br>rbd-demo-1.img<br>rbd-demo.img<br>[root@ceph--1 ~]# rbd info ceph-demo/rbd-demo.img<br>rbd image &#x27;rbd-demo.img&#x27;:<br>        size 1 GiB in 256 objects<br>        order 22 (4 MiB objects)<br>        snapshot_count: 0<br>        id: 1686fe84952f<br>        block_name_prefix: rbd_data.1686fe84952f<br>        format: 2<br>        features: layering, exclusive-lock, object-map, fast-diff, deep-flatten<br>        op_features:<br>        flags:<br>        create_timestamp: Thu Jan  7 15:06:15 2021<br>        access_timestamp: Thu Jan  7 15:06:15 2021<br>        modify_timestamp: Thu Jan  7 15:06:15 2021<br>[root@ceph--1 ~]# rbd map  ceph-demo/rbd-demo.img<br>rbd: sysfs write failed<br>RBD image feature set mismatch. You can disable features unsupported by the kernel with &quot;rbd feature disable ceph-demo/rbd-demo.img object-map fast-diff deep-flatten&quot;.<br>In some cases useful info is found in syslog - try &quot;dmesg | tail&quot;.<br>rbd: map failed: (6) No such device or address<br>[root@ceph--1 ~]# rbd -p ceph-demo ls<br>rbd-demo-1.img<br>rbd-demo.img<br>[root@ceph--1 ~]# rbd feature disable ceph-demo/rbd-demo.img deep-flatten<br>[root@ceph--1 ~]# rbd feature disable ceph-demo/rbd-demo.img fast-diff<br>[root@ceph--1 ~]# rbd feature disable ceph-demo/rbd-demo.img object-map<br>[root@ceph--1 ~]# rbd feature disable ceph-demo/rbd-demo.img exclusive-lock<br>[root@ceph--1 ~]# rbd info ceph-demo/rbd-demo.img<br>rbd image &#x27;rbd-demo.img&#x27;:<br>        size 1 GiB in 256 objects<br>        order 22 (4 MiB objects)<br>        snapshot_count: 0<br>        id: 16e0aec0c2a8<br>        block_name_prefix: rbd_data.16e0aec0c2a8<br>        format: 2<br>        features: layering<br>        op_features:<br>        flags:<br>        create_timestamp: Thu Jan  7 15:18:34 2021<br>        access_timestamp: Thu Jan  7 15:18:34 2021<br>        modify_timestamp: Thu Jan  7 15:18:34 2021<br><br>[root@ceph--1 ~]# rbd map  ceph-demo/rbd-demo.img<br>/dev/rbd0<br>[root@ceph--1 ~]# rbd device list<br>id pool      namespace image        snap device    <br>0  ceph-demo           rbd-demo.img -    /dev/rbd0<br>[root@ceph--1 ~]# fdisk -l /dev/rbd0<br><br>Disk /dev/rbd0: 1073 MB, 1073741824 bytes, 2097152 sectors<br>Units = sectors of 1 * 512 = 512 bytes<br>Sector size (logical/physical): 512 bytes / 512 bytes<br>I/O size (minimum/optimal): 4194304 bytes / 4194304 bytes<br></code></pre></td></tr></table></figure><h1 id="挂载该设备"><a href="#挂载该设备" class="headerlink" title="挂载该设备"></a>挂载该设备</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@ceph--1 ~]# mkdir /mnt/ceph-demo<br>[root@ceph--1 ~]# mkfs.ext4 /dev/rbd0<br>mke2fs 1.42.9 (28-Dec-2013)<br>Discarding device blocks: done<br>Filesystem label=<br>OS type: Linux<br>Block size=4096 (log=2)<br>Fragment size=4096 (log=2)<br>Stride=1024 blocks, Stripe width=1024 blocks<br>65536 inodes, 262144 blocks<br>13107 blocks (5.00%) reserved for the super user<br>First data block=0<br>Maximum filesystem blocks=268435456<br>8 block groups<br>32768 blocks per group, 32768 fragments per group<br>8192 inodes per group<br>Superblock backups stored on blocks:<br>        32768, 98304, 163840, 229376<br><br>Allocating group tables: done<br>Writing inode tables: done<br>Creating journal (8192 blocks): done<br>Writing superblocks and filesystem accounting information: done<br>[root@ceph--1 ~]# df -Th /dev/rbd0<br>Filesystem     Type  Size  Used Avail Use% Mounted on<br>/dev/rbd0      ext4  976M  2.6M  907M   1% /mnt/ceph-demo<br><br>[root@ceph--1 ~]# cd /mnt/ceph-demo/<br>[root@ceph--1 ceph-demo]# ls -l<br>total 16<br>drwx------ 2 root root 16384 Jan  7 15:48 lost+found<br>[root@ceph--1 ceph-demo]# cp /etc/passwd .<br>[root@ceph--1 ceph-demo]# ls<br>lost+found  passwd<br></code></pre></td></tr></table></figure><h1 id="RBD扩容"><a href="#RBD扩容" class="headerlink" title="RBD扩容"></a>RBD扩容</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@ceph--1 ~]# rbd info ceph-demo/rbd-demo.img<br>rbd image &#x27;rbd-demo.img&#x27;:<br>        size 1 GiB in 256 objects<br>        order 22 (4 MiB objects)<br>        snapshot_count: 0<br>        id: 16e0aec0c2a8<br>        block_name_prefix: rbd_data.16e0aec0c2a8<br>        format: 2<br>        features: layering<br>        op_features:<br>        flags:<br>        create_timestamp: Thu Jan  7 15:18:34 2021<br>        access_timestamp: Thu Jan  7 15:18:34 2021<br>        modify_timestamp: Thu Jan  7 15:18:34 2021<br>[root@ceph--1 ~]# rbd resize ceph-demo/rbd-demo.img -s 5G  # 扩容到5G<br>Resizing image: 100% complete...done.<br>[root@ceph--1 ~]# rbd info ceph-demo/rbd-demo.img<br>rbd image &#x27;rbd-demo.img&#x27;:<br>        size 5 GiB in 1280 objects<br>        order 22 (4 MiB objects)<br>        snapshot_count: 0<br>        id: 16e0aec0c2a8<br>        block_name_prefix: rbd_data.16e0aec0c2a8<br>        format: 2<br>        features: layering<br>        op_features:<br>        flags:<br>        create_timestamp: Thu Jan  7 15:18:34 2021<br>        access_timestamp: Thu Jan  7 15:18:34 2021<br>        modify_timestamp: Thu Jan  7 15:18:34 2021<br>[root@ceph--1 ~]# df -Th /mnt/ceph-demo    # 扩容挂载点<br>Filesystem     Type  Size  Used Avail Use% Mounted on<br>/dev/rbd0      ext4  976M  2.6M  907M   1% /mnt/ceph-demo<br>[root@ceph--1 ~]# resize2fs  /dev/rbd0<br>resize2fs 1.42.9 (28-Dec-2013)<br>Filesystem at /dev/rbd0 is mounted on /mnt/ceph-demo; on-line resizing required<br>old_desc_blocks = 1, new_desc_blocks = 1<br>The filesystem on /dev/rbd0 is now 1310720 blocks long.<br>[root@ceph--1 ~]# cd /mnt/ceph-demo/<br>[root@ceph--1 ceph-demo]# ls<br>lost+found  passwd<br>[root@ceph--1 ceph-demo]# cat passwd <br><br>[root@ceph--1 ~]# df -Th /mnt/ceph-demo<br>Filesystem     Type  Size  Used Avail Use% Mounted on<br>/dev/rbd0      ext4  4.9G  4.0M  4.7G   1% /mnt/ceph-demo<br></code></pre></td></tr></table></figure><h1 id="RBD数据迁移"><a href="#RBD数据迁移" class="headerlink" title="RBD数据迁移"></a>RBD数据迁移</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs shell">模拟ceph--1主机挂机或吊盘<br>[root@ceph--1 ceph-demo]# umount /mnt/ceph-demo/   # 卸载挂载分区<br>umount: /mnt/ceph-demo: target is busy.<br>        (In some cases useful info about processes that use<br>         the device is found by lsof(8) or fuser(1))<br>[root@ceph--1 ceph-demo]# fuser -mv -k /mnt/ceph-demo/  # 如果报以上错误请用该命令卸载<br>                     USER        PID ACCESS COMMAND<br>/mnt/ceph-demo:      root     kernel mount /mnt/ceph-demo<br>                     root      25679 ..c.. bash<br>[root@ceph--1 ~]# umount /mnt/ceph-demo/<br>[root@ceph--1 ~]# df -T<br>Filesystem         Type           1K-blocks    Used Available Use% Mounted on<br>devtmpfs           devtmpfs         8189812       0   8189812   0% /dev<br>tmpfs              tmpfs            8215516       0   8215516   0% /dev/shm<br>tmpfs              tmpfs            8215516   16956   8198560   1% /run<br>tmpfs              tmpfs            8215516       0   8215516   0% /sys/fs/cgroup<br>/dev/vda1          ext4            98232872 3026300  90975308   4% /<br>tmpfs              tmpfs            8215516      52   8215464   1% /var/lib/ceph/osd/ceph-0<br>tmpfs              tmpfs            1643104       0   1643104   0% /run/user/1000<br>10.140.11.8:6789:/ ceph            48599040       0  48599040   0% /mnt/cephfs<br>ceph-fuse          fuse.ceph-fuse  48599040       0  48599040   0% /mnt/ceph-fuse<br>tmpfs              tmpfs            1643104       0   1643104   0% /run/user/0<br>可以看到ceph-demo已经umount 掉了<br><span class="hljs-meta">#</span><span class="bash"> 使用ceph--2主机挂载该设备</span><br>[root@ceph--1 ceph-demo]# ssh ceph--2<br>[root@ceph--2 ~]# rbd device list<br>[root@ceph--2 ~]# rbd -p ceph-demo ls<br>rbd-demo.img<br>[root@ceph--2 ~]# rbd map ceph-demo/rbd-demo.img<br>/dev/rbd0<br>[root@ceph--2 ~]# mkdir ljw<br>[root@ceph--2 ~]# mount /dev/rbd0 /root/ljw/<br>[root@ceph--2 ljw]# df -Th /root/ljw<br>Filesystem     Type  Size  Used Avail Use% Mounted on<br>/dev/rbd0      ext4  4.9G  4.0M  4.7G   1% /root/ljw<br></code></pre></td></tr></table></figure><h1 id="数据验证"><a href="#数据验证" class="headerlink" title="数据验证"></a>数据验证</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@ceph--2 ~]# cd /root/ljw/<br>[root@ceph--2 ljw]# ls<br>lost+found  passwd<br>[root@ceph--2 ljw]# cat passwd<br>[root@ceph--2 ljw]# cp /etc/fstab .<br>[root@ceph--2 ljw]# ls <br>fstab  lost+found  passwd<br><br></code></pre></td></tr></table></figure><h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><p><a href="https://blog.csdn.net/wdz306ling/article/details/87930133">https://blog.csdn.net/wdz306ling/article/details/87930133</a></p>]]></content>
    
    
    <categories>
      
      <category>ceph</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ceph</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>使用ceph提供文件存储</title>
    <link href="/posts/%E4%BD%BF%E7%94%A8ceph%E6%8F%90%E4%BE%9B%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8/"/>
    <url>/posts/%E4%BD%BF%E7%94%A8ceph%E6%8F%90%E4%BE%9B%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8/</url>
    
    <content type="html"><![CDATA[<p>上篇文章我讲了如果<a href="https://www.lijiawang.org/posts/ceph-deploy-%E5%AE%89%E8%A3%85ceph/">使用ceph-deploy搭建一个ceph完整的集群</a>，这次我来说下如何使用ceph的文件存储</p><a id="more"></a><h1 id="查看ceph集群状态"><a href="#查看ceph集群状态" class="headerlink" title="查看ceph集群状态"></a>查看ceph集群状态</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@ceph--1 ~]# ceph -s<br>  cluster:<br>    id:     faa2e2c4-98bc-47c4-a5b4-a478721b7ea2<br>    health: HEALTH_OK<br><br>  services:<br>    mon: 3 daemons, quorum ceph--1,ceph--2,ceph--3 (age 18h)<br>    mgr: ceph--1(active, since 17h), standbys: ceph--2, ceph--3<br>    mds:  3 up:standby<br>    osd: 3 osds: 3 up (since 17h), 3 in (since 17h)<br>    rgw: 3 daemons active (ceph--1, ceph--2, ceph--3)<br><br>  task status:<br><br>  data:<br>    pools:   4 pools, 128 pgs<br>    objects: 219 objects, 1.2 KiB<br>    usage:   3.0 GiB used, 147 GiB / 150 GiB avail<br>    pgs:     128 active+clean<br><br>[root@ceph--1 ~]# ceph mds stat<br> 3 up:standby<br></code></pre></td></tr></table></figure><blockquote><p>如果想使用ceph的文件存储，必须需要启动ceph-mds组件。同时一个Ceph文件系统至少需要两个RADOS池，一个用于数据，另一个用于元数据，对元数据池使用更高的复制级别，因为此池中的任何数据丢失都可能导致整个文件系统无法访问。<br>对元数据池使用较低延迟的存储（例如SSD），因为这将直接影响在客户端上观察到的文件系统操作的延迟。</p></blockquote><h1 id="创建数据和元数据POOL"><a href="#创建数据和元数据POOL" class="headerlink" title="创建数据和元数据POOL"></a>创建数据和元数据POOL</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@ceph--1 ~]# ceph osd pool create cephfs_data 32<br>pool &#x27;cephfs_data&#x27; created<br>[root@ceph--1 ~]# ceph osd pool create cephfs_metadata 32<br>pool &#x27;cephfs_metadata&#x27; created<br>[root@ceph--1 ~]# ceph osd lspools<br>1 .rgw.root<br>2 default.rgw.control<br>3 default.rgw.meta<br>4 default.rgw.log<br>5 cephfs_data<br>6 cephfs_metadata<br></code></pre></td></tr></table></figure><h1 id="创建ceph文件系统"><a href="#创建ceph文件系统" class="headerlink" title="创建ceph文件系统"></a>创建ceph文件系统</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@ceph--1 ~]# ceph fs new cephfs cephfs_metadata cephfs_data<br>new fs with metadata pool 6 and data pool 5<br>[root@ceph--1 ~]# ceph fs ls<br>name: cephfs, metadata pool: cephfs_metadata, data pools: [cephfs_data ]<br>[root@ceph--1 ~]# ceph mds stat<br>cephfs:1 &#123;0=ceph--2=up:active&#125; 2 up:standby<br>[root@ceph--1 ~]# ceph -s<br>  cluster:<br>    id:     faa2e2c4-98bc-47c4-a5b4-a478721b7ea2<br>    health: HEALTH_OK<br><br>  services:<br>    mon: 3 daemons, quorum ceph--1,ceph--2,ceph--3 (age 18h)<br>    mgr: ceph--1(active, since 17h), standbys: ceph--2, ceph--3<br>    mds: cephfs:1 &#123;0=ceph--2=up:active&#125; 2 up:standby<br>    osd: 3 osds: 3 up (since 18h), 3 in (since 18h)<br>    rgw: 3 daemons active (ceph--1, ceph--2, ceph--3)<br><br>  task status:<br>    scrub status:<br>        mds.ceph--2: idle<br><br>  data:<br>    pools:   6 pools, 192 pgs<br>    objects: 241 objects, 3.4 KiB<br>    usage:   3.0 GiB used, 147 GiB / 150 GiB avail<br>    pgs:     192 active+clean<br></code></pre></td></tr></table></figure><blockquote><p>挂载文件系统一般分为内核级别挂载和系统用户空间挂载，现在linux内核已经提供内核级别挂载</p></blockquote><h1 id="内核级别挂载"><a href="#内核级别挂载" class="headerlink" title="内核级别挂载"></a>内核级别挂载</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@ceph--1 cephfs]# lsmod |grep ceph   # centos 提供的内核节点挂载模块<br>ceph                  363016  1<br>libceph               306750  1 ceph<br>libcrc32c              12644  1 libceph<br>dns_resolver           13140  1 libceph<br>[root@ceph--1 ~]# which mount.ceph        #查看该模块<br>/sbin/mount.ceph<br>[root@ceph--1 ~]# rpm -qf /sbin/mount.ceph<br>ceph-common-14.2.16-0.el7.x86_64<br>[root@ceph--1 ~]# mkdir -p /mnt/cephfs/   创建挂载点<br>[root@ceph--1 ~]# mount -t ceph 10.140.11.8:6789:/ /mnt/cephfs/ -o name=admin<br>[root@ceph--1 ~]# df -h<br>Filesystem          Size  Used Avail Use% Mounted on<br>devtmpfs            7.9G     0  7.9G   0% /dev<br>tmpfs               7.9G     0  7.9G   0% /dev/shm<br>tmpfs               7.9G   17M  7.9G   1% /run<br>tmpfs               7.9G     0  7.9G   0% /sys/fs/cgroup<br>/dev/vda1            94G  2.7G   87G   3% /<br>tmpfs               7.9G   52K  7.9G   1% /var/lib/ceph/osd/ceph-0<br>tmpfs               1.6G     0  1.6G   0% /run/user/1000<br>10.140.11.8:6789:/   47G     0   47G   0% /mnt/cephfs<br>[root@ceph--1 ~]# ceph df<br>RAW STORAGE:<br>    CLASS     SIZE        AVAIL       USED       RAW USED     %RAW USED<br>    hdd       150 GiB     147 GiB     14 MiB      3.0 GiB          2.01<br>    TOTAL     150 GiB     147 GiB     14 MiB      3.0 GiB          2.01<br><br>POOLS:<br>    POOL                    ID     PGS     STORED      OBJECTS     USED        %USED     MAX AVAIL<br>    .rgw.root                1      32     1.2 KiB           4     768 KiB         0        46 GiB<br>    default.rgw.control      2      32         0 B           8         0 B         0        46 GiB<br>    default.rgw.meta         3      32         0 B           0         0 B         0        46 GiB<br>    default.rgw.log          4      32         0 B         207         0 B         0        46 GiB<br>    cephfs_data              5      32         0 B           0         0 B         0        46 GiB<br>    cephfs_metadata          6      32     2.9 KiB          22     1.5 MiB         0        46 GiB<br></code></pre></td></tr></table></figure><h1 id="验证内核级别的挂载点"><a href="#验证内核级别的挂载点" class="headerlink" title="验证内核级别的挂载点"></a>验证内核级别的挂载点</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@ceph--1 ~]# cd /mnt/cephfs/<br>[root@ceph--1 cephfs]# ls<br>[root@ceph--1 cephfs]# ls -l<br>total 0<br>[root@ceph--1 cephfs]# touch lijiawang<br>[root@ceph--1 cephfs]# echo &quot;aaaaaaaaaaaaaaaaaaaaa&quot; &gt; lijiawang <br>[root@ceph--1 cephfs]# cat lijiawang <br></code></pre></td></tr></table></figure><h1 id="用户空间挂载"><a href="#用户空间挂载" class="headerlink" title="用户空间挂载"></a>用户空间挂载</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@ceph--1 ~]# yum -y install ceph-fuse<br>[root@ceph--1 ~]# mkdir /mnt/ceph-fuse   # 创建挂载点<br>[root@ceph--1 ~]# ceph-fuse -n client.admin -m 10.140.11.8:6789,10.140.11.6:6789,10.140.11.24:6789 /mnt/ceph-fuse/<br>2021-01-06 09:03:28.223 7fcdd94d1f80 -1 init, newargv = 0x5588725df4c0 newargc=9<br>ceph-fuse[23357]: starting ceph client<br>ceph-fuse[23357]: starting fuse<br>[root@ceph--1 ~]# df -h<br>Filesystem          Size  Used Avail Use% Mounted on<br>devtmpfs            7.9G     0  7.9G   0% /dev<br>tmpfs               7.9G     0  7.9G   0% /dev/shm<br>tmpfs               7.9G   17M  7.9G   1% /run<br>tmpfs               7.9G     0  7.9G   0% /sys/fs/cgroup<br>/dev/vda1            94G  2.8G   87G   4% /<br>tmpfs               7.9G   52K  7.9G   1% /var/lib/ceph/osd/ceph-0<br>tmpfs               1.6G     0  1.6G   0% /run/user/1000<br>10.140.11.8:6789:/   47G     0   47G   0% /mnt/cephfs<br>ceph-fuse            47G     0   47G   0% /mnt/ceph-fuse<br></code></pre></td></tr></table></figure><h1 id="验证用户空间级别别的挂载点"><a href="#验证用户空间级别别的挂载点" class="headerlink" title="验证用户空间级别别的挂载点"></a>验证用户空间级别别的挂载点</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@ceph--1 ~]# cd /mnt/ceph-fuse<br>[root@ceph--1 ceph-fuse]# ls -l<br>total 1<br>-rw-r--r-- 1 root root 22 Jan  6 08:56 lijiawang<br>[root@ceph--1 ceph-fuse]# cat lijiawang <br>aaaaaaaaaaaaaaaaaaaaa<br>[root@ceph--1 ceph-fuse]# df -T|grep ceph-fuse<br>ceph-fuse          fuse.ceph-fuse  48746496       0  48746496   0% /mnt/ceph-fuse<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>ceph</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ceph</tag>
      
      <tag>cephfs</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ceph-deploy 安装ceph</title>
    <link href="/posts/ceph-deploy-%E5%AE%89%E8%A3%85ceph/"/>
    <url>/posts/ceph-deploy-%E5%AE%89%E8%A3%85ceph/</url>
    
    <content type="html"><![CDATA[<h1 id="ceph-安装"><a href="#ceph-安装" class="headerlink" title="ceph 安装"></a>ceph 安装</h1><p>使用ceph-deploy 安装ceph，ceph-deploy现在只能安装nautilus或者之前的版本，octopus和octopus以后的版本不在使用ceph-deploy方式安装，而是建议使用cephadm来安装，后期我会写一个cephadm安装配置ceph集群。</p><h1 id="准备机器"><a href="#准备机器" class="headerlink" title="准备机器"></a>准备机器</h1><p>下面的机器是我用kvm虚化出来3台，每个机器上有一个数据盘vdb</p><table><thead><tr><th>主机</th><th>应用</th></tr></thead><tbody><tr><td>ceph–1</td><td>ceph-deploy ceph-mon ceph-mgr ceph-raw ceph-mds ceph-osd</td></tr><tr><td>ceph–2</td><td>ceph-mon ceph-mgr ceph-raw ceph-mds ceph-osd</td></tr><tr><td>ceph–3</td><td>ceph-mon ceph-mgr ceph-raw ceph-mds ceph-osd</td></tr></tbody></table><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> 所有机器配置hosts主机名解析</span><br><span class="hljs-meta">#</span><span class="bash"> cat /etc/hosts</span><br>127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4<br>::1         localhost localhost.localdomain localhost6 localhost6.localdomain6<br>10.140.11.8  ceph--1<br>10.140.11.6  ceph--2<br>10.140.11.24 ceph--3<br><span class="hljs-meta">#</span><span class="bash"> lsblk</span> <br>NAME   MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT<br>sr0     11:0    1  478K  0 rom<br>vda    253:0    0  100G  0 disk<br>└─vda1 253:1    0  100G  0 part /<br>vdb    253:16   0   50G  0 disk<br></code></pre></td></tr></table></figure><h1 id="deploy节点到ceph主机配置免密登录"><a href="#deploy节点到ceph主机配置免密登录" class="headerlink" title="deploy节点到ceph主机配置免密登录"></a>deploy节点到ceph主机配置免密登录</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@ceph--1 ~]# ssh-keygen <br>Generating public/private rsa key pair.<br>Enter file in which to save the key (/root/.ssh/id_rsa): <br>Enter passphrase (empty for no passphrase): <br>Enter same passphrase again:<br>Your identification has been saved in /root/.ssh/id_rsa.<br>Your public key has been saved in /root/.ssh/id_rsa.pub.<br>The key fingerprint is:<br>SHA256:g981QUE32gwwVNoVB2aSzyTDEUmg2CQdSTgi05eYUWM root@ceph--1.localdomain<br>The key&#x27;s randomart image is:<br>+---[RSA 2048]----+<br>|   ..=E=++=B@*Oo.|<br>|  o =.=*o. =*@.o |<br>|   o o..o . +*o  |<br>|       .     .o  |<br>|      . S   o    |<br>|       . o . .   |<br>|        . .      |<br>|                 |<br>|                 |<br>+----[SHA256]-----+<br><br>[root@ceph--1 ~]# ssh-copy-id ceph--1<br>[root@ceph--1 ~]# ssh-copy-id ceph--2<br>[root@ceph--1 ~]# ssh-copy-id ceph--3<br></code></pre></td></tr></table></figure><h1 id="所有节点配置国内ceph源"><a href="#所有节点配置国内ceph源" class="headerlink" title="所有节点配置国内ceph源"></a>所有节点配置国内ceph源</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> cat /etc/yum.repos.d/ceph.repo</span><br>[norch]<br>name=norch<br>baseurl=https://mirrors.aliyun.com/ceph/rpm-nautilus/el7/noarch/<br>enabled=1<br>gpgcheck=0<br><br>[x86_64]<br>name=x86_64<br>baseurl=https://mirrors.aliyun.com/ceph/rpm-nautilus/el7/x86_64/<br>enabled=1<br>gpgcheck=0<br></code></pre></td></tr></table></figure><h1 id="在ceph–1节点配置安装并配置ceph-deploy节点"><a href="#在ceph–1节点配置安装并配置ceph-deploy节点" class="headerlink" title="在ceph–1节点配置安装并配置ceph-deploy节点"></a>在ceph–1节点配置安装并配置ceph-deploy节点</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@ceph--1 ~]# mkdir ceph-deploy<br>[root@ceph--1 ~]# yum -y install python-setuptools ceph-deploy<br>[root@ceph--1 ceph-deploy]# ceph-deploy new ceph--1<br>[ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf<br>[ceph_deploy.cli][INFO  ] Invoked (2.0.1): /bin/ceph-deploy new ceph--1<br>[ceph_deploy.cli][INFO  ] ceph-deploy options:<br>[ceph_deploy.cli][INFO  ]  username                      : None<br>[ceph_deploy.cli][INFO  ]  func                          : &lt;function new at 0x7f4162516de8&gt;<br>[ceph_deploy.cli][INFO  ]  verbose                       : False<br>[ceph_deploy.cli][INFO  ]  overwrite_conf                : False<br>[ceph_deploy.cli][INFO  ]  quiet                         : False<br>[ceph_deploy.cli][INFO  ]  cd_conf                       : &lt;ceph_deploy.conf.cephdeploy.Conf instance at 0x7f4161c98e18&gt;<br>[ceph_deploy.cli][INFO  ]  cluster                       : ceph<br>[ceph_deploy.cli][INFO  ]  ssh_copykey                   : True<br>[ceph_deploy.cli][INFO  ]  mon                           : [&#x27;ceph--1&#x27;]<br>[ceph_deploy.cli][INFO  ]  public_network                : None<br>[ceph_deploy.cli][INFO  ]  ceph_conf                     : None<br>[ceph_deploy.cli][INFO  ]  cluster_network               : None<br>[ceph_deploy.cli][INFO  ]  default_release               : False<br>[ceph_deploy.cli][INFO  ]  fsid                          : None<br>[ceph_deploy.new][DEBUG ] Creating new cluster named ceph<br>[ceph_deploy.new][INFO  ] making sure passwordless SSH succeeds<br>[ceph--1][DEBUG ] connected to host: ceph--1 <br>[ceph--1][DEBUG ] detect platform information from remote host<br>[ceph--1][DEBUG ] detect machine type<br>[ceph--1][DEBUG ] find the location of an executable<br>[ceph--1][INFO  ] Running command: /usr/sbin/ip link show<br>[ceph--1][INFO  ] Running command: /usr/sbin/ip addr show<br>[ceph--1][DEBUG ] IP addresses found: [u&#x27;10.140.11.8&#x27;]<br>[ceph_deploy.new][DEBUG ] Resolving host ceph--1<br>[ceph_deploy.new][DEBUG ] Monitor ceph--1 at 10.140.11.8<br>[ceph_deploy.new][DEBUG ] Monitor initial members are [&#x27;ceph--1&#x27;]<br>[ceph_deploy.new][DEBUG ] Monitor addrs are [&#x27;10.140.11.8&#x27;]<br>[ceph_deploy.new][DEBUG ] Creating a random mon key...<br>[ceph_deploy.new][DEBUG ] Writing monitor keyring to ceph.mon.keyring...<br>[ceph_deploy.new][DEBUG ] Writing initial config to ceph.conf...<br></code></pre></td></tr></table></figure><h1 id="所有节点安装ceph基础软件包"><a href="#所有节点安装ceph基础软件包" class="headerlink" title="所有节点安装ceph基础软件包"></a>所有节点安装ceph基础软件包</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> yum -y install ceph ceph-mon ceph-mgr ceph-radosgw ceph-mds ceph-mgr-dashboard</span><br></code></pre></td></tr></table></figure><h1 id="使用ceph-deploy创建mon节点"><a href="#使用ceph-deploy创建mon节点" class="headerlink" title="使用ceph-deploy创建mon节点"></a>使用ceph-deploy创建mon节点</h1><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><code class="hljs yaml">[<span class="hljs-string">root@ceph--1</span> <span class="hljs-string">ceph-deploy</span>]<span class="hljs-comment"># ceph-deploy mon create-initial</span><br>[<span class="hljs-string">root@ceph--1</span> <span class="hljs-string">ceph-deploy</span>]<span class="hljs-comment"># ceph-deploy mon add ceph--2 # 添加mon 节点</span><br>[<span class="hljs-string">root@ceph--1</span> <span class="hljs-string">ceph-deploy</span>]<span class="hljs-comment"># ceph-deploy mon add ceph--3</span><br>[<span class="hljs-string">root@ceph--1</span> <span class="hljs-string">ceph-deploy</span>]<span class="hljs-comment"># ceph-deploy admin ceph--1 ceph--2 ceph--3  # 同步配置文件</span><br>[<span class="hljs-string">root@ceph--1</span> <span class="hljs-string">ceph-deploy</span>]<span class="hljs-comment"># ceph -s</span><br>  <span class="hljs-attr">cluster:</span><br>    <span class="hljs-attr">id:</span>     <span class="hljs-string">faa2e2c4-98bc-47c4-a5b4-a478721b7ea2</span><br>    <span class="hljs-attr">health:</span> <span class="hljs-string">HEALTH_OK</span><br><br>  <span class="hljs-attr">services:</span><br>    <span class="hljs-attr">mon:</span> <span class="hljs-number">3</span> <span class="hljs-string">daemons,</span> <span class="hljs-string">quorum</span> <span class="hljs-string">ceph--1,ceph--2,ceph--3</span> <span class="hljs-string">(age</span> <span class="hljs-string">49s)</span><br>    <span class="hljs-attr">mgr:</span> <span class="hljs-literal">no</span> <span class="hljs-string">daemons</span> <span class="hljs-string">active</span><br>    <span class="hljs-attr">osd: 0 osds:</span> <span class="hljs-number">0</span> <span class="hljs-string">up,</span> <span class="hljs-number">0</span> <span class="hljs-string">in</span><br><br>  <span class="hljs-attr">data:</span><br>    <span class="hljs-attr">pools:</span>   <span class="hljs-number">0</span> <span class="hljs-string">pools,</span> <span class="hljs-number">0</span> <span class="hljs-string">pgs</span><br>    <span class="hljs-attr">objects:</span> <span class="hljs-number">0</span> <span class="hljs-string">objects,</span> <span class="hljs-number">0</span> <span class="hljs-string">B</span><br>    <span class="hljs-attr">usage:</span>   <span class="hljs-number">0</span> <span class="hljs-string">B</span> <span class="hljs-string">used,</span> <span class="hljs-number">0</span> <span class="hljs-string">B</span> <span class="hljs-string">/</span> <span class="hljs-number">0</span> <span class="hljs-string">B</span> <span class="hljs-string">avail</span><br>    <span class="hljs-attr">pgs:</span><br><br>[<span class="hljs-string">root@ceph--1</span> <span class="hljs-string">ceph-deploy</span>]<span class="hljs-comment"># ceph-deploy mgr create ceph--1 ceph--2 ceph--3</span><br>[<span class="hljs-string">root@ceph--1</span> <span class="hljs-string">ceph-deploy</span>]<span class="hljs-comment"># ceph -s</span><br>  <span class="hljs-attr">cluster:</span><br>    <span class="hljs-attr">id:</span>     <span class="hljs-string">faa2e2c4-98bc-47c4-a5b4-a478721b7ea2</span><br>    <span class="hljs-attr">health:</span> <span class="hljs-string">HEALTH_WARN</span><br>            <span class="hljs-string">OSD</span> <span class="hljs-string">count</span> <span class="hljs-number">0</span> <span class="hljs-string">&lt;</span> <span class="hljs-string">osd_pool_default_size</span> <span class="hljs-number">3</span><br><br>  <span class="hljs-attr">services:</span><br>    <span class="hljs-attr">mon:</span> <span class="hljs-number">3</span> <span class="hljs-string">daemons,</span> <span class="hljs-string">quorum</span> <span class="hljs-string">ceph--1,ceph--2,ceph--3</span> <span class="hljs-string">(age</span> <span class="hljs-string">2m)</span><br>    <span class="hljs-attr">mgr:</span> <span class="hljs-string">ceph--1(active,</span> <span class="hljs-string">since</span> <span class="hljs-string">14m),</span> <span class="hljs-attr">standbys:</span> <span class="hljs-string">ceph--2,</span> <span class="hljs-string">ceph--3</span><br>    <span class="hljs-attr">osd: 0 osds:</span> <span class="hljs-number">0</span> <span class="hljs-string">up,</span> <span class="hljs-number">0</span> <span class="hljs-string">in</span><br><br>  <span class="hljs-attr">data:</span><br>    <span class="hljs-attr">pools:</span>   <span class="hljs-number">0</span> <span class="hljs-string">pools,</span> <span class="hljs-number">0</span> <span class="hljs-string">pgs</span><br>    <span class="hljs-attr">objects:</span> <span class="hljs-number">0</span> <span class="hljs-string">objects,</span> <span class="hljs-number">0</span> <span class="hljs-string">B</span><br>    <span class="hljs-attr">usage:</span>   <span class="hljs-number">0</span> <span class="hljs-string">B</span> <span class="hljs-string">used,</span> <span class="hljs-number">0</span> <span class="hljs-string">B</span> <span class="hljs-string">/</span> <span class="hljs-number">0</span> <span class="hljs-string">B</span> <span class="hljs-string">avail</span><br>    <span class="hljs-attr">pgs:</span><br></code></pre></td></tr></table></figure><h1 id="创建osd"><a href="#创建osd" class="headerlink" title="创建osd"></a>创建osd</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@ceph--1 ceph-deploy]# ceph-deploy osd create ceph--1 --data /dev/vdb <br>[root@ceph--1 ceph-deploy]# ceph-deploy osd create ceph--2 --data /dev/vdb <br>[root@ceph--1 ceph-deploy]# ceph-deploy osd create ceph--3 --data /dev/vdb <br>[root@ceph--1 ceph-deploy]# ceph -s<br>  cluster:<br>    id:     faa2e2c4-98bc-47c4-a5b4-a478721b7ea2<br>    health: HEALTH_OK<br><br>  services:<br>    mon: 3 daemons, quorum ceph--1,ceph--2,ceph--3 (age 8m)<br>    mgr: ceph--1(active, since 20m), standbys: ceph--2, ceph--3<br>    osd: 3 osds: 3 up (since 32s), 3 in (since 32s)<br><br>  data:<br>    pools:   0 pools, 0 pgs<br>    objects: 0 objects, 0 B<br>    usage:   3.0 GiB used, 147 GiB / 150 GiB avail<br>    pgs:<br></code></pre></td></tr></table></figure><h1 id="使用ceph-deploy配置ceph对象存储ceph-raw"><a href="#使用ceph-deploy配置ceph对象存储ceph-raw" class="headerlink" title="使用ceph-deploy配置ceph对象存储ceph-raw"></a>使用ceph-deploy配置ceph对象存储ceph-raw</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@ceph--1 ceph-deploy]# ceph-deploy rgw create ceph--1 ceph--2 ceph--3<br></code></pre></td></tr></table></figure><h1 id="使用ceph-deploy配置ceph文件存储ceph-mds"><a href="#使用ceph-deploy配置ceph文件存储ceph-mds" class="headerlink" title="使用ceph-deploy配置ceph文件存储ceph-mds"></a>使用ceph-deploy配置ceph文件存储ceph-mds</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@ceph--1 ceph-deploy]# ceph-deploy mds create ceph--1 ceph--2 ceph--3<br>[root@ceph--1 ceph-deploy]# ceph -s<br>  cluster:<br>    id:     faa2e2c4-98bc-47c4-a5b4-a478721b7ea2<br>    health: HEALTH_OK<br><br>  services:<br>    mon: 3 daemons, quorum ceph--1,ceph--2,ceph--3 (age 17m)<br>    mgr: ceph--1(active, since 29m), standbys: ceph--2, ceph--3<br>    mds:  3 up:standby<br>    osd: 3 osds: 3 up (since 9m), 3 in (since 9m)<br>    rgw: 3 daemons active (ceph--1, ceph--2, ceph--3)<br><br>  task status:<br><br>  data:<br>    pools:   4 pools, 128 pgs<br>    objects: 187 objects, 1.2 KiB<br>    usage:   3.0 GiB used, 147 GiB / 150 GiB avail<br>    pgs:     128 active+clean<br><br>  io:<br>    client:   83 KiB/s rd, 0 B/s wr, 82 op/s rd, 54 op/s wr<br></code></pre></td></tr></table></figure><h1 id="开启并设置ceph-dashboard"><a href="#开启并设置ceph-dashboard" class="headerlink" title="开启并设置ceph-dashboard"></a>开启并设置ceph-dashboard</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@ceph--1 ceph-deploy]# ceph  mgr module enable  dashboard<br>[root@ceph--1 ceph-deploy]# ceph  dashboard create-self-signed-cert<br>Self-signed certificate created<br>[root@ceph--1 ceph-deploy]# ceph config set mgr mgr/dashboard/server_addr 10.140.11.8<br>[root@ceph--1 ceph-deploy]# ceph config set mgr mgr/dashboard/server_port 8080<br>[root@ceph--1 ceph-deploy]# ceph config set mgr mgr/dashboard/ssl_server_port 8443<br>[root@ceph--1 ceph-deploy]# ceph dashboard set-login-credentials admin admin<br>******************************************************************<br>***          WARNING: this command is deprecated.              ***<br>*** Please use the ac-user-* related commands to manage users. ***<br>******************************************************************<br>Username and password updated<br>systemctl restart ceph-mgr@ceph-node1<br>[root@ceph--1 ceph-deploy]# ceph mgr services<br>&#123;<br>    &quot;dashboard&quot;: &quot;https://ceph--1:8443/&quot;<br>&#125;<br></code></pre></td></tr></table></figure><p>请用谷歌浏览器访问<a href="https://10.140.11.8:8443/#/dashboard">https://10.140.11.8:8443/#/dashboard</a><br>账号密码admin/admin<br><img src="https://ljw.howieli.cn/blog/2021-01-06/ceph-01.png" alt="dashboard"></p>]]></content>
    
    
    <categories>
      
      <category>ceph</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ceph</tag>
      
      <tag>ceph-deploy</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Tekton安装及Hello world</title>
    <link href="/posts/Tekton%E5%AE%89%E8%A3%85%E5%8F%8AHello-world/"/>
    <url>/posts/Tekton%E5%AE%89%E8%A3%85%E5%8F%8AHello-world/</url>
    
    <content type="html"><![CDATA[<a id="more"></a><h1 id="准备k8s集群"><a href="#准备k8s集群" class="headerlink" title="准备k8s集群"></a>准备k8s集群</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@k8s-master ~]# kubectl get nodes<br>NAME         STATUS   ROLES    AGE   VERSION<br>k8s-master   Ready    master   28d   v1.19.4<br>k8s-node01   Ready    &lt;none&gt;   28d   v1.19.4<br>k8s-node02   Ready    &lt;none&gt;   28d   v1.19.4<br>k8s-node03   Ready    &lt;none&gt;   28d   v1.19.4<br>k8s-node04   Ready    &lt;none&gt;   28d   v1.19.4<br></code></pre></td></tr></table></figure><h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs css"># <span class="hljs-selector-tag">kubectl</span> <span class="hljs-selector-tag">apply</span> <span class="hljs-selector-tag">--filename</span> <span class="hljs-selector-tag">https</span>://<span class="hljs-selector-tag">storage</span><span class="hljs-selector-class">.googleapis</span><span class="hljs-selector-class">.com</span>/<span class="hljs-selector-tag">tekton-releases</span>/<span class="hljs-selector-tag">pipeline</span>/<span class="hljs-selector-tag">latest</span>/<span class="hljs-selector-tag">release</span><span class="hljs-selector-class">.yaml</span><br></code></pre></td></tr></table></figure><p>检查安装的tekton相关的CRD</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-comment"># kubectl api-resources |grep tekton</span><br><span class="hljs-string">clustertasks</span>                                      <span class="hljs-string">tekton.dev</span>                         <span class="hljs-literal">false</span>        <span class="hljs-string">ClusterTask</span><br><span class="hljs-string">conditions</span>                                        <span class="hljs-string">tekton.dev</span>                         <span class="hljs-literal">true</span>         <span class="hljs-string">Condition</span><br><span class="hljs-string">pipelineresources</span>                                 <span class="hljs-string">tekton.dev</span>                         <span class="hljs-literal">true</span>         <span class="hljs-string">PipelineResource</span><br><span class="hljs-string">pipelineruns</span>                      <span class="hljs-string">pr,prs</span>          <span class="hljs-string">tekton.dev</span>                         <span class="hljs-literal">true</span>         <span class="hljs-string">PipelineRun</span><br><span class="hljs-string">pipelines</span>                                         <span class="hljs-string">tekton.dev</span>                         <span class="hljs-literal">true</span>         <span class="hljs-string">Pipeline</span><br><span class="hljs-string">runs</span>                                              <span class="hljs-string">tekton.dev</span>                         <span class="hljs-literal">true</span>         <span class="hljs-string">Run</span><br><span class="hljs-string">taskruns</span>                          <span class="hljs-string">tr,trs</span>          <span class="hljs-string">tekton.dev</span>                         <span class="hljs-literal">true</span>         <span class="hljs-string">TaskRun</span><br><span class="hljs-string">tasks</span>                                             <span class="hljs-string">tekton.dev</span>                         <span class="hljs-literal">true</span>         <span class="hljs-string">Task</span><br></code></pre></td></tr></table></figure><p>查看pod</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-comment"># kubectl get pods --namespace tekton-pipelines</span><br><span class="hljs-attribute">NAME</span>                                           READY   STATUS    RESTARTS   AGE<br><span class="hljs-attribute">tekton</span>-pipelines-controller-<span class="hljs-number">5</span>cdb<span class="hljs-number">46974</span>f-lfnrs   <span class="hljs-number">1</span>/<span class="hljs-number">1</span>     Running   <span class="hljs-number">0</span>          <span class="hljs-number">6</span>d<br><span class="hljs-attribute">tekton</span>-pipelines-webhook-<span class="hljs-number">6479</span>d<span class="hljs-number">769</span>ff-<span class="hljs-number">7</span>ct<span class="hljs-number">5</span>g      <span class="hljs-number">1</span>/<span class="hljs-number">1</span>     Running   <span class="hljs-number">0</span>          <span class="hljs-number">6</span>d<br></code></pre></td></tr></table></figure><h1 id="安装CLI"><a href="#安装CLI" class="headerlink" title="安装CLI"></a>安装CLI</h1><p>CLI：<a href="https://github.com/tektoncd/cli#installing-tkn">https://github.com/tektoncd/cli#installing-tkn</a><br>我这是centos7操作系统</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs vim"># rpm -Uvh http<span class="hljs-variable">s:</span>//github.<span class="hljs-keyword">com</span>/tektoncd/cli/releases/download/v0.<span class="hljs-number">14.0</span>/tektoncd-cli-<span class="hljs-number">0.14</span>.<span class="hljs-number">0</span>_Linux-<span class="hljs-number">64</span>bit.rpm<br># tkn -h<br>CLI <span class="hljs-keyword">for</span> tekton pipelines<br><br>Usage:<br>tkn [flags]<br>tkn [<span class="hljs-keyword">command</span>]<br><br>Available Command<span class="hljs-variable">s:</span><br>  clustertask           Manage ClusterTasks<br>  clustertriggerbinding Manage ClusterTriggerBindings<br>  condition             Manage Conditions<br>  eventlistener         Manage EventListeners<br>  hub                   Interact with tekton hub<br>  pipeline              Manage pipelines<br>  pipelinerun           Manage PipelineRuns<br>  resource              Manage pipeline resources<br>  task                  Manage Tasks<br>  taskrun               Manage TaskRuns<br>  triggerbinding        Manage TriggerBindings<br>  triggertemplate       Manage TriggerTemplates<br><br>Other Command<span class="hljs-variable">s:</span><br>  completion            Prints <span class="hljs-keyword">shell</span> completion scripts<br>  <span class="hljs-keyword">version</span>               Prints <span class="hljs-keyword">version</span> information<br><br>Flag<span class="hljs-variable">s:</span><br>  -h, --<span class="hljs-keyword">help</span>   <span class="hljs-keyword">help</span> <span class="hljs-keyword">for</span> tkn<br><br>Use <span class="hljs-string">&quot;tkn [command] --help&quot;</span> <span class="hljs-keyword">for</span> more information about <span class="hljs-keyword">a</span> <span class="hljs-keyword">command</span>.<br>http<span class="hljs-variable">s:</span>//github.<span class="hljs-keyword">com</span>/tektoncd/cli#installing-tkn<br></code></pre></td></tr></table></figure><h1 id="Tekton：hello-world"><a href="#Tekton：hello-world" class="headerlink" title="Tekton：hello world"></a>Tekton：hello world</h1><p>创建一个简单的Task, 只有一个step就是打印出”hello world”</p><figure class="highlight dts"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs dts"><span class="hljs-meta"># cat hello_task.yml </span><br><span class="hljs-symbol">apiVersion:</span> tekton.dev/v1alpha1<br><span class="hljs-symbol">kind:</span> Task<br><span class="hljs-symbol">metadata:</span><br><span class="hljs-symbol">  name:</span> echo-hello-world<br><span class="hljs-symbol">spec:</span><br><span class="hljs-symbol">  steps:</span><br>    - name: echo<br><span class="hljs-symbol">      image:</span> alpine<br><span class="hljs-symbol">      command:</span><br>        - echo<br><span class="hljs-symbol">      args:</span><br>        - <span class="hljs-string">&quot;hello world&quot;</span><br></code></pre></td></tr></table></figure><p>创建一个TaskRun执行上面的Task</p><figure class="highlight dts"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs dts"><span class="hljs-meta"># cat hello_task_run.yml </span><br><span class="hljs-symbol">apiVersion:</span> tekton.dev/v1alpha1<br><span class="hljs-symbol">kind:</span> TaskRun<br><span class="hljs-symbol">metadata:</span><br><span class="hljs-symbol">  name:</span> echo-hello-world-task-run<br><span class="hljs-symbol">spec:</span><br><span class="hljs-symbol">  taskRef:</span><br><span class="hljs-symbol">    name:</span> echo-hello-world<br></code></pre></td></tr></table></figure><p>运行task</p><figure class="highlight sqf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs sqf"><span class="hljs-meta"># kubectl apply -f hello_task.yml </span><br>task.tekton.dev/<span class="hljs-built_in">echo</span>-hello-world created<br><span class="hljs-meta"># kubectl get task</span><br><span class="hljs-built_in">NAME</span>               AGE<br><span class="hljs-built_in">echo</span>-hello-world   <span class="hljs-number">12</span>s<br><span class="hljs-meta"># tkn task list</span><br><span class="hljs-built_in">NAME</span>               DESCRIPTION   AGE<br><span class="hljs-built_in">echo</span>-hello-world                 <span class="hljs-number">1</span> minute ago<br></code></pre></td></tr></table></figure><p>运行一个taskrun</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs css"># <span class="hljs-selector-tag">kubectl</span> <span class="hljs-selector-tag">apply</span> <span class="hljs-selector-tag">-f</span> <span class="hljs-selector-tag">hello_task_run</span><span class="hljs-selector-class">.yml</span> <br><span class="hljs-selector-tag">taskrun</span><span class="hljs-selector-class">.tekton</span><span class="hljs-selector-class">.dev</span>/<span class="hljs-selector-tag">echo-hello-world-task-run</span> <span class="hljs-selector-tag">created</span><br></code></pre></td></tr></table></figure><p>检查TaskRun的输出, 执行命令</p><figure class="highlight sqf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><code class="hljs sqf"><span class="hljs-meta"># tkn taskrun list</span><br><span class="hljs-built_in">NAME</span>                        STARTED          DURATION    STATUS<br><span class="hljs-built_in">echo</span>-hello-world-task-run   <span class="hljs-number">25</span> seconds ago   <span class="hljs-number">6</span> seconds   Succeeded<br><span class="hljs-meta"># kubectl get taskrun</span><br><span class="hljs-built_in">NAME</span>                        SUCCEEDED   REASON      STARTTIME   COMPLETIONTIME<br><span class="hljs-built_in">echo</span>-hello-world-task-run   <span class="hljs-literal">True</span>        Succeeded   <span class="hljs-number">37</span>s         <span class="hljs-number">31</span>s<br><br><span class="hljs-meta"># kubectl get pod</span><br><span class="hljs-built_in">NAME</span>                                  READY   STATUS      RESTARTS   AGE<br><span class="hljs-built_in">echo</span>-hello-world-task-run-pod-<span class="hljs-number">8</span>xnvb   <span class="hljs-number">0</span>/<span class="hljs-number">1</span>     Completed   <span class="hljs-number">0</span>          <span class="hljs-number">74</span>s<br><br>可以看到有一个pod生成<br><br><br><span class="hljs-meta"># tkn taskrun describe echo-hello-world-task-run</span><br><span class="hljs-built_in">Name</span>:              <span class="hljs-built_in">echo</span>-hello-world-task-run<br>Namespace:         <span class="hljs-keyword">default</span><br>Task Ref:          <span class="hljs-built_in">echo</span>-hello-world<br>Service Account:   <span class="hljs-keyword">default</span><br>Timeout:           <span class="hljs-number">1</span>h0m0s<br>Labels:<br> app.kubernetes.io/managed-by=tekton-pipelines<br> tekton.dev/task=<span class="hljs-built_in">echo</span>-hello-world<br><br>Status<br><br>STARTED        DURATION    STATUS<br><span class="hljs-number">1</span> minute ago   <span class="hljs-number">6</span> seconds   Succeeded<br><br>Input <span class="hljs-built_in">Resources</span><br><br>No input <span class="hljs-built_in">resources</span><br><br>Output <span class="hljs-built_in">Resources</span><br><br>No output <span class="hljs-built_in">resources</span><br><br><span class="hljs-built_in">Params</span><br><br>No <span class="hljs-built_in">params</span><br><br>Results<br><br>No results<br><br>Workspaces<br><br>No workspaces<br><br>Steps<br><br><span class="hljs-built_in">NAME</span>     STATUS<br><span class="hljs-built_in">echo</span>   Completed<br><br>Sidecars<br><br>No sidecars<br></code></pre></td></tr></table></figure><p>Succeeded状态表示task执行成功.</p><p>查看实际的输出，执行命令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># tkn taskrun logs echo-hello-world-task-run</span><br>[<span class="hljs-built_in">echo</span>] hello world<br></code></pre></td></tr></table></figure><p>可以看到结果如下</p><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs angelscript"><span class="hljs-string">[echo]</span> hello world<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>tekton</category>
      
    </categories>
    
    
    <tags>
      
      <tag>kubernetes</tag>
      
      <tag>CI/CD</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2020年总结</title>
    <link href="/posts/2020%E5%B9%B4%E6%80%BB%E7%BB%93/"/>
    <url>/posts/2020%E5%B9%B4%E6%80%BB%E7%BB%93/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>openstack instance vm_power_state 状态置为 NOSTATE</title>
    <link href="/posts/openstack-instance-vm-power-state-%E7%8A%B6%E6%80%81%E7%BD%AE%E4%B8%BA-NOSTATE/"/>
    <url>/posts/openstack-instance-vm-power-state-%E7%8A%B6%E6%80%81%E7%BD%AE%E4%B8%BA-NOSTATE/</url>
    
    <content type="html"><![CDATA[<h3 id="openstack-上的instance运行正常，但是power-state状态为NOSTATE，导致instance热迁移失败"><a href="#openstack-上的instance运行正常，但是power-state状态为NOSTATE，导致instance热迁移失败" class="headerlink" title="openstack 上的instance运行正常，但是power_state状态为NOSTATE，导致instance热迁移失败"></a>openstack 上的instance运行正常，但是power_state状态为NOSTATE，导致instance热迁移失败</h3><h3 id="现象"><a href="#现象" class="headerlink" title="现象"></a>现象</h3><p>1、再一次热迁移时候发现迁移时候报以下错误</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-comment"># openstack server migrate --live node-53 86487ef4-cc12-4be6-995e-46f5ac093901</span><br><span class="hljs-attribute">Cannot</span> &#x27;os-migrateLive&#x27; instance <span class="hljs-number">86487</span>ef<span class="hljs-number">4</span>-cc<span class="hljs-number">12</span>-<span class="hljs-number">4</span>be<span class="hljs-number">6</span>-<span class="hljs-number">995</span>e-<span class="hljs-number">46</span>f<span class="hljs-number">5</span>ac<span class="hljs-number">093901</span> while it is in power_state <span class="hljs-number">0</span> (HTTP <span class="hljs-number">409</span>) (Request-ID: req-<span class="hljs-number">6</span>c<span class="hljs-number">14</span>e<span class="hljs-number">0</span>ee-c<span class="hljs-number">3</span>df-<span class="hljs-number">42</span>de-<span class="hljs-number">873</span>d-<span class="hljs-number">9</span>ecc<span class="hljs-number">8</span>ad<span class="hljs-number">215</span>cc)<br></code></pre></td></tr></table></figure><p>2、查看实例，发现虚拟机运行正常，但是 power_state 为 NOSTATE</p><figure class="highlight gherkin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs gherkin"><span class="hljs-comment"># openstack server show 86487ef4-cc12-4be6-995e-46f5ac093901</span><br>+-------------------------------------+----------------------------------------------------------------+<br>|<span class="hljs-string"> Field                               </span>|<span class="hljs-string"> Value                                                          </span>|<br>+-------------------------------------+----------------------------------------------------------------+<br>|<span class="hljs-string"> OS-DCF:diskConfig                   </span>|<span class="hljs-string"> MANUAL                                                         </span>|<br>|<span class="hljs-string"> OS-EXT-AZ:availability_zone         </span>|<span class="hljs-string"> m_cpu+san                                                      </span>|<br>|<span class="hljs-string"> OS-EXT-SRV-ATTR:host                </span>|<span class="hljs-string"> node-27                                                        </span>|<br>|<span class="hljs-string"> OS-EXT-SRV-ATTR:hypervisor_hostname </span>|<span class="hljs-string"> node-27                                                        </span>|<br>|<span class="hljs-string"> OS-EXT-SRV-ATTR:instance_name       </span>|<span class="hljs-string"> instance-000071aa                                              </span>|<br>|<span class="hljs-string"> OS-EXT-STS:power_state              </span>|<span class="hljs-string"> NOSTATE                                                        </span>|<br>|<span class="hljs-string"> OS-EXT-STS:task_state               </span>|<span class="hljs-string"> None                                                           </span>|<br>|<span class="hljs-string"> OS-EXT-STS:vm_state                 </span>|<span class="hljs-string"> active                                                         </span>|<br>|<span class="hljs-string"> OS-SRV-USG:launched_at              </span>|<span class="hljs-string"> 2018-09-21T05:40:36.000000                                     </span>|<br>|<span class="hljs-string"> OS-SRV-USG:terminated_at            </span>|<span class="hljs-string"> None                                                           </span>|<br>|<span class="hljs-string"> accessIPv4                          </span>|<span class="hljs-string">                                                                </span>|<br>|<span class="hljs-string"> accessIPv6                          </span>|<span class="hljs-string">                                                                </span>|<br>|<span class="hljs-string"> addresses                           </span>|<span class="hljs-string"> vlan_10.122.44.0/23=10.122.45.53                               </span>|<br>|<span class="hljs-string"> config_drive                        </span>|<span class="hljs-string"> True                                                           </span>|<br>|<span class="hljs-string"> created                             </span>|<span class="hljs-string"> 2018-09-21T05:40:31Z                                           </span>|<br>|<span class="hljs-string"> flavor                              </span>|<span class="hljs-string"> 16-64-100 (1cbe4ea1-8a67-4027-afd4-8f31a8b94851)               </span>|<br>|<span class="hljs-string"> hostId                              </span>|<span class="hljs-string"> 2730fa3d62e347ddb67e155f6eed973787a868c82316f7e6ba641b10       </span>|<br>|<span class="hljs-string"> id                                  </span>|<span class="hljs-string"> 86487ef4-cc12-4be6-995e-46f5ac093901                           </span>|<br>|<span class="hljs-string"> image                               </span>|<span class="hljs-string"> lenovo-centos-7-release (b9f8f864-4217-4ac6-a116-62ecfa0fc074) </span>|<br>|<span class="hljs-string"> key_name                            </span>|<span class="hljs-string"> None                                                           </span>|<br>|<span class="hljs-string"> name                                </span>|<span class="hljs-string"> SLP3YM7HRCX                                                    </span>|<br>|<span class="hljs-string"> progress                            </span>|<span class="hljs-string"> 100                                                            </span>|<br>|<span class="hljs-string"> project_id                          </span>|<span class="hljs-string"> e992715df18a417997c068e5f9834b0f                               </span>|<br>|<span class="hljs-string"> properties                          </span>|<span class="hljs-string">                                                                </span>|<br>|<span class="hljs-string"> security_groups                     </span>|<span class="hljs-string"> name=&#x27;default&#x27;                                                 </span>|<br>|<span class="hljs-string"> status                              </span>|<span class="hljs-string"> ACTIVE                                                         </span>|<br>|<span class="hljs-string"> updated                             </span>|<span class="hljs-string"> 2019-10-09T07:58:58Z                                           </span>|<br>|<span class="hljs-string"> user_id                             </span>|<span class="hljs-string"> af518aeb935c4d258f8cb7d302c83797                               </span>|<br>|<span class="hljs-string"> volumes_attached                    </span>|<span class="hljs-string"> id=&#x27;a821856b-409f-4e6e-becd-eb4b2344c7d8&#x27;                      </span>|<br>+-------------------------------------+----------------------------------------------------------------+<br></code></pre></td></tr></table></figure><h3 id="查找原因"><a href="#查找原因" class="headerlink" title="查找原因"></a>查找原因</h3><p>列出该虚拟机的历史操作</p><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs subunit"># nova instance-action-list 86487ef4-cc12<span class="hljs-string">-4</span>be6<span class="hljs-string">-995</span>e<span class="hljs-string">-46</span>f5ac093901<br>+----------------+------------------------------------------+---------+----------------------------+<br>| Action         | Request_ID                               | Message | Start_Time                 |<br>+----------------+------------------------------------------+---------+----------------------------+<br>| create         | req<span class="hljs-string">-4</span>d01a466-ce7a<span class="hljs-string">-48</span>a9-b575-e129b61bcc30 | -       | 2018<span class="hljs-string">-09</span><span class="hljs-string">-21</span>T05:40:30.000000 |<br>| live-migration | req<span class="hljs-string">-2</span>d93bc73-e43f<span class="hljs-string">-4</span>ca6<span class="hljs-string">-88</span>fc-a6ad1a4021a6 | -       | 2018<span class="hljs-string">-09</span><span class="hljs-string">-21</span>T05:44:47.000000 |<br>| live-migration | req<span class="hljs-string">-7</span>fcbfb15<span class="hljs-string">-7</span>e3c<span class="hljs-string">-471</span>d<span class="hljs-string">-8</span>ea5<span class="hljs-string">-020</span>d438d23c3 | -       | 2019<span class="hljs-string">-03</span><span class="hljs-string">-24</span>T13:00:30.000000 |<br>| live-migration | req<span class="hljs-string">-2</span>b009c4f-caef<span class="hljs-string">-4</span>d90<span class="hljs-string">-81</span>fd-d480c1b9efad | Error   | 2019<span class="hljs-string">-10</span><span class="hljs-string">-09</span>T07:00:13.000000 |<br>| live-migration | req-b364724b<span class="hljs-string">-6</span>b28<span class="hljs-string">-4</span>d5f<span class="hljs-string">-8</span>f14<span class="hljs-string">-05</span>d1c4abeceb | Error   | 2019<span class="hljs-string">-10</span><span class="hljs-string">-09</span>T07:07:37.000000 |<br>| live-migration | req<span class="hljs-string">-8669</span>d3b1-e57e<span class="hljs-string">-4</span>b35<span class="hljs-string">-8</span>d40-a110b64f47c8 | Error   | 2019<span class="hljs-string">-10</span><span class="hljs-string">-09</span>T07:29:13.000000 |<br>| live-migration | req-ecec98ca<span class="hljs-string">-3777</span><span class="hljs-string">-4</span>ca3-afcd<span class="hljs-string">-83</span>e36381e038 | Error   | 2019<span class="hljs-string">-10</span><span class="hljs-string">-09</span>T07:55:12.000000 |<br>| live-migration | req<span class="hljs-string">-6378</span>f839<span class="hljs-string">-89</span>c0<span class="hljs-string">-43</span>c0<span class="hljs-string">-8</span>a65<span class="hljs-string">-40</span>e5b775283c | Error   | 2019<span class="hljs-string">-10</span><span class="hljs-string">-09</span>T07:58:14.000000 |<br>| live-migration | req-e966caa7<span class="hljs-string">-06</span>e5<span class="hljs-string">-4</span>a29-a371<span class="hljs-string">-517</span>d98c37121 | Error   | 2019<span class="hljs-string">-10</span><span class="hljs-string">-09</span>T07:58:58.000000 |<br>| live-migration | req<span class="hljs-string">-6</span>c14e0ee-c3df<span class="hljs-string">-42</span>de<span class="hljs-string">-873</span>d<span class="hljs-string">-9</span>ecc8ad215cc | Error   | 2019<span class="hljs-string">-10</span><span class="hljs-string">-09</span>T08:31:47.000000 |<br>+----------------+------------------------------------------+---------+----------------------------+<br></code></pre></td></tr></table></figure><p>列出该实例的迁移记录</p><figure class="highlight gherkin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br></pre></td><td class="code"><pre><code class="hljs gherkin"><span class="hljs-comment"># nova  migration-list|grep 86487ef4-cc12-4be6-995e-46f5ac093901</span><br>|<span class="hljs-string"> 84892 </span>|<span class="hljs-string"> -           </span>|<span class="hljs-string"> -         </span>|<span class="hljs-string"> node-27        </span>|<span class="hljs-string"> node-29            </span>|<span class="hljs-string"> -            </span>|<span class="hljs-string"> error         </span>|<span class="hljs-string"> 86487ef4-cc12-4be6-995e-46f5ac093901 </span>|<span class="hljs-string"> 189        </span>|<span class="hljs-string"> 189        </span>|<span class="hljs-string"> 2019-10-09T07:07:37.000000 </span>|<span class="hljs-string"> 2019-10-09T07:07:37.000000 </span>|<span class="hljs-string"> live-migration </span>|<br>|<span class="hljs-string"> 54307 </span>|<span class="hljs-string"> -           </span>|<span class="hljs-string"> -         </span>|<span class="hljs-string"> node-27        </span>|<span class="hljs-string"> node-53            </span>|<span class="hljs-string"> -            </span>|<span class="hljs-string"> error         </span>|<span class="hljs-string"> 86487ef4-cc12-4be6-995e-46f5ac093901 </span>|<span class="hljs-string"> 189        </span>|<span class="hljs-string"> 189        </span>|<span class="hljs-string"> 2019-03-24T13:00:30.000000 </span>|<span class="hljs-string"> 2019-03-24T17:10:10.000000 </span>|<span class="hljs-string"> live-migration </span>|<br>|<span class="hljs-string"> 84904 </span>|<span class="hljs-string"> -           </span>|<span class="hljs-string"> -         </span>|<span class="hljs-string"> node-27        </span>|<span class="hljs-string"> node-53            </span>|<span class="hljs-string"> -            </span>|<span class="hljs-string"> error         </span>|<span class="hljs-string"> 86487ef4-cc12-4be6-995e-46f5ac093901 </span>|<span class="hljs-string"> 189        </span>|<span class="hljs-string"> 189        </span>|<span class="hljs-string"> 2019-10-09T07:29:13.000000 </span>|<span class="hljs-string"> 2019-10-09T07:29:13.000000 </span>|<span class="hljs-string"> live-migration </span>|<br>|<span class="hljs-string"> 84913 </span>|<span class="hljs-string"> -           </span>|<span class="hljs-string"> -         </span>|<span class="hljs-string"> node-27        </span>|<span class="hljs-string"> node-53            </span>|<span class="hljs-string"> -            </span>|<span class="hljs-string"> error         </span>|<span class="hljs-string"> 86487ef4-cc12-4be6-995e-46f5ac093901 </span>|<span class="hljs-string"> 189        </span>|<span class="hljs-string"> 189        </span>|<span class="hljs-string"> 2019-10-09T07:55:12.000000 </span>|<span class="hljs-string"> 2019-10-09T07:55:12.000000 </span>|<span class="hljs-string"> live-migration </span>|<br>|<span class="hljs-string"> 84919 </span>|<span class="hljs-string"> -           </span>|<span class="hljs-string"> -         </span>|<span class="hljs-string"> node-27        </span>|<span class="hljs-string"> node-53            </span>|<span class="hljs-string"> -            </span>|<span class="hljs-string"> error         </span>|<span class="hljs-string"> 86487ef4-cc12-4be6-995e-46f5ac093901 </span>|<span class="hljs-string"> 189        </span>|<span class="hljs-string"> 189        </span>|<span class="hljs-string"> 2019-10-09T07:58:14.000000 </span>|<span class="hljs-string"> 2019-10-09T07:58:14.000000 </span>|<span class="hljs-string"> live-migration </span>|<br>|<span class="hljs-string"> 84922 </span>|<span class="hljs-string"> -           </span>|<span class="hljs-string"> -         </span>|<span class="hljs-string"> node-27        </span>|<span class="hljs-string"> node-53            </span>|<span class="hljs-string"> -            </span>|<span class="hljs-string"> error         </span>|<span class="hljs-string"> 86487ef4-cc12-4be6-995e-46f5ac093901 </span>|<span class="hljs-string"> 189        </span>|<span class="hljs-string"> 189        </span>|<span class="hljs-string"> 2019-10-09T07:58:58.000000 </span>|<span class="hljs-string"> 2019-10-09T07:58:58.000000 </span>|<span class="hljs-string"> live-migration </span>|<br>|<span class="hljs-string"> 84931 </span>|<span class="hljs-string"> -           </span>|<span class="hljs-string"> -         </span>|<span class="hljs-string"> node-27        </span>|<span class="hljs-string"> node-53            </span>|<span class="hljs-string"> -            </span>|<span class="hljs-string"> error         </span>|<span class="hljs-string"> 86487ef4-cc12-4be6-995e-46f5ac093901 </span>|<span class="hljs-string"> 189        </span>|<span class="hljs-string"> 189        </span>|<span class="hljs-string"> 2019-10-09T08:31:47.000000 </span>|<span class="hljs-string"> 2019-10-09T08:31:47.000000 </span>|<span class="hljs-string"> live-migration </span>|<br>|<span class="hljs-string"> 84889 </span>|<span class="hljs-string"> -           </span>|<span class="hljs-string"> -         </span>|<span class="hljs-string"> node-27        </span>|<span class="hljs-string"> node-89            </span>|<span class="hljs-string"> -            </span>|<span class="hljs-string"> error         </span>|<span class="hljs-string"> 86487ef4-cc12-4be6-995e-46f5ac093901 </span>|<span class="hljs-string"> 189        </span>|<span class="hljs-string"> 189        </span>|<span class="hljs-string"> 2019-10-09T07:00:13.000000 </span>|<span class="hljs-string"> 2019-10-09T07:00:13.000000 </span>|<span class="hljs-string"> live-migration </span>|<br>|<span class="hljs-string"> 42241 </span>|<span class="hljs-string"> -           </span>|<span class="hljs-string"> -         </span>|<span class="hljs-string"> node-33        </span>|<span class="hljs-string"> node-27            </span>|<span class="hljs-string"> -            </span>|<span class="hljs-string"> completed     </span>|<span class="hljs-string"> 86487ef4-cc12-4be6-995e-46f5ac093901 </span>|<span class="hljs-string"> 189        </span>|<span class="hljs-string"> 189        </span>|<span class="hljs-string"> 2018-09-21T05:44:48.000000 </span>|<span class="hljs-string"> 2018-09-21T05:45:13.000000 </span>|<span class="hljs-string"> live-migration </span>|<br>可以看到该实例从node-27节点迁移到node-85、node-53、node-29节点均为迁移成功<br>查看instance所在的节点<br><span class="hljs-comment"># ssh node-27</span><br><br>root<span class="hljs-meta">@node-27:~#</span> virsh list --all|<span class="hljs-string">grep instance-000071aa </span><br><span class="hljs-string">root@node-27:~# exit</span><br><span class="hljs-string">logout</span><br><span class="hljs-string">Connection to node-27 closed.</span><br><span class="hljs-string"># ssh node-53</span><br><br><span class="hljs-string">root@node-53:~#  virsh list --all</span>|<span class="hljs-string">grep instance-000071aa</span><br><span class="hljs-string"> 91    instance-000071aa              running</span><br><span class="hljs-string">root@node-53:~# exit</span><br><span class="hljs-string">logout</span><br><span class="hljs-string">Connection to node-53 closed.</span><br><span class="hljs-string"># openstack server show 86487ef4-cc12-4be6-995e-46f5ac093901</span><br><span class="hljs-string">+-------------------------------------+----------------------------------------------------------------+</span><br>|<span class="hljs-string"> Field                               </span>|<span class="hljs-string"> Value                                                          </span>|<br>+-------------------------------------+----------------------------------------------------------------+<br>|<span class="hljs-string"> OS-DCF:diskConfig                   </span>|<span class="hljs-string"> MANUAL                                                         </span>|<br>|<span class="hljs-string"> OS-EXT-AZ:availability_zone         </span>|<span class="hljs-string"> m_cpu+san                                                      </span>|<br>|<span class="hljs-string"> OS-EXT-SRV-ATTR:host                </span>|<span class="hljs-string"> node-27                                                        </span>|<br>|<span class="hljs-string"> OS-EXT-SRV-ATTR:hypervisor_hostname </span>|<span class="hljs-string"> node-27                                                        </span>|<br>|<span class="hljs-string"> OS-EXT-SRV-ATTR:instance_name       </span>|<span class="hljs-string"> instance-000071aa                                              </span>|<br>|<span class="hljs-string"> OS-EXT-STS:power_state              </span>|<span class="hljs-string"> NOSTATE                                                        </span>|<br>|<span class="hljs-string"> OS-EXT-STS:task_state               </span>|<span class="hljs-string"> None                                                           </span>|<br>|<span class="hljs-string"> OS-EXT-STS:vm_state                 </span>|<span class="hljs-string"> active                                                         </span>|<br>|<span class="hljs-string"> OS-SRV-USG:launched_at              </span>|<span class="hljs-string"> 2018-09-21T05:40:36.000000                                     </span>|<br>|<span class="hljs-string"> OS-SRV-USG:terminated_at            </span>|<span class="hljs-string"> None                                                           </span>|<br>|<span class="hljs-string"> accessIPv4                          </span>|<span class="hljs-string">                                                                </span>|<br>|<span class="hljs-string"> accessIPv6                          </span>|<span class="hljs-string">                                                                </span>|<br>|<span class="hljs-string"> addresses                           </span>|<span class="hljs-string"> vlan_10.122.44.0/23=10.122.45.53                               </span>|<br>|<span class="hljs-string"> config_drive                        </span>|<span class="hljs-string"> True                                                           </span>|<br>|<span class="hljs-string"> created                             </span>|<span class="hljs-string"> 2018-09-21T05:40:31Z                                           </span>|<br>|<span class="hljs-string"> flavor                              </span>|<span class="hljs-string"> 16-64-100 (1cbe4ea1-8a67-4027-afd4-8f31a8b94851)               </span>|<br>|<span class="hljs-string"> hostId                              </span>|<span class="hljs-string"> 2730fa3d62e347ddb67e155f6eed973787a868c82316f7e6ba641b10       </span>|<br>|<span class="hljs-string"> id                                  </span>|<span class="hljs-string"> 86487ef4-cc12-4be6-995e-46f5ac093901                           </span>|<br>|<span class="hljs-string"> image                               </span>|<span class="hljs-string"> lenovo-centos-7-release (b9f8f864-4217-4ac6-a116-62ecfa0fc074) </span>|<br>|<span class="hljs-string"> key_name                            </span>|<span class="hljs-string"> None                                                           </span>|<br>|<span class="hljs-string"> name                                </span>|<span class="hljs-string"> SLP3YM7HRCX                                                    </span>|<br>|<span class="hljs-string"> progress                            </span>|<span class="hljs-string"> 100                                                            </span>|<br>|<span class="hljs-string"> project_id                          </span>|<span class="hljs-string"> e992715df18a417997c068e5f9834b0f                               </span>|<br>|<span class="hljs-string"> properties                          </span>|<span class="hljs-string">                                                                </span>|<br>|<span class="hljs-string"> security_groups                     </span>|<span class="hljs-string"> name=&#x27;default&#x27;                                                 </span>|<br>|<span class="hljs-string"> status                              </span>|<span class="hljs-string"> ACTIVE                                                         </span>|<br>|<span class="hljs-string"> updated                             </span>|<span class="hljs-string"> 2019-10-09T08:31:47Z                                           </span>|<br>|<span class="hljs-string"> user_id                             </span>|<span class="hljs-string"> af518aeb935c4d258f8cb7d302c83797                               </span>|<br>|<span class="hljs-string"> volumes_attached                    </span>|<span class="hljs-string"> id=&#x27;a821856b-409f-4e6e-becd-eb4b2344c7d8&#x27;                      </span>|<br>+-------------------------------------+----------------------------------------------------------------+<br>root<span class="hljs-meta">@node-1:~#</span> mysql -unova -pnova<br>MariaDB [(none)]&gt; select <span class="hljs-symbol">*</span> from nova.instances where uuid=&#x27;86487ef4-cc12-4be6-995e-46f5ac093901&#x27;\G;<br><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span> 1. row <span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><br>              created_at: 2018-09-21 05:40:31<br>              updated_at: 2019-10-09 08:31:47<br>              deleted_at: NULL<br>                      id: 29098<br>             internal_id: NULL<br>                 user_id: af518aeb935c4d258f8cb7d302c83797<br>              project_id: e992715df18a417997c068e5f9834b0f<br>               image_ref: b9f8f864-4217-4ac6-a116-62ecfa0fc074<br>               kernel_id: <br>              ramdisk_id: <br>            launch_index: 0<br>                key_name: NULL<br>                key_data: NULL<br>             power_state: 0<br>                vm_state: active<br>               memory_mb: 65536<br>                   vcpus: 16<br>                hostname: slp3ym7hrcx<br>                    host: node-27<br>               user_data: NULL<br>          reservation_id: r-o8dr8ibn<br>             launched_at: 2018-09-21 05:40:36<br>           terminated_at: NULL<br>            display_name: SLP3YM7HRCX<br>     display_description: SLP3YM7HRCX<br>       availability_zone: no_san<br>                  locked: 0<br>                 os_type: NULL<br>             launched_on: node-33<br>        instance_type_id: 189<br>                 vm_mode: NULL<br>                    uuid: 86487ef4-cc12-4be6-995e-46f5ac093901<br>            architecture: NULL<br>        root_device_name: /dev/vda<br>            access_ip_v4: NULL<br>            access_ip_v6: NULL<br>            config_drive: True<br>              task_state: NULL<br>default_ephemeral_device: NULL<br>     default_swap_device: NULL<br>                progress: 100<br>        auto_disk_config: 0<br>      shutdown_terminate: 0<br>       disable_terminate: 0<br>                 root_gb: 100<br>            ephemeral_gb: 0<br>               cell_name: NULL<br>                    node: node-27<br>                 deleted: 0<br>               locked_by: NULL<br>                 cleaned: 1<br>      ephemeral_key_uuid: NULL<br>1 row in set (0.00 sec)<br><br>ERROR: No query specified<br></code></pre></td></tr></table></figure><p>发现实际instance启动在node-53节点上，但是数据库记录在node-27节点上</p><p>manager.py 捕获到 InstanceNotFound 设置 vm_power_state=NOSTATE</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python">/usr/lib/python2<span class="hljs-number">.7</span>/dist-packages/nova/compute/manager.py<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_query_driver_power_state_and_sync</span>(<span class="hljs-params">self, context, db_instance</span>):</span><br>        <span class="hljs-keyword">if</span> db_instance.task_state <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            LOG.info(_LI(<span class="hljs-string">&quot;During sync_power_state the instance has a &quot;</span><br>                         <span class="hljs-string">&quot;pending task (%(task)s). Skip.&quot;</span>),<br>                     &#123;<span class="hljs-string">&#x27;task&#x27;</span>: db_instance.task_state&#125;, instance=db_instance)<br>            <span class="hljs-keyword">return</span><br>        <span class="hljs-comment"># No pending tasks. Now try to figure out the real vm_power_state.</span><br>        <span class="hljs-keyword">try</span>:<br>            vm_instance = self.driver.get_info(db_instance)<br>            vm_power_state = vm_instance.state<br>        <span class="hljs-keyword">except</span> exception.InstanceNotFound:         <span class="hljs-comment"># 可以看到如果InstanceNotFound</span><br>            vm_power_state = power_state.NOSTATE   <span class="hljs-comment"># power_state设置为NOSTATE</span><br>        <span class="hljs-comment"># Note(maoy): the above get_info call might take a long time,</span><br>        <span class="hljs-comment"># for example, because of a broken libvirt driver.</span><br>        <span class="hljs-keyword">try</span>:<br>            self._sync_instance_power_state(context,<br>                                            db_instance,<br>                                            vm_power_state,<br>                                            use_slave=<span class="hljs-literal">True</span>)<br>        <span class="hljs-keyword">except</span> exception.InstanceNotFound:<br>            <span class="hljs-comment"># NOTE(hanlind): If the instance gets deleted during sync,</span><br>            <span class="hljs-comment"># silently ignore.</span><br>            <span class="hljs-keyword">pass</span><br></code></pre></td></tr></table></figure><h3 id="得到原因"><a href="#得到原因" class="headerlink" title="得到原因"></a>得到原因</h3><p>由于之前 migrate ERROR 导致 instance 实际启动的节点和数据库记录的节点不一致，comoute 通过 manager.py 捕获到 InstanceNotFound， 所以把 vm_power_state 状态置为 NOSTATE</p><h3 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h3><p>更新数据库，将数据库中的instance对应的host、node更新成实际instance启动的节点信息。硬重启虚拟机</p><figure class="highlight gherkin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br></pre></td><td class="code"><pre><code class="hljs gherkin">root<span class="hljs-meta">@node-1:~#</span> mysql -unova -pnova<br>MariaDB [(none)]&gt; update nova.instances set host=&#x27;node-53&#x27; where uuid=&#x27;86487ef4-cc12-4be6-995e-46f5ac093901&#x27;\G;<br>Query OK, 1 row affected (0.00 sec)<br>Rows matched: 1  Changed: 1  Warnings: 0<br><br>ERROR: No query specified<br><br>MariaDB [(none)]&gt; update nova.instances set node=&#x27;node-53&#x27; where uuid=&#x27;86487ef4-cc12-4be6-995e-46f5ac093901&#x27;\G;<br>Query OK, 1 row affected (0.00 sec)<br>Rows matched: 1  Changed: 1  Warnings: 0<br><br>ERROR: No query specified<br><br>MariaDB [(none)]&gt; select <span class="hljs-symbol">*</span> from nova.instances where uuid=&#x27;86487ef4-cc12-4be6-995e-46f5ac093901&#x27;\G;<br><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span> 1. row <span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><br>              created_at: 2018-09-21 05:40:31<br>              updated_at: 2019-10-09 08:31:47<br>              deleted_at: NULL<br>                      id: 29098<br>             internal_id: NULL<br>                 user_id: af518aeb935c4d258f8cb7d302c83797<br>              project_id: e992715df18a417997c068e5f9834b0f<br>               image_ref: b9f8f864-4217-4ac6-a116-62ecfa0fc074<br>               kernel_id: <br>              ramdisk_id: <br>            launch_index: 0<br>                key_name: NULL<br>                key_data: NULL<br>             power_state: 0<br>                vm_state: active<br>               memory_mb: 65536<br>                   vcpus: 16<br>                hostname: slp3ym7hrcx<br>                    host: node-53<br>               user_data: NULL<br>          reservation_id: r-o8dr8ibn<br>             launched_at: 2018-09-21 05:40:36<br>           terminated_at: NULL<br>            display_name: SLP3YM7HRCX<br>     display_description: SLP3YM7HRCX<br>       availability_zone: no_san<br>                  locked: 0<br>                 os_type: NULL<br>             launched_on: node-33<br>        instance_type_id: 189<br>                 vm_mode: NULL<br>                    uuid: 86487ef4-cc12-4be6-995e-46f5ac093901<br>            architecture: NULL<br>        root_device_name: /dev/vda<br>            access_ip_v4: NULL<br>            access_ip_v6: NULL<br>            config_drive: True<br>              task_state: NULL<br>default_ephemeral_device: NULL<br>     default_swap_device: NULL<br>                progress: 100<br>        auto_disk_config: 0<br>      shutdown_terminate: 0<br>       disable_terminate: 0<br>                 root_gb: 100<br>            ephemeral_gb: 0<br>               cell_name: NULL<br>                    node: node-53<br>                 deleted: 0<br>               locked_by: NULL<br>                 cleaned: 1<br>      ephemeral_key_uuid: NULL<br>1 row in set (0.00 sec)<br><br>ERROR: No query specified<br><br>MariaDB [(none)]&gt; exit<br>Bye<br><span class="hljs-comment"># openstack server show 86487ef4-cc12-4be6-995e-46f5ac093901</span><br>+-------------------------------------+----------------------------------------------------------------+<br>|<span class="hljs-string"> Field                               </span>|<span class="hljs-string"> Value                                                          </span>|<br>+-------------------------------------+----------------------------------------------------------------+<br>|<span class="hljs-string"> OS-DCF:diskConfig                   </span>|<span class="hljs-string"> MANUAL                                                         </span>|<br>|<span class="hljs-string"> OS-EXT-AZ:availability_zone         </span>|<span class="hljs-string"> no_san                                                         </span>|<br>|<span class="hljs-string"> OS-EXT-SRV-ATTR:host                </span>|<span class="hljs-string"> node-53                                                        </span>|<br>|<span class="hljs-string"> OS-EXT-SRV-ATTR:hypervisor_hostname </span>|<span class="hljs-string"> node-53                                                        </span>|<br>|<span class="hljs-string"> OS-EXT-SRV-ATTR:instance_name       </span>|<span class="hljs-string"> instance-000071aa                                              </span>|<br>|<span class="hljs-string"> OS-EXT-STS:power_state              </span>|<span class="hljs-string"> NOSTATE                                                        </span>|<br>|<span class="hljs-string"> OS-EXT-STS:task_state               </span>|<span class="hljs-string"> None                                                           </span>|<br>|<span class="hljs-string"> OS-EXT-STS:vm_state                 </span>|<span class="hljs-string"> active                                                         </span>|<br>|<span class="hljs-string"> OS-SRV-USG:launched_at              </span>|<span class="hljs-string"> 2018-09-21T05:40:36.000000                                     </span>|<br>|<span class="hljs-string"> OS-SRV-USG:terminated_at            </span>|<span class="hljs-string"> None                                                           </span>|<br>|<span class="hljs-string"> accessIPv4                          </span>|<span class="hljs-string">                                                                </span>|<br>|<span class="hljs-string"> accessIPv6                          </span>|<span class="hljs-string">                                                                </span>|<br>|<span class="hljs-string"> addresses                           </span>|<span class="hljs-string"> vlan_10.122.44.0/23=10.122.45.53                               </span>|<br>|<span class="hljs-string"> config_drive                        </span>|<span class="hljs-string"> True                                                           </span>|<br>|<span class="hljs-string"> created                             </span>|<span class="hljs-string"> 2018-09-21T05:40:31Z                                           </span>|<br>|<span class="hljs-string"> flavor                              </span>|<span class="hljs-string"> 16-64-100 (1cbe4ea1-8a67-4027-afd4-8f31a8b94851)               </span>|<br>|<span class="hljs-string"> hostId                              </span>|<span class="hljs-string"> f5274ebe433297bc376a4ce09151591735a04d2b6762280866de2729       </span>|<br>|<span class="hljs-string"> id                                  </span>|<span class="hljs-string"> 86487ef4-cc12-4be6-995e-46f5ac093901                           </span>|<br>|<span class="hljs-string"> image                               </span>|<span class="hljs-string"> lenovo-centos-7-release (b9f8f864-4217-4ac6-a116-62ecfa0fc074) </span>|<br>|<span class="hljs-string"> key_name                            </span>|<span class="hljs-string"> None                                                           </span>|<br>|<span class="hljs-string"> name                                </span>|<span class="hljs-string"> SLP3YM7HRCX                                                    </span>|<br>|<span class="hljs-string"> progress                            </span>|<span class="hljs-string"> 100                                                            </span>|<br>|<span class="hljs-string"> project_id                          </span>|<span class="hljs-string"> e992715df18a417997c068e5f9834b0f                               </span>|<br>|<span class="hljs-string"> properties                          </span>|<span class="hljs-string">                                                                </span>|<br>|<span class="hljs-string"> security_groups                     </span>|<span class="hljs-string"> name=&#x27;default&#x27;                                                 </span>|<br>|<span class="hljs-string"> status                              </span>|<span class="hljs-string"> ACTIVE                                                         </span>|<br>|<span class="hljs-string"> updated                             </span>|<span class="hljs-string"> 2019-10-09T08:31:47Z                                           </span>|<br>|<span class="hljs-string"> user_id                             </span>|<span class="hljs-string"> af518aeb935c4d258f8cb7d302c83797                               </span>|<br>|<span class="hljs-string"> volumes_attached                    </span>|<span class="hljs-string"> id=&#x27;a821856b-409f-4e6e-becd-eb4b2344c7d8&#x27;                      </span>|<br>+-------------------------------------+----------------------------------------------------------------+<br><br><br><br><span class="hljs-comment"># nova reboot --hard 86487ef4-cc12-4be6-995e-46f5ac093901</span><br><br><span class="hljs-comment"># openstack  server show 86487ef4-cc12-4be6-995e-46f5ac093901</span><br>+-------------------------------------+----------------------------------------------------------------+<br>|<span class="hljs-string"> Field                               </span>|<span class="hljs-string"> Value                                                          </span>|<br>+-------------------------------------+----------------------------------------------------------------+<br>|<span class="hljs-string"> OS-DCF:diskConfig                   </span>|<span class="hljs-string"> MANUAL                                                         </span>|<br>|<span class="hljs-string"> OS-EXT-AZ:availability_zone         </span>|<span class="hljs-string"> no_san                                                         </span>|<br>|<span class="hljs-string"> OS-EXT-SRV-ATTR:host                </span>|<span class="hljs-string"> node-53                                                        </span>|<br>|<span class="hljs-string"> OS-EXT-SRV-ATTR:hypervisor_hostname </span>|<span class="hljs-string"> node-53                                                        </span>|<br>|<span class="hljs-string"> OS-EXT-SRV-ATTR:instance_name       </span>|<span class="hljs-string"> instance-000071aa                                              </span>|<br>|<span class="hljs-string"> OS-EXT-STS:power_state              </span>|<span class="hljs-string"> Running                                                        </span>|<br>|<span class="hljs-string"> OS-EXT-STS:task_state               </span>|<span class="hljs-string"> None                                                           </span>|<br>|<span class="hljs-string"> OS-EXT-STS:vm_state                 </span>|<span class="hljs-string"> active                                                         </span>|<br>|<span class="hljs-string"> OS-SRV-USG:launched_at              </span>|<span class="hljs-string"> 2018-09-21T05:40:36.000000                                     </span>|<br>|<span class="hljs-string"> OS-SRV-USG:terminated_at            </span>|<span class="hljs-string"> None                                                           </span>|<br>|<span class="hljs-string"> accessIPv4                          </span>|<span class="hljs-string">                                                                </span>|<br>|<span class="hljs-string"> accessIPv6                          </span>|<span class="hljs-string">                                                                </span>|<br>|<span class="hljs-string"> addresses                           </span>|<span class="hljs-string"> vlan_10.122.44.0/23=10.122.45.53                               </span>|<br>|<span class="hljs-string"> config_drive                        </span>|<span class="hljs-string"> True                                                           </span>|<br>|<span class="hljs-string"> created                             </span>|<span class="hljs-string"> 2018-09-21T05:40:31Z                                           </span>|<br>|<span class="hljs-string"> flavor                              </span>|<span class="hljs-string"> 16-64-100 (1cbe4ea1-8a67-4027-afd4-8f31a8b94851)               </span>|<br>|<span class="hljs-string"> hostId                              </span>|<span class="hljs-string"> 4bfe001f6fa4d443f283121b56139ef8bfbff47be8106387cc19edc6       </span>|<br>|<span class="hljs-string"> id                                  </span>|<span class="hljs-string"> 86487ef4-cc12-4be6-995e-46f5ac093901                           </span>|<br>|<span class="hljs-string"> image                               </span>|<span class="hljs-string"> lenovo-centos-7-release (b9f8f864-4217-4ac6-a116-62ecfa0fc074) </span>|<br>|<span class="hljs-string"> key_name                            </span>|<span class="hljs-string"> None                                                           </span>|<br>|<span class="hljs-string"> name                                </span>|<span class="hljs-string"> SLP3YM7HRCX                                                    </span>|<br>|<span class="hljs-string"> progress                            </span>|<span class="hljs-string"> 0                                                              </span>|<br>|<span class="hljs-string"> project_id                          </span>|<span class="hljs-string"> e992715df18a417997c068e5f9834b0f                               </span>|<br>|<span class="hljs-string"> properties                          </span>|<span class="hljs-string">                                                                </span>|<br>|<span class="hljs-string"> security_groups                     </span>|<span class="hljs-string"> name=&#x27;default&#x27;                                                 </span>|<br>|<span class="hljs-string"> status                              </span>|<span class="hljs-string"> ACTIVE                                                         </span>|<br>|<span class="hljs-string"> updated                             </span>|<span class="hljs-string"> 2019-10-09T10:04:19Z                                           </span>|<br>|<span class="hljs-string"> user_id                             </span>|<span class="hljs-string"> af518aeb935c4d258f8cb7d302c83797                               </span>|<br>|<span class="hljs-string"> volumes_attached                    </span>|<span class="hljs-string"> id=&#x27;a821856b-409f-4e6e-becd-eb4b2344c7d8&#x27;                      </span>|<br>|<span class="hljs-string">                                     </span>|<span class="hljs-string"> id=&#x27;ff7d2dad-1954-4906-ae18-95c76a09b442&#x27;                      </span>|<br>+-------------------------------------+----------------------------------------------------------------+<br><br><span class="hljs-comment"># openstack server migrate --live node-89 86487ef4-cc12-4be6-995e-46f5ac093901</span><br><br><span class="hljs-comment"># openstack  server show 86487ef4-cc12-4be6-995e-46f5ac093901</span><br>+-------------------------------------+----------------------------------------------------------------+<br>|<span class="hljs-string"> Field                               </span>|<span class="hljs-string"> Value                                                          </span>|<br>+-------------------------------------+----------------------------------------------------------------+<br>|<span class="hljs-string"> OS-DCF:diskConfig                   </span>|<span class="hljs-string"> MANUAL                                                         </span>|<br>|<span class="hljs-string"> OS-EXT-AZ:availability_zone         </span>|<span class="hljs-string"> Contingency                                                    </span>|<br>|<span class="hljs-string"> OS-EXT-SRV-ATTR:host                </span>|<span class="hljs-string"> node-89                                                        </span>|<br>|<span class="hljs-string"> OS-EXT-SRV-ATTR:hypervisor_hostname </span>|<span class="hljs-string"> node-89                                                        </span>|<br>|<span class="hljs-string"> OS-EXT-SRV-ATTR:instance_name       </span>|<span class="hljs-string"> instance-000071aa                                              </span>|<br>|<span class="hljs-string"> OS-EXT-STS:power_state              </span>|<span class="hljs-string"> Running                                                        </span>|<br>|<span class="hljs-string"> OS-EXT-STS:task_state               </span>|<span class="hljs-string"> None                                                           </span>|<br>|<span class="hljs-string"> OS-EXT-STS:vm_state                 </span>|<span class="hljs-string"> active                                                         </span>|<br>|<span class="hljs-string"> OS-SRV-USG:launched_at              </span>|<span class="hljs-string"> 2018-09-21T05:40:36.000000                                     </span>|<br>|<span class="hljs-string"> OS-SRV-USG:terminated_at            </span>|<span class="hljs-string"> None                                                           </span>|<br>|<span class="hljs-string"> accessIPv4                          </span>|<span class="hljs-string">                                                                </span>|<br>|<span class="hljs-string"> accessIPv6                          </span>|<span class="hljs-string">                                                                </span>|<br>|<span class="hljs-string"> addresses                           </span>|<span class="hljs-string"> vlan_10.122.44.0/23=10.122.45.53                               </span>|<br>|<span class="hljs-string"> config_drive                        </span>|<span class="hljs-string"> True                                                           </span>|<br>|<span class="hljs-string"> created                             </span>|<span class="hljs-string"> 2018-09-21T05:40:31Z                                           </span>|<br>|<span class="hljs-string"> flavor                              </span>|<span class="hljs-string"> 16-64-100 (1cbe4ea1-8a67-4027-afd4-8f31a8b94851)               </span>|<br>|<span class="hljs-string"> hostId                              </span>|<span class="hljs-string"> 4bfe001f6fa4d443f283121b56139ef8bfbff47be8106387cc19edc6       </span>|<br>|<span class="hljs-string"> id                                  </span>|<span class="hljs-string"> 86487ef4-cc12-4be6-995e-46f5ac093901                           </span>|<br>|<span class="hljs-string"> image                               </span>|<span class="hljs-string"> lenovo-centos-7-release (b9f8f864-4217-4ac6-a116-62ecfa0fc074) </span>|<br>|<span class="hljs-string"> key_name                            </span>|<span class="hljs-string"> None                                                           </span>|<br>|<span class="hljs-string"> name                                </span>|<span class="hljs-string"> SLP3YM7HRCX                                                    </span>|<br>|<span class="hljs-string"> progress                            </span>|<span class="hljs-string"> 0                                                              </span>|<br>|<span class="hljs-string"> project_id                          </span>|<span class="hljs-string"> e992715df18a417997c068e5f9834b0f                               </span>|<br>|<span class="hljs-string"> properties                          </span>|<span class="hljs-string">                                                                </span>|<br>|<span class="hljs-string"> security_groups                     </span>|<span class="hljs-string"> name=&#x27;default&#x27;                                                 </span>|<br>|<span class="hljs-string"> status                              </span>|<span class="hljs-string"> ACTIVE                                                         </span>|<br>|<span class="hljs-string"> updated                             </span>|<span class="hljs-string"> 2019-10-09T10:04:19Z                                           </span>|<br>|<span class="hljs-string"> user_id                             </span>|<span class="hljs-string"> af518aeb935c4d258f8cb7d302c83797                               </span>|<br>|<span class="hljs-string"> volumes_attached                    </span>|<span class="hljs-string"> id=&#x27;a821856b-409f-4e6e-becd-eb4b2344c7d8&#x27;                      </span>|<br>|<span class="hljs-string">                                     </span>|<span class="hljs-string"> id=&#x27;ff7d2dad-1954-4906-ae18-95c76a09b442&#x27;                      </span>|<br>+-------------------------------------+----------------------------------------------------------------+<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>openstack</category>
      
    </categories>
    
    
    <tags>
      
      <tag>openstack</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>使用kubeadm升级kubernetes集群</title>
    <link href="/posts/%E4%BD%BF%E7%94%A8kubeadm%E5%8D%87%E7%BA%A7kubernetes%E9%9B%86%E7%BE%A4/"/>
    <url>/posts/%E4%BD%BF%E7%94%A8kubeadm%E5%8D%87%E7%BA%A7kubernetes%E9%9B%86%E7%BE%A4/</url>
    
    <content type="html"><![CDATA[<h3 id="kubeadm升级k8s集群-v1-15-3-to-v1-16-0版"><a href="#kubeadm升级k8s集群-v1-15-3-to-v1-16-0版" class="headerlink" title="kubeadm升级k8s集群(v1.15.3 to v1.16.0版)"></a><code>kubeadm</code>升级<code>k8s</code>集群(<code>v1.15.3 to v1.16.0</code>版)</h3><blockquote><p><code>Kubernetes</code>在2019年9月18日发布了年度的第三个版本<code>1.16</code>,本篇文章介绍使用<code>kubeadm</code>升级现在有的集群到<code>v1.16.0</code>。</p></blockquote><h3 id="Kubernetes-1-16版本的发布徽章"><a href="#Kubernetes-1-16版本的发布徽章" class="headerlink" title="Kubernetes 1.16版本的发布徽章"></a><code>Kubernetes 1.16</code>版本的发布徽章</h3><p><img src="https://ljw.howieli.cn/blog/2019-9-23/Kubernetes%201.16%E7%89%88%E6%9C%AC%E7%9A%84%E5%8F%91%E5%B8%83%E5%BE%BD%E7%AB%A0.jpg" alt="Kubernetes 1.16版本的发布徽章"></p><h3 id="查看k8s集群版本"><a href="#查看k8s集群版本" class="headerlink" title="查看k8s集群版本"></a>查看<code>k8s</code>集群版本</h3><p>查看当前的<code>k8s</code>版本</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@k8s-master ~]# kubectl get nodes<br>NAME         STATUS   ROLES    AGE   VERSION<br>k8s-master   Ready    master   20d   v1.15.3<br>k8s-node1    Ready    &lt;none&gt;   20d   v1.15.3<br>k8s-node2    Ready    &lt;none&gt;   20d   v1.15.3<br>k8s-node3    Ready    &lt;none&gt;   20d   v1.15.3<br></code></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@k8s-master ~]# kubeadm version<br>kubeadm version: &amp;version.Info&#123;Major:&quot;1&quot;, Minor:&quot;15&quot;, GitVersion:&quot;v1.15.3&quot;, GitCommit:&quot;2d3c76f9091b6bec110a5e63777c332469e0cba2&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2019-08-19T11:11:18Z&quot;, GoVersion:&quot;go1.12.9&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux/amd64&quot;&#125;<br></code></pre></td></tr></table></figure><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs crmsh">[root@k8s-<span class="hljs-keyword">master</span> <span class="hljs-title">~]# kubelet</span> --<span class="hljs-keyword">version</span><br>Kubernetes v1.<span class="hljs-number">15.3</span><br><br><br>[root@k8s-<span class="hljs-keyword">master</span> <span class="hljs-title">~]# systemctl</span> status kubelet.service |grep Active<br>   Active: active (running) since Fri <span class="hljs-number">2019</span>-<span class="hljs-number">08</span>-<span class="hljs-number">30</span> <span class="hljs-number">07</span>:<span class="hljs-number">26</span>:<span class="hljs-number">33</span> EDT; <span class="hljs-number">3</span> weeks <span class="hljs-number">0</span> days ago<br></code></pre></td></tr></table></figure><figure class="highlight less"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs less"><span class="hljs-selector-attr">[root@k8s-master yum.repos.d]</span># <span class="hljs-selector-tag">kubectl</span> <span class="hljs-selector-tag">version</span><br><span class="hljs-selector-tag">Client</span> <span class="hljs-selector-tag">Version</span>: <span class="hljs-selector-tag">version</span><span class="hljs-selector-class">.Info</span>&#123;<span class="hljs-attribute">Major</span>:<span class="hljs-string">&quot;1&quot;</span>, <span class="hljs-attribute">Minor</span>:<span class="hljs-string">&quot;15&quot;</span>, <span class="hljs-attribute">GitVersion</span>:<span class="hljs-string">&quot;v1.15.3&quot;</span>, <span class="hljs-attribute">GitCommit</span>:<span class="hljs-string">&quot;2d3c76f9091b6bec110a5e63777c332469e0cba2&quot;</span>, <span class="hljs-attribute">GitTreeState</span>:<span class="hljs-string">&quot;clean&quot;</span>, <span class="hljs-attribute">BuildDate</span>:<span class="hljs-string">&quot;2019-08-19T11:13:54Z&quot;</span>, <span class="hljs-attribute">GoVersion</span>:<span class="hljs-string">&quot;go1.12.9&quot;</span>, <span class="hljs-attribute">Compiler</span>:<span class="hljs-string">&quot;gc&quot;</span>, <span class="hljs-attribute">Platform</span>:<span class="hljs-string">&quot;linux/amd64&quot;</span>&#125;<br><span class="hljs-selector-tag">Server</span> <span class="hljs-selector-tag">Version</span>: <span class="hljs-selector-tag">version</span><span class="hljs-selector-class">.Info</span>&#123;<span class="hljs-attribute">Major</span>:<span class="hljs-string">&quot;1&quot;</span>, <span class="hljs-attribute">Minor</span>:<span class="hljs-string">&quot;15&quot;</span>, <span class="hljs-attribute">GitVersion</span>:<span class="hljs-string">&quot;v1.15.3&quot;</span>, <span class="hljs-attribute">GitCommit</span>:<span class="hljs-string">&quot;2d3c76f9091b6bec110a5e63777c332469e0cba2&quot;</span>, <span class="hljs-attribute">GitTreeState</span>:<span class="hljs-string">&quot;clean&quot;</span>, <span class="hljs-attribute">BuildDate</span>:<span class="hljs-string">&quot;2019-08-19T11:05:50Z&quot;</span>, <span class="hljs-attribute">GoVersion</span>:<span class="hljs-string">&quot;go1.12.9&quot;</span>, <span class="hljs-attribute">Compiler</span>:<span class="hljs-string">&quot;gc&quot;</span>, <span class="hljs-attribute">Platform</span>:<span class="hljs-string">&quot;linux/amd64&quot;</span>&#125;<br></code></pre></td></tr></table></figure><h3 id="查看版本支持"><a href="#查看版本支持" class="headerlink" title="查看版本支持"></a>查看版本支持</h3><figure class="highlight llvm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs llvm">[root<span class="hljs-title">@k8s-master</span> yum.repos.d]# yum list kubelet kubeadm kubectl<br>Loaded plugins: fastestmirror<span class="hljs-punctuation">,</span> langpacks<br>Loading mirror speeds from cached hostfile<br> * base: mirrors.aliyun.com<br> * extras: mirrors.aliyun.com<br> * updates: mirrors.aliyun.com<br>Installed Packages<br>kubeadm.<span class="hljs-keyword">x</span><span class="hljs-number">86</span>_<span class="hljs-number">64</span>                                                        <span class="hljs-number">1.15</span>.<span class="hljs-number">3</span><span class="hljs-number">-0</span>                                                        <span class="hljs-title">@kubernetes</span><br>kubectl.<span class="hljs-keyword">x</span><span class="hljs-number">86</span>_<span class="hljs-number">64</span>                                                        <span class="hljs-number">1.15</span>.<span class="hljs-number">3</span><span class="hljs-number">-0</span>                                                        <span class="hljs-title">@kubernetes</span><br>kubelet.<span class="hljs-keyword">x</span><span class="hljs-number">86</span>_<span class="hljs-number">64</span>                                                        <span class="hljs-number">1.15</span>.<span class="hljs-number">3</span><span class="hljs-number">-0</span>                                                        <span class="hljs-title">@kubernetes</span><br>Available Packages<br>kubeadm.<span class="hljs-keyword">x</span><span class="hljs-number">86</span>_<span class="hljs-number">64</span>                                                        <span class="hljs-number">1.16</span>.<span class="hljs-number">0</span><span class="hljs-number">-0</span>                                                        kubernetes <br>kubectl.<span class="hljs-keyword">x</span><span class="hljs-number">86</span>_<span class="hljs-number">64</span>                                                        <span class="hljs-number">1.16</span>.<span class="hljs-number">0</span><span class="hljs-number">-0</span>                                                        kubernetes <br>kubelet.<span class="hljs-keyword">x</span><span class="hljs-number">86</span>_<span class="hljs-number">64</span>                                                        <span class="hljs-number">1.16</span>.<span class="hljs-number">0</span><span class="hljs-number">-0</span>                                                        kubernetes <br></code></pre></td></tr></table></figure><h3 id="升级kubelet-kubeadm-kubectl版本到1-16-0-每个节点都执行"><a href="#升级kubelet-kubeadm-kubectl版本到1-16-0-每个节点都执行" class="headerlink" title="升级kubelet kubeadm kubectl版本到1.16.0(每个节点都执行)"></a>升级<code>kubelet</code> <code>kubeadm</code> <code>kubectl</code>版本到<code>1.16.0</code>(每个节点都执行)</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@k8s-master ~]# yum install -y kubelet kubeadm kubectl<br><span class="hljs-meta">#</span><span class="bash"> systemctl daemon-reload &amp;&amp; systemctl restart kubelet &amp;&amp; systemctl status kubelet</span><br><br><br>[root@k8s-master ~]# kubectl version<br>Client Version: version.Info&#123;Major:&quot;1&quot;, Minor:&quot;16&quot;, GitVersion:&quot;v1.16.0&quot;, GitCommit:&quot;2bd9643cee5b3b3a5ecbd3af49d09018f0773c77&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2019-09-18T14:36:53Z&quot;, GoVersion:&quot;go1.12.9&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux/amd64&quot;&#125;<br>Server Version: version.Info&#123;Major:&quot;1&quot;, Minor:&quot;15&quot;, GitVersion:&quot;v1.15.3&quot;, GitCommit:&quot;2d3c76f9091b6bec110a5e63777c332469e0cba2&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2019-08-19T11:05:50Z&quot;, GoVersion:&quot;go1.12.9&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux/amd64&quot;&#125;<br>[root@k8s-master ~]# kubelet --version<br>Kubernetes v1.16.0<br>[root@k8s-master ~]# kubeadm version<br>kubeadm version: &amp;version.Info&#123;Major:&quot;1&quot;, Minor:&quot;16&quot;, GitVersion:&quot;v1.16.0&quot;, GitCommit:&quot;2bd9643cee5b3b3a5ecbd3af49d09018f0773c77&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2019-09-18T14:34:01Z&quot;, GoVersion:&quot;go1.12.9&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux/amd64&quot;&#125;<br><br></code></pre></td></tr></table></figure><h3 id="升级k8s集群"><a href="#升级k8s集群" class="headerlink" title="升级k8s集群"></a>升级<code>k8s</code>集群</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@k8s-master ~]# kubectl get nodes<br>NAME         STATUS   ROLES    AGE   VERSION<br>k8s-master   Ready    master   21d   v1.15.3<br>k8s-node1    Ready    &lt;none&gt;   20d   v1.15.3<br>k8s-node2    Ready    &lt;none&gt;   20d   v1.15.3<br>k8s-node3    Ready    &lt;none&gt;   20d   v1.15.3<br></code></pre></td></tr></table></figure><h3 id="升级集群"><a href="#升级集群" class="headerlink" title="升级集群"></a>升级集群</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@k8s-master ~]# kubeadm config print init-defaults &gt; kubeadm1.16.yaml<br><br>[root@k8s-master ~]# cat kubeadm1.16.yaml <br>apiVersion: kubeadm.k8s.io/v1beta2<br>bootstrapTokens:<br>- groups:<br>  - system:bootstrappers:kubeadm:default-node-token<br>  token: abcdef.0123456789abcdef<br>  ttl: 24h0m0s<br>  usages:<br>  - signing<br>  - authentication<br>kind: InitConfiguration<br>localAPIEndpoint:<br>  advertiseAddress: 10.122.17.200<br>  bindPort: 6443<br>nodeRegistration:<br>  criSocket: /var/run/dockershim.sock<br>  name: k8s-master<br>  taints:<br>  - effect: NoSchedule<br>    key: node-role.kubernetes.io/master<br>---<br>apiServer:<br>  timeoutForControlPlane: 4m0s<br>apiVersion: kubeadm.k8s.io/v1beta2<br>certificatesDir: /etc/kubernetes/pki<br>clusterName: kubernetes<br>controllerManager: &#123;&#125;<br>dns:<br>  type: CoreDNS<br>etcd:<br>  local:<br>    dataDir: /var/lib/etcd<br>imageRepository: registry.aliyuncs.com/google_containers<br>kind: ClusterConfiguration<br>kubernetesVersion: v1.16.0<br>networking:<br>  dnsDomain: cluster.local<br>  podSubnet: 192.168.0.0/16<br>  serviceSubnet: 10.96.0.0/12<br>scheduler: &#123;&#125;<br>---<br>apiVersion: kubeproxy.config.k8s.io/v1alpha1<br>kind: KubeProxyConfiguration<br>mode: ipvs<br><br></code></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@k8s-master ~]# kubeadm upgrade apply --config kubeadm1.16.yaml<br><br><br>.......<br>.......<br>.......<br>[addons] Applied essential addon: CoreDNS<br>[addons] Applied essential addon: kube-proxy<br><br>[upgrade/successful] SUCCESS! Your cluster was upgraded to &quot;v1.16.0&quot;. Enjoy!<br><br>[upgrade/kubelet] Now that your control plane is upgraded, please proceed with upgrading your kubelets if you haven&#x27;t already done so.<br><br></code></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@k8s-master ~]# kubectl version<br>Client Version: version.Info&#123;Major:&quot;1&quot;, Minor:&quot;16&quot;, GitVersion:&quot;v1.16.0&quot;, GitCommit:&quot;2bd9643cee5b3b3a5ecbd3af49d09018f0773c77&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2019-09-18T14:36:53Z&quot;, GoVersion:&quot;go1.12.9&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux/amd64&quot;&#125;<br>Server Version: version.Info&#123;Major:&quot;1&quot;, Minor:&quot;16&quot;, GitVersion:&quot;v1.16.0&quot;, GitCommit:&quot;2bd9643cee5b3b3a5ecbd3af49d09018f0773c77&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2019-09-18T14:27:17Z&quot;, GoVersion:&quot;go1.12.9&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux/amd64&quot;&#125;<br><br></code></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@k8s-node1 ~]# yum clean all<br>[root@k8s-node1 ~]# yum install -y kubelet kubeadm kubectl<br><br>systemctl daemon-reload &amp;&amp; systemctl restart kubelet &amp;&amp; systemctl status kubelet<br><br><br><br>[root@k8s-node1 ~]# kubectl version<br>Client Version: version.Info&#123;Major:&quot;1&quot;, Minor:&quot;16&quot;, GitVersion:&quot;v1.16.0&quot;, GitCommit:&quot;2bd9643cee5b3b3a5ecbd3af49d09018f0773c77&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2019-09-18T14:36:53Z&quot;, GoVersion:&quot;go1.12.9&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux/amd64&quot;&#125;<br>The connection to the server localhost:8080 was refused - did you specify the right host or port?<br>[root@k8s-node1 ~]# kubelet --version<br>Kubernetes v1.16.0<br>[root@k8s-node1 ~]# kubeadm version<br>kubeadm version: &amp;version.Info&#123;Major:&quot;1&quot;, Minor:&quot;16&quot;, GitVersion:&quot;v1.16.0&quot;, GitCommit:&quot;2bd9643cee5b3b3a5ecbd3af49d09018f0773c77&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2019-09-18T14:34:01Z&quot;, GoVersion:&quot;go1.12.9&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux/amd64&quot;&#125;<br><br></code></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@k8s-master ~]# kubectl version<br>Client Version: version.Info&#123;Major:&quot;1&quot;, Minor:&quot;16&quot;, GitVersion:&quot;v1.16.0&quot;, GitCommit:&quot;2bd9643cee5b3b3a5ecbd3af49d09018f0773c77&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2019-09-18T14:36:53Z&quot;, GoVersion:&quot;go1.12.9&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux/amd64&quot;&#125;<br>Server Version: version.Info&#123;Major:&quot;1&quot;, Minor:&quot;16&quot;, GitVersion:&quot;v1.16.0&quot;, GitCommit:&quot;2bd9643cee5b3b3a5ecbd3af49d09018f0773c77&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2019-09-18T14:27:17Z&quot;, GoVersion:&quot;go1.12.9&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux/amd64&quot;&#125;<br><br><br><br>[root@k8s-master ~]# kubectl get nodes<br>NAME         STATUS   ROLES    AGE   VERSION<br>k8s-master   Ready    master   21d   v1.16.0<br>k8s-node1    Ready    &lt;none&gt;   21d   v1.16.0<br>k8s-node2    Ready    &lt;none&gt;   21d   v1.16.0<br>k8s-node3    Ready    &lt;none&gt;   21d   v1.16.0<br><br><br>[root@k8s-master ~]# kubectl get pod -A<br>NAMESPACE       NAME                                        READY   STATUS    RESTARTS   AGE<br>default         curl-6bf6db5c4f-dqw9x                       1/1     Running   2          20d<br>ingress-nginx   nginx-ingress-controller-6956498fcf-jkzbs   1/1     Running   1          16d<br>kube-system     calico-kube-controllers-65b8787765-hx9cr    1/1     Running   1          21d<br>kube-system     calico-node-2rlsz                           1/1     Running   1          21d<br>kube-system     calico-node-5vz46                           1/1     Running   1          21d<br>kube-system     calico-node-6szsd                           1/1     Running   1          21d<br>kube-system     calico-node-s5nmr                           1/1     Running   1          21d<br>kube-system     calicoctl                                   1/1     Running   1          20d<br>kube-system     coredns-58cc8c89f4-cnl45                    1/1     Running   1          26m<br>kube-system     coredns-58cc8c89f4-nj8hr                    1/1     Running   1          9m4s<br>kube-system     etcd-k8s-master                             1/1     Running   0          6m35s<br>kube-system     kube-apiserver-k8s-master                   1/1     Running   0          6m30s<br>kube-system     kube-controller-manager-k8s-master          1/1     Running   0          6m35s<br>kube-system     kube-proxy-52mbf                            1/1     Running   1          26m<br>kube-system     kube-proxy-gf75r                            1/1     Running   1          25m<br>kube-system     kube-proxy-hbvjf                            1/1     Running   1          26m<br>kube-system     kube-proxy-zppdf                            1/1     Running   1          26m<br>kube-system     kube-scheduler-k8s-master                   1/1     Running   0          6m31s<br>kube-system     kubernetes-dashboard-5dc4c54b55-c7sv2       1/1     Running   1          20d<br><br><br>[root@k8s-master ~]# kubectl get pod<br>NAME                    READY   STATUS    RESTARTS   AGE<br>curl-6bf6db5c4f-dqw9x   1/1     Running   2          20d<br><br></code></pre></td></tr></table></figure><h3 id="验证cordns"><a href="#验证cordns" class="headerlink" title="验证cordns"></a>验证<code>cordns</code></h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@k8s-master ~]# kubectl get pod<br>NAME                    READY   STATUS    RESTARTS   AGE<br>curl-6bf6db5c4f-dqw9x   1/1     Running   2          20d<br>[root@k8s-master ~]# kubectl exec -it curl-6bf6db5c4f-dqw9x -- /bin/sh<br>/bin/sh: shopt: not found<br>[ root@curl-6bf6db5c4f-dqw9x:/ ]$ nslookup kubernetes.default<br>Server:    10.96.0.10<br>Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local<br><br>Name:      kubernetes.default<br>Address 1: 10.96.0.1 kubernetes.default.svc.cluster.local<br>[ root@curl-6bf6db5c4f-dqw9x:/ ]$ exit<br>[root@k8s-master ~]# <br></code></pre></td></tr></table></figure><h3 id="未解决的问题"><a href="#未解决的问题" class="headerlink" title="未解决的问题"></a>未解决的问题</h3><p><code>kubectl get cs</code>显示为<code>unknown</code><br>错误提示信息示例如下:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> kubectl get cs</span><br>NAME                 AGE<br>scheduler            &lt;unknown&gt;<br>controller-manager   &lt;unknown&gt;<br>etcd-0               &lt;unknown&gt;<br></code></pre></td></tr></table></figure><p>这个问题似乎对集群没有太大的影响，暂未解决，后续愿意确认之后会继续更新，如果有大神知道如何解决，请指点。</p><h3 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h3><p>1、 <a href="https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-upgrade/">https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-upgrade/</a><br>2、 <a href="http://www.sohu.com/a/342118551_198222">http://www.sohu.com/a/342118551_198222</a><br>3、 <a href="https://www.linuxidc.com/Linux/2019-09/160728.htm">https://www.linuxidc.com/Linux/2019-09/160728.htm</a></p>]]></content>
    
    
    <categories>
      
      <category>kubernetes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>kubeadm</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>在k8s上部署ingress-nginx并使用</title>
    <link href="/posts/%E5%9C%A8k8s%E4%B8%8A%E9%83%A8%E7%BD%B2ingress-nginx%E5%B9%B6%E4%BD%BF%E7%94%A8/"/>
    <url>/posts/%E5%9C%A8k8s%E4%B8%8A%E9%83%A8%E7%BD%B2ingress-nginx%E5%B9%B6%E4%BD%BF%E7%94%A8/</url>
    
    <content type="html"><![CDATA[<a id="more"></a><p>[TOC]</p><h3 id="部署nginx-ingress"><a href="#部署nginx-ingress" class="headerlink" title="部署nginx-ingress"></a>部署<code>nginx-ingress</code></h3><h3 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h3><p>准备一套<code>k8s</code>环境</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> kubectl get cs</span><br>NAME                 STATUS    MESSAGE             ERROR<br>controller-manager   Healthy   ok                  <br>scheduler            Healthy   ok                  <br>etcd-0               Healthy   &#123;&quot;health&quot;:&quot;true&quot;&#125;   <br><span class="hljs-meta">#</span><span class="bash"> kubectl cluster-info</span><br>Kubernetes master is running at https://10.122.17.200:6443<br>KubeDNS is running at https://10.122.17.200:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy<br><br>To further debug and diagnose cluster problems, use &#x27;kubectl cluster-info dump&#x27;.<br><span class="hljs-meta">#</span><span class="bash"> kubectl get nodes</span><br>NAME         STATUS   ROLES    AGE    VERSION<br>k8s-master   Ready    master   4d1h   v1.15.3<br>k8s-node1    Ready    &lt;none&gt;   4d1h   v1.15.3<br>k8s-node2    Ready    &lt;none&gt;   4d     v1.15.3<br>k8s-node3    Ready    &lt;none&gt;   4d     v1.15.3<br></code></pre></td></tr></table></figure><h3 id="下载安装nginx-ingress的yaml文件"><a href="#下载安装nginx-ingress的yaml文件" class="headerlink" title="下载安装nginx-ingress的yaml文件"></a>下载安装<code>nginx-ingress</code>的<code>yaml</code>文件</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/mandatory.yaml</span><br></code></pre></td></tr></table></figure><h3 id="设置nginx-ingress-controller调度节点"><a href="#设置nginx-ingress-controller调度节点" class="headerlink" title="设置nginx-ingress-controller调度节点"></a>设置<code>nginx-ingress-controller</code>调度节点</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> kubectl label nodes k8s-node1 hosts=nginx-ingress-controller</span><br><span class="hljs-meta">#</span><span class="bash"> kubectl get node -l hosts=nginx-ingress-controller --show-labels</span> <br>NAME        STATUS   ROLES    AGE    VERSION   LABELS<br>k8s-node1   Ready    &lt;none&gt;   4d6h   v1.15.3   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,hosts=nginx-ingress-controller,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-node1,kubernetes.io/os=linux<br></code></pre></td></tr></table></figure><h3 id="修改mandatory-yaml文件"><a href="#修改mandatory-yaml文件" class="headerlink" title="修改mandatory.yaml文件"></a>修改<code>mandatory.yaml</code>文件</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> vim mandatory.yaml</span><br>......<br>apiVersion: apps/v1<br>kind: Deployment<br>......<br>spec:<br>  replicas: 1<br>  selector:<br>    matchLabels:<br>      app.kubernetes.io/name: ingress-nginx<br>      app.kubernetes.io/part-of: ingress-nginx<br>  template:<br>    ......<br>    spec:<br>      hostNetwork: true    # 使用hostNetwork 模式<br>      nodeSelector:        # 使用nodeSelector<br>        hosts: nginx-ingress-controller   # 选择标签为hosts=nginx-ingress-controller节点<br>      serviceAccountName: nginx-ingress-serviceaccount<br>      containers:<br>        - name: nginx-ingress-controller<br>          image: lijiawang/nginx-ingress-controller:0.25.1   # 更换镜像源<br>  ......<br>......<br></code></pre></td></tr></table></figure><h3 id="执行"><a href="#执行" class="headerlink" title="执行"></a>执行</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> kubectl apply -f mandatory.yaml</span><br><span class="hljs-meta">#</span><span class="bash"> kubectl get pod -n ingress-nginx</span><br><span class="hljs-meta">#</span><span class="bash"> kubectl get pod -n ingress-nginx -o wide</span><br>NAME                                        READY   STATUS    RESTARTS   AGE   IP              NODE        NOMINATED NODE   READINESS GATES<br>nginx-ingress-controller-6956498fcf-jkzbs   1/1     Running   0          55s   10.122.17.204   k8s-node1   &lt;none&gt;           &lt;none&gt;<br></code></pre></td></tr></table></figure><p>可以看到<code>nginx-ingress-controller-6956498fcf-jkzbs</code> <code>POD</code> 调度到了打好标记的<code>k8s-node1</code>。</p><h3 id="登录nginx-ingress-controller节点验证"><a href="#登录nginx-ingress-controller节点验证" class="headerlink" title="登录nginx-ingress-controller节点验证"></a>登录<code>nginx-ingress-controller</code>节点验证</h3><p>因为<code>k8s-node1</code>为<code>nginx-ingress-controller</code>节点，所有登录<code>k8s-node1</code>节点即可</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> ssh k8s-node1</span><br>[root@k8s-node1 ~]# netstat -lntp|grep 80<br>tcp        0      0 0.0.0.0:80              0.0.0.0:*               LISTEN      5901/nginx: master  <br>tcp6       0      0 :::80                   :::*                    LISTEN      5901/nginx: master  <br>[root@k8s-node1 ~]# netstat -lntp|grep 443<br>tcp        0      0 0.0.0.0:443             0.0.0.0:*               LISTEN      5901/nginx: master  <br>tcp6       0      0 :::443                  :::*                    LISTEN      5901/nginx: master  <br>[root@k8s-node1 ~]# exit<br>logout<br>Connection to k8s-node1 closed.<br></code></pre></td></tr></table></figure><h3 id="部署应用"><a href="#部署应用" class="headerlink" title="部署应用"></a>部署应用</h3><p>部署测试应用</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-comment"># cat deploy-demon.yaml </span><br><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">Service</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">myapp</span><br>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">default</span><br><span class="hljs-attr">spec:</span><br>  <span class="hljs-attr">selector:</span><br>    <span class="hljs-attr">app:</span> <span class="hljs-string">myapp</span><br>    <span class="hljs-attr">release:</span> <span class="hljs-string">canary</span><br>  <span class="hljs-attr">ports:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">http</span><br>    <span class="hljs-attr">port:</span> <span class="hljs-number">80</span><br>    <span class="hljs-attr">targetPort:</span> <span class="hljs-number">80</span><br><span class="hljs-meta">---</span><br><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">apps/v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">Deployment</span><br><span class="hljs-attr">metadata:</span> <br>  <span class="hljs-attr">name:</span> <span class="hljs-string">myapp-deploy</span><br><span class="hljs-attr">spec:</span><br>  <span class="hljs-attr">replicas:</span> <span class="hljs-number">5</span><br>  <span class="hljs-attr">selector:</span> <br>    <span class="hljs-attr">matchLabels:</span><br>      <span class="hljs-attr">app:</span> <span class="hljs-string">myapp</span><br>      <span class="hljs-attr">release:</span> <span class="hljs-string">canary</span><br>  <span class="hljs-attr">template:</span><br>    <span class="hljs-attr">metadata:</span><br>      <span class="hljs-attr">labels:</span><br>        <span class="hljs-attr">app:</span> <span class="hljs-string">myapp</span><br>        <span class="hljs-attr">release:</span> <span class="hljs-string">canary</span><br>    <span class="hljs-attr">spec:</span><br>      <span class="hljs-attr">containers:</span><br>      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">myapp</span><br>        <span class="hljs-attr">image:</span> <span class="hljs-string">lijiawang/myapp:v2</span><br>        <span class="hljs-attr">ports:</span><br>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">httpd</span><br>          <span class="hljs-attr">containerPort:</span> <span class="hljs-number">80</span><br><span class="hljs-comment"># kubectl  apply -f deploy-demon.yaml </span><br><span class="hljs-string">service/myapp</span> <span class="hljs-string">created</span><br><span class="hljs-string">deployment.apps/myapp-deploy</span> <span class="hljs-string">created</span><br><span class="hljs-comment"># kubectl get pod </span><br><span class="hljs-string">NAME</span>                           <span class="hljs-string">READY</span>   <span class="hljs-string">STATUS</span>    <span class="hljs-string">RESTARTS</span>   <span class="hljs-string">AGE</span><br><span class="hljs-string">curl-6bf6db5c4f-dqw9x</span>          <span class="hljs-number">1</span><span class="hljs-string">/1</span>     <span class="hljs-string">Running</span>   <span class="hljs-number">1</span>          <span class="hljs-string">16d</span><br><span class="hljs-string">myapp-deploy-c69757d67-9bv4w</span>   <span class="hljs-number">1</span><span class="hljs-string">/1</span>     <span class="hljs-string">Running</span>   <span class="hljs-number">0</span>          <span class="hljs-string">38s</span><br><span class="hljs-string">myapp-deploy-c69757d67-j2kxc</span>   <span class="hljs-number">1</span><span class="hljs-string">/1</span>     <span class="hljs-string">Running</span>   <span class="hljs-number">0</span>          <span class="hljs-string">38s</span><br><span class="hljs-string">myapp-deploy-c69757d67-jgm5v</span>   <span class="hljs-number">1</span><span class="hljs-string">/1</span>     <span class="hljs-string">Running</span>   <span class="hljs-number">0</span>          <span class="hljs-string">38s</span><br><span class="hljs-string">myapp-deploy-c69757d67-n5rxw</span>   <span class="hljs-number">1</span><span class="hljs-string">/1</span>     <span class="hljs-string">Running</span>   <span class="hljs-number">0</span>          <span class="hljs-string">38s</span><br><span class="hljs-string">myapp-deploy-c69757d67-t4nxr</span>   <span class="hljs-number">1</span><span class="hljs-string">/1</span>     <span class="hljs-string">Running</span>   <span class="hljs-number">0</span>          <span class="hljs-string">38s</span><br><span class="hljs-comment"># kubectl get svc</span><br><span class="hljs-string">NAME</span>         <span class="hljs-string">TYPE</span>        <span class="hljs-string">CLUSTER-IP</span>       <span class="hljs-string">EXTERNAL-IP</span>   <span class="hljs-string">PORT(S)</span>   <span class="hljs-string">AGE</span><br><span class="hljs-string">kubernetes</span>   <span class="hljs-string">ClusterIP</span>   <span class="hljs-number">10.96</span><span class="hljs-number">.0</span><span class="hljs-number">.1</span>        <span class="hljs-string">&lt;none&gt;</span>        <span class="hljs-number">443</span><span class="hljs-string">/TCP</span>   <span class="hljs-string">17d</span><br><span class="hljs-string">myapp</span>        <span class="hljs-string">ClusterIP</span>   <span class="hljs-number">10.104</span><span class="hljs-number">.246</span><span class="hljs-number">.234</span>   <span class="hljs-string">&lt;none&gt;</span>        <span class="hljs-number">80</span><span class="hljs-string">/TCP</span>    <span class="hljs-string">2m58s</span><br></code></pre></td></tr></table></figure><h3 id="以ingress方式暴露应用"><a href="#以ingress方式暴露应用" class="headerlink" title="以ingress方式暴露应用"></a>以<code>ingress</code>方式暴露应用</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> cat ingress-myapp.yaml</span> <br>apiVersion: extensions/v1beta1<br>kind: Ingress<br>metadata:<br>  name: ingress-myapp<br>  namespace: default<br>  annotations: <br>    kubernetes.io/ingress.class: &quot;nginx&quot;<br>spec:<br>  rules:<br>  - host: myapp.lijw19.com   # 定义域名<br>    http:<br>      paths:<br>      - path: <br>        backend:<br>          serviceName: myapp   # 跟你要爆了的服务的svc名字相同<br>          servicePort: 80     # 要暴露的端口<br><span class="hljs-meta">#</span><span class="bash"> kubectl apply -f ingress-myapp.yaml</span> <br>ingress.extensions/ingress-myapp created<br><span class="hljs-meta">#</span><span class="bash"> kubectl get ingresses.</span><br>NAME            HOSTS              ADDRESS   PORTS   AGE<br>ingress-myapp   myapp.lijw19.com             80      10s<br></code></pre></td></tr></table></figure><p>本地<code>host</code>解析<br><code>windows</code>的<code>hosts</code>文件在<code>c:\windows\system32\drivers\etc</code>下<br>修改<code>host</code>文件，增加以下内容</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">10.122.17.204 myapp.lijw19.com <br></code></pre></td></tr></table></figure><h3 id="访问应用"><a href="#访问应用" class="headerlink" title="访问应用"></a>访问应用</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> `seq 10`; <span class="hljs-keyword">do</span> curl http://myapp.lijw19.com; <span class="hljs-keyword">done</span></span><br>Hello MyApp | Version: v2 | &lt;a href=&quot;hostname.html&quot;&gt;Pod Name&lt;/a&gt;<br>Hello MyApp | Version: v2 | &lt;a href=&quot;hostname.html&quot;&gt;Pod Name&lt;/a&gt;<br>Hello MyApp | Version: v2 | &lt;a href=&quot;hostname.html&quot;&gt;Pod Name&lt;/a&gt;<br>Hello MyApp | Version: v2 | &lt;a href=&quot;hostname.html&quot;&gt;Pod Name&lt;/a&gt;<br>Hello MyApp | Version: v2 | &lt;a href=&quot;hostname.html&quot;&gt;Pod Name&lt;/a&gt;<br>Hello MyApp | Version: v2 | &lt;a href=&quot;hostname.html&quot;&gt;Pod Name&lt;/a&gt;<br>Hello MyApp | Version: v2 | &lt;a href=&quot;hostname.html&quot;&gt;Pod Name&lt;/a&gt;<br>Hello MyApp | Version: v2 | &lt;a href=&quot;hostname.html&quot;&gt;Pod Name&lt;/a&gt;<br>Hello MyApp | Version: v2 | &lt;a href=&quot;hostname.html&quot;&gt;Pod Name&lt;/a&gt;<br>Hello MyApp | Version: v2 | &lt;a href=&quot;hostname.html&quot;&gt;Pod Name&lt;/a&gt;<br></code></pre></td></tr></table></figure><p>使用浏览器访问<code>http://myapp.lijw19.com</code><br><img src="https://ljw.howieli.cn/blog/2019-9-18/http0.png"></p><p><img src="https://ljw.howieli.cn/blog/2019-9-18/http1.png"><br>这里我们是使用的http访问的，那如果要使用</p><h3 id="使用https访问"><a href="#使用https访问" class="headerlink" title="使用https访问"></a>使用<code>https</code>访问</h3><p>创建证书</p><figure class="highlight vbnet"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs vbnet"># openssl genrsa -out tls.<span class="hljs-keyword">key</span> <span class="hljs-number">2048</span><br>Generating RSA <span class="hljs-keyword">private</span> <span class="hljs-keyword">key</span>, <span class="hljs-number">2048</span> bit <span class="hljs-type">long</span> modulus<br>......................+++<br>.....................................+++<br>e <span class="hljs-built_in">is</span> <span class="hljs-number">65537</span> (<span class="hljs-number">0</span>x10001)<br># ls -l tls.<span class="hljs-keyword">key</span> <br>-rw-r--r-- <span class="hljs-number">1</span> root root <span class="hljs-number">1675</span> Sep <span class="hljs-number">16</span> <span class="hljs-number">08</span>:<span class="hljs-number">43</span> tls.<span class="hljs-keyword">key</span><br># openssl req -<span class="hljs-built_in">new</span> -x509 -<span class="hljs-keyword">key</span> tls.<span class="hljs-keyword">key</span> -out tls.crt<br>You are about <span class="hljs-keyword">to</span> be asked <span class="hljs-keyword">to</span> enter information that will be incorporated<br><span class="hljs-keyword">into</span> your certificate request.<br>What you are about <span class="hljs-keyword">to</span> enter <span class="hljs-built_in">is</span> what <span class="hljs-built_in">is</span> called a Distinguished Name <span class="hljs-built_in">or</span> a DN.<br>There are quite a few fields but you can leave some blank<br><span class="hljs-keyword">For</span> some fields there will be a <span class="hljs-keyword">default</span> value,<br><span class="hljs-keyword">If</span> you enter <span class="hljs-comment">&#x27;.&#x27;, the field will be left blank.</span><br>-----<br>Country Name (<span class="hljs-number">2</span> letter code) [XX]:CN     <br>State <span class="hljs-built_in">or</span> Province Name (full name) []:lijw19<br>Locality Name (eg, city) [<span class="hljs-keyword">Default</span> City]:lijw19<br>Organization Name (eg, company) [<span class="hljs-keyword">Default</span> Company Ltd]:test<br>Organizational Unit Name (eg, section) []:test<br>Common Name (eg, your name <span class="hljs-built_in">or</span> your server<span class="hljs-comment">&#x27;s hostname) []:test.lijw19.com</span><br>Email Address []:<br>[root@k8s-master ~]# ls -l tls.*<br>-rw-r--r-- <span class="hljs-number">1</span> root root <span class="hljs-number">1318</span> Sep <span class="hljs-number">16</span> <span class="hljs-number">08</span>:<span class="hljs-number">46</span> tls.crt<br>-rw-r--r-- <span class="hljs-number">1</span> root root <span class="hljs-number">1675</span> Sep <span class="hljs-number">16</span> <span class="hljs-number">08</span>:<span class="hljs-number">43</span> tls.<span class="hljs-keyword">key</span><br><br></code></pre></td></tr></table></figure><h3 id="证书转成secret"><a href="#证书转成secret" class="headerlink" title="证书转成secret"></a>证书转成<code>secret</code></h3><p>将创建好的证书转成<code>secret</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> kubectl create secret tls lijw-ingress-secret --cert=tls.crt --key=tls.key</span><br>secret/lijw-ingress-secret created<br><span class="hljs-meta">#</span><span class="bash"> kubectl get secrets</span> <br>NAME                  TYPE                                  DATA   AGE<br>default-token-vmfwt   kubernetes.io/service-account-token   3      17d<br>lijw-ingress-secret   kubernetes.io/tls                     2      10s<br><br><br></code></pre></td></tr></table></figure><h3 id="创建https-ingress"><a href="#创建https-ingress" class="headerlink" title="创建https ingress"></a>创建<code>https</code> <code>ingress</code></h3><p>修改下<code>ingress-myapp-https.yaml</code>加入刚刚添加的<code>secret</code>，修改后的文件如下：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-comment"># cat ingress-myapp-https.yaml </span><br><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">extensions/v1beta1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">Ingress</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">ingress-myapp-https</span><br>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">default</span><br>  <span class="hljs-attr">annotations:</span> <br>    <span class="hljs-attr">kubernetes.io/ingress.class:</span> <span class="hljs-string">&quot;nginx&quot;</span><br><span class="hljs-attr">spec:</span><br>  <span class="hljs-attr">tls:</span>    <span class="hljs-comment"># 添加了tls这一段</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-attr">hosts:</span><br>    <span class="hljs-bullet">-</span> <span class="hljs-string">test.lijw19.com</span><br>    <span class="hljs-attr">secretName:</span> <span class="hljs-string">lijw-ingress-secret</span>  <span class="hljs-comment"># 到这结束</span><br>  <span class="hljs-attr">rules:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-attr">host:</span> <span class="hljs-string">test.lijw19.com</span><br>    <span class="hljs-attr">http:</span><br>      <span class="hljs-attr">paths:</span><br>      <span class="hljs-bullet">-</span> <span class="hljs-attr">path:</span> <br>        <span class="hljs-attr">backend:</span><br>          <span class="hljs-attr">serviceName:</span> <span class="hljs-string">myapp</span><br>          <span class="hljs-attr">servicePort:</span> <span class="hljs-number">80</span><br><br><br><br><br><span class="hljs-comment"># kubectl apply -f ingress-myapp-https.yaml </span><br><span class="hljs-string">ingress.extensions/ingress-myapp-https</span> <span class="hljs-string">created</span><br><span class="hljs-comment"># kubectl get ingresses ingress-myapp-https </span><br><span class="hljs-string">NAME</span>                  <span class="hljs-string">HOSTS</span>             <span class="hljs-string">ADDRESS</span>   <span class="hljs-string">PORTS</span>     <span class="hljs-string">AGE</span><br><span class="hljs-string">ingress-myapp-https</span>   <span class="hljs-string">test.lijw19.com</span>             <span class="hljs-number">80</span><span class="hljs-string">,</span> <span class="hljs-number">443</span>   <span class="hljs-string">13s</span><br></code></pre></td></tr></table></figure><h3 id="本地host解析"><a href="#本地host解析" class="headerlink" title="本地host解析"></a>本地<code>host</code>解析</h3><p><code>windows</code>的<code>hosts</code>文件在<code>c:\windows\system32\drivers\etc</code>下<br>修改<code>host</code>文件，增加以下内容</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">10.122.17.204 myapp.lijw19.com  test.lijw19.com<br></code></pre></td></tr></table></figure><p>在浏览器访问<code>https://test.lijw19.com</code><br><img src="https://ljw.howieli.cn/blog/2019-9-18/https0.png"></p><p><img src="https://ljw.howieli.cn/blog/2019-9-18/https1.png"></p>]]></content>
    
    
    <categories>
      
      <category>kubernetes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>kubernets</tag>
      
      <tag>ingress</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>使用kubeadm搭建kubernetes1.15</title>
    <link href="/posts/%E4%BD%BF%E7%94%A8kubeadm%E6%90%AD%E5%BB%BAkubernetes1.15/"/>
    <url>/posts/%E4%BD%BF%E7%94%A8kubeadm%E6%90%AD%E5%BB%BAkubernetes1.15/</url>
    
    <content type="html"><![CDATA[<h3 id="使用kubeadm方式安装kubernetes"><a href="#使用kubeadm方式安装kubernetes" class="headerlink" title="使用kubeadm方式安装kubernetes"></a>使用<code>kubeadm</code>方式安装<code>kubernetes</code></h3><a id="more"></a><p>[TOC]</p><h3 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h3><p>在所有节点执行，此环节所有的节点操作系统均匀<code>centos7.6</code></p><h4 id="1、配置hosts解析"><a href="#1、配置hosts解析" class="headerlink" title="1、配置hosts解析"></a>1、配置<code>hosts</code>解析</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> cat /etc/hosts</span><br>10.122.17.206 k8s-node3<br>10.122.17.205 k8s-node2<br>10.122.17.204 k8s-node1<br>10.122.17.200 k8s-master<br><span class="hljs-meta">#</span><span class="bash"> masker 主机到node免密</span><br></code></pre></td></tr></table></figure><h4 id="2、禁用防火墙"><a href="#2、禁用防火墙" class="headerlink" title="2、禁用防火墙"></a>2、禁用防火墙</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">systemctl stop firewalld<br>systemctl disable firewalld<br></code></pre></td></tr></table></figure><h4 id="3、禁用SELINUX"><a href="#3、禁用SELINUX" class="headerlink" title="3、禁用SELINUX"></a>3、禁用<code>SELINUX</code></h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> setenforce 0</span><br><span class="hljs-meta">#</span><span class="bash"> sed -i <span class="hljs-string">&#x27;s/SELINUX=enforcing/SELINUX=disabled/&#x27;</span> /etc/selinux/config</span><br><span class="hljs-meta">#</span><span class="bash"> cat /etc/selinux/config</span><br>SELINUX=disabled<br></code></pre></td></tr></table></figure><h4 id="4、创建-etc-sysctl-d-k8s-conf-添加如下内容-并使修改生效"><a href="#4、创建-etc-sysctl-d-k8s-conf-添加如下内容-并使修改生效" class="headerlink" title="4、创建/etc/sysctl.d/k8s.conf,添加如下内容,并使修改生效"></a>4、创建<code>/etc/sysctl.d/k8s.conf</code>,添加如下内容,并使修改生效</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> cat /etc/sysctl.d/k8s.conf</span> <br>net.bridge.bridge-nf-call-ip6tables = 1<br>net.bridge.bridge-nf-call-iptables = 1<br>net.ipv4.ip_forward = 1<br><span class="hljs-meta">#</span><span class="bash"> modprobe br_netfilter</span><br><span class="hljs-meta">#</span><span class="bash"> sysctl -p /etc/sysctl.d/k8s.conf</span><br></code></pre></td></tr></table></figure><h4 id="5、安装ipvs"><a href="#5、安装ipvs" class="headerlink" title="5、安装ipvs"></a>5、安装ipvs</h4><figure class="highlight gradle"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs gradle"># cat &gt; <span class="hljs-regexp">/etc/</span>sysconfig<span class="hljs-regexp">/modules/i</span>pvs.modules &lt;&lt;EOF<br>#!<span class="hljs-regexp">/bin/</span>bash<br>modprobe -- ip_vs<br>modprobe -- ip_vs_rr<br>modprobe -- ip_vs_wrr<br>modprobe -- ip_vs_sh<br>modprobe -- nf_conntrack_ipv4<br>EOF<br># chmod <span class="hljs-number">755</span> <span class="hljs-regexp">/etc/</span>sysconfig<span class="hljs-regexp">/modules/i</span>pvs.modules &amp;&amp; bash <span class="hljs-regexp">/etc/</span>sysconfig<span class="hljs-regexp">/modules/i</span>pvs.modules &amp;&amp; lsmod | <span class="hljs-keyword">grep</span> -e ip_vs -e nf_conntrack_ipv4<br></code></pre></td></tr></table></figure><p>上面脚本创建了的<code>/etc/sysconfig/modules/ipvs.modules</code>文件，保证在节点重启后能自动加载所需模块。使用<code>lsmod | grep -e ip_vs -e nf_conntrack_ipv4</code>命令查看是否已经正确加载所需的内核模块。<br>接下来还需要确保各个节点上已经安装了 ipset 软件包</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> yum install ipset -y</span><br></code></pre></td></tr></table></figure><p>为了便于查看 <code>ipvs</code> 的代理规则，最好安装一下管理工具 <code>ipvsadm</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> yum -y install ipvsadm -y</span><br></code></pre></td></tr></table></figure><h4 id="6、安装时间同步服务"><a href="#6、安装时间同步服务" class="headerlink" title="6、安装时间同步服务"></a>6、安装时间同步服务</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> yum install chrony -y</span><br><span class="hljs-meta">#</span><span class="bash"> systemctl <span class="hljs-built_in">enable</span> chronyd</span><br><span class="hljs-meta">#</span><span class="bash"> systemctl start chronyd</span><br><span class="hljs-meta">#</span><span class="bash"> chronyc sources</span><br></code></pre></td></tr></table></figure><h4 id="7、关闭swap分区"><a href="#7、关闭swap分区" class="headerlink" title="7、关闭swap分区"></a>7、关闭swap分区</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> swapoff -a</span><br></code></pre></td></tr></table></figure><p>修改<code>/etc/fstab</code>文件，注释掉<code>SWAP</code>的自动挂载，使用<code>free -m</code>确认<code>swap</code>已经关闭。<code>swappiness</code>参数调整，修改<code>/etc/sysctl.d/k8s.conf</code>添加下面一行：</p><h4 id="8、安装docker-ce"><a href="#8、安装docker-ce" class="headerlink" title="8、安装docker-ce"></a>8、安装<code>docker-ce</code></h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> yum install -y yum-utils device-mapper-persistent-data lvm2</span><br><span class="hljs-meta">#</span><span class="bash"> yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo</span><br><span class="hljs-meta">#</span><span class="bash"> yum makecache fast</span><br><span class="hljs-meta">#</span><span class="bash"> yum list docker-ce --showduplicates | sort -r</span><br><span class="hljs-meta">#</span><span class="bash"> 可以选择安装一个版本，比如我们这里安装最新版本</span><br><span class="hljs-meta">#</span><span class="bash"> yum install docker-ce-19.03.1-3.el7 -y</span><br></code></pre></td></tr></table></figure><h4 id="9、配置Docker镜像加速器"><a href="#9、配置Docker镜像加速器" class="headerlink" title="9、配置Docker镜像加速器"></a>9、配置<code>Docker</code>镜像加速器</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs shell">cat &lt;&lt;EOF &gt;/etc/docker/daemon.json<br>&#123;<br>  &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;],<br>  &quot;registry-mirrors&quot; : [<br>    &quot;https://ot2k4d59.mirror.aliyuncs.com/&quot;<br>  ]<br>&#125;<br>EOF<br></code></pre></td></tr></table></figure><h4 id="10、启动Docker"><a href="#10、启动Docker" class="headerlink" title="10、启动Docker"></a>10、启动<code>Docker</code></h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> systemctl daemon-reload</span><br><span class="hljs-meta">#</span><span class="bash"> systemctl restart docker</span><br><span class="hljs-meta">#</span><span class="bash"> systemctl <span class="hljs-built_in">enable</span> docker</span><br></code></pre></td></tr></table></figure><h4 id="11、安装kubeadm、kubelet、kubectl"><a href="#11、安装kubeadm、kubelet、kubectl" class="headerlink" title="11、安装kubeadm、kubelet、kubectl"></a>11、安装<code>kubeadm</code>、<code>kubelet</code>、<code>kubectl</code></h4><p>配置<code>yum</code>源</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs shell">cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo<br>[kubernetes]<br>name=Kubernetes<br>baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/<br>enabled=1<br>gpgcheck=1<br>repo_gpgcheck=1<br>gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg<br>EOF<br></code></pre></td></tr></table></figure><p>安装<code>kubeadm</code>、<code>kubelet</code>、<code>kubectl</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes -y</span><br><span class="hljs-meta">#</span><span class="bash"> kubeadm version</span><br>kubeadm version: &amp;version.Info&#123;Major:&quot;1&quot;, Minor:&quot;15&quot;, GitVersion:&quot;v1.15.3&quot;, GitCommit:&quot;2d3c76f9091b6bec110a5e63777c332469e0cba2&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2019-08-19T11:11:18Z&quot;, GoVersion:&quot;go1.12.9&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux/amd64&quot;&#125;<br></code></pre></td></tr></table></figure><p>将 <code>kubelet</code> 设置成开机启动</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> systemctl <span class="hljs-built_in">enable</span> kubelet.service</span><br></code></pre></td></tr></table></figure><p>到这里为止上面所有的操作都需要在所有节点执行配置</p><h3 id="初始化集群"><a href="#初始化集群" class="headerlink" title="初始化集群"></a>初始化集群</h3><h4 id="1、在master节点配置配置kubeadm初始文件"><a href="#1、在master节点配置配置kubeadm初始文件" class="headerlink" title="1、在master节点配置配置kubeadm初始文件"></a>1、在master节点配置配置kubeadm初始文件</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash">  kubeadm config <span class="hljs-built_in">print</span> init-defaults &gt; kubeadm.yaml</span><br><span class="hljs-meta">#</span><span class="bash"> cat kubeadm.yaml</span> <br>apiVersion: kubeadm.k8s.io/v1beta2<br>bootstrapTokens:<br>- groups:<br>  - system:bootstrappers:kubeadm:default-node-token<br>  token: abcdef.0123456789abcdef<br>  ttl: 24h0m0s<br>  usages:<br>  - signing<br>  - authentication<br>kind: InitConfiguration<br>localAPIEndpoint:<br>  advertiseAddress: 10.122.17.200   # apiserver 节点内网IP<br>  bindPort: 6443<br>nodeRegistration:<br>  criSocket: /var/run/dockershim.sock<br>  name: k8s-master<br>  taints:<br>  - effect: NoSchedule<br>    key: node-role.kubernetes.io/master<br>---<br>apiServer:<br>  timeoutForControlPlane: 4m0s<br>apiVersion: kubeadm.k8s.io/v1beta2<br>certificatesDir: /etc/kubernetes/pki<br>clusterName: kubernetes<br>controllerManager: &#123;&#125;<br>dns:<br>  type: CoreDNS<br>etcd:<br>  local:<br>    dataDir: /var/lib/etcd<br>imageRepository: registry.aliyuncs.com/google_containers    # 科学上网使用阿里registry源<br>kind: ClusterConfiguration<br>kubernetesVersion: v1.15.3    # k8s版本<br>networking:<br>  dnsDomain: cluster.local<br>  podSubnet: 192.168.0.0/16   # 我们这里是准备安装 calico 网络插件的，需要将 networking.podSubnet 设置为192.168.0.0/16<br>  serviceSubnet: 10.96.0.0/12<br>scheduler: &#123;&#125;<br>---<br>apiVersion: kubeproxy.config.k8s.io/v1alpha1<br>kind: KubeProxyConfiguration<br>mode: ipvs       # kube-proxy 模式<br></code></pre></td></tr></table></figure><h4 id="2、初始化master配置文件"><a href="#2、初始化master配置文件" class="headerlink" title="2、初始化master配置文件"></a>2、初始化<code>master</code>配置文件</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> kubeadm init --config kubeadm.yaml</span><br>Your Kubernetes control-plane has initialized successfully!<br><br>To start using your cluster, you need to run the following as a regular user:<br><br>  mkdir -p $HOME/.kube<br>  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config<br>  sudo chown $(id -u):$(id -g) $HOME/.kube/config<br><br>You should now deploy a pod network to the cluster.<br>Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at:<br>  https://kubernetes.io/docs/concepts/cluster-administration/addons/<br><br>Then you can join any number of worker nodes by running the following on each as root:<br><br>kubeadm join 10.122.17.200:6443 --token abcdef.0123456789abcdef \<br>    --discovery-token-ca-cert-hash sha256:44a4ed41c1f69a23139b5d1e2bb6ec158d835257c6aab0a7607a122e77f51c04 <br></code></pre></td></tr></table></figure><h4 id="3、拷贝-kubeconfig-文件"><a href="#3、拷贝-kubeconfig-文件" class="headerlink" title="3、拷贝 kubeconfig 文件"></a>3、拷贝 kubeconfig 文件</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> mkdir -p <span class="hljs-variable">$HOME</span>/.kube</span><br><span class="hljs-meta">#</span><span class="bash"> sudo cp -i /etc/kubernetes/admin.conf <span class="hljs-variable">$HOME</span>/.kube/config</span><br><span class="hljs-meta">#</span><span class="bash"> sudo chown $(id -u):$(id -g) <span class="hljs-variable">$HOME</span>/.kube/config</span><br></code></pre></td></tr></table></figure><h4 id="4、kubectl自动补全"><a href="#4、kubectl自动补全" class="headerlink" title="4、kubectl自动补全"></a>4、kubectl自动补全</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> yum install -y bash-completion</span><br><span class="hljs-meta">#</span><span class="bash"> locate bash_completion</span><br>/etc/bash_completion.d<br>/etc/bash_completion.d/git<br>/etc/bash_completion.d/iprutils<br>/etc/bash_completion.d/oscap<br>/etc/bash_completion.d/redefine_filedir<br>/etc/bash_completion.d/scl.bash<br>/etc/bash_completion.d/yum-utils.bash<br>/etc/profile.d/bash_completion.sh<br>/root/naftis/vendor/github.com/spf13/cobra/bash_completions.go<br>/usr/share/bash-completion/bash_completion<br><span class="hljs-meta">#</span><span class="bash"> <span class="hljs-built_in">source</span> /usr/share/bash-completion/bash_completion</span><br><span class="hljs-meta">#</span><span class="bash"> <span class="hljs-built_in">source</span> &lt;(kubectl completion bash)</span><br><span class="hljs-meta">#</span><span class="bash"> <span class="hljs-built_in">echo</span> <span class="hljs-string">&#x27;source /usr/share/bash-completion/bash_completion&#x27;</span> &gt;&gt; .bashrc</span> <br><span class="hljs-meta">#</span><span class="bash"> <span class="hljs-built_in">echo</span> <span class="hljs-string">&#x27;source &lt;(kubectl completion bash)&#x27;</span> &gt;&gt;.bashrc</span><br></code></pre></td></tr></table></figure><h3 id="添加计算节点"><a href="#添加计算节点" class="headerlink" title="添加计算节点"></a>添加计算节点</h3><h4 id="1、使用kubeadm添加计算节点"><a href="#1、使用kubeadm添加计算节点" class="headerlink" title="1、使用kubeadm添加计算节点"></a>1、使用<code>kubeadm</code>添加计算节点</h4><p><code>node</code>节点记住初始化集群上面的配置和操作要提前做好，必须安装好<code>kubeadm</code>、<code>kubelet</code>，执行以下命名</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> kubeadm join 10.122.17.200:6443 --token abcdef.0123456789abcdef \</span><br><span class="bash">    --discovery-token-ca-cert-hash sha256:44a4ed41c1f69a23139b5d1e2bb6ec158d835257c6aab0a7607a122e77f51c04</span> <br></code></pre></td></tr></table></figure><p>如果忘记了上面的<code>join</code>命令可以在<code>master</code>节点使用命令<code>kubeadm token create --print-join-command</code>重新获取,如下。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> kubeadm token create --print-join-command</span><br>kubeadm join 10.122.17.200:6443 --token dkb75n.dxkqjiv96eu7ky2s \<br>    --discovery-token-ca-cert-hash sha256:44a4ed41c1f69a23139b5d1e2bb6ec158d835257c6aab0a7607a122e77f51c04 <br></code></pre></td></tr></table></figure><h4 id="2、执行成功后可以在master节点运行kubectl-get-nodes命令"><a href="#2、执行成功后可以在master节点运行kubectl-get-nodes命令" class="headerlink" title="2、执行成功后可以在master节点运行kubectl get nodes命令"></a>2、执行成功后可以在<code>master</code>节点运行<code>kubectl get nodes</code>命令</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> kubectl get nodes</span><br>NAME         STATUS      ROLES    AGE     VERSION<br>k8s-master   NotReady    master   3m    v1.15.3<br>k8s-node1    NotReady    &lt;none&gt;   48m   v1.15.3<br>k8s-node2    NotReady    &lt;none&gt;   24m   v1.15.3<br>k8s-node3    NotReady    &lt;none&gt;   13m   v1.15.3<br></code></pre></td></tr></table></figure><p>可以看到是<code>NotReady</code>状态，这是因为还没有安装网络插件，接下来安装网络插件</p><h3 id="安装calio网络组建"><a href="#安装calio网络组建" class="headerlink" title="安装calio网络组建"></a>安装<code>calio</code>网络组建</h3><p>以下命令在控制节点执行</p><h4 id="1、安装calio网络插件"><a href="#1、安装calio网络插件" class="headerlink" title="1、安装calio网络插件"></a>1、安装<code>calio</code>网络插件</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash">  wget https://docs.projectcalico.org/v3.8/manifests/calico.yaml</span><br><span class="hljs-meta">#</span><span class="bash">  如果节点是多网卡，可以在资源清单文件中指定内网网卡</span><br><span class="hljs-meta">#</span><span class="bash"> vi calico.yaml</span><br>......<br>spec:<br>  containers:<br>  - env:<br>    - name: DATASTORE_TYPE<br>      value: kubernetes<br>    - name: IP_AUTODETECTION_METHOD  # DaemonSet中添加该环境变量<br>      value: interface=eth0    # 指定内网网卡<br>    - name: WAIT_FOR_DATASTORE<br>      value: &quot;true&quot;<br>......<br><span class="hljs-meta">#</span><span class="bash"> kubectl apply -f calico.yaml  <span class="hljs-comment"># 安装calico网络插件</span></span><br></code></pre></td></tr></table></figure><p>等待一段时间可以查看下<code>pod</code>的状态</p><h4 id="2、查看pod状态"><a href="#2、查看pod状态" class="headerlink" title="2、查看pod状态"></a>2、查看<code>pod</code>状态</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> kubectl get pods -n kube-system</span><br>NAME                                       READY   STATUS    RESTARTS   AGE<br>calico-kube-controllers-65b8787765-hx9cr   1/1     Running   0          3h50m<br>calico-node-2rlsz                          1/1     Running   0          3h18m<br>calico-node-5vz46                          1/1     Running   0          3h50m<br>calico-node-6szsd                          1/1     Running   0          3h50m<br>calico-node-s5nmr                          1/1     Running   0          3h29m<br>coredns-bccdc95cf-9jf8w                    1/1     Running   0          4h8m<br>coredns-bccdc95cf-gk27l                    1/1     Running   0          4h8m<br>etcd-k8s-master                            1/1     Running   0          4h7m<br>kube-apiserver-k8s-master                  1/1     Running   0          4h7m<br>kube-controller-manager-k8s-master         1/1     Running   0          4h7m<br>kube-proxy-5jbft                           1/1     Running   0          3h29m<br>kube-proxy-d9p8w                           1/1     Running   0          3h18m<br>kube-proxy-h6n88                           1/1     Running   0          3h54m<br>kube-proxy-mqgj7                           1/1     Running   0          4h8m<br>kube-scheduler-k8s-master                  1/1     Running   0          4h7m<br></code></pre></td></tr></table></figure><p>网络插件运行成功了，node 状态也正常了：</p><h4 id="3、查看node状态"><a href="#3、查看node状态" class="headerlink" title="3、查看node状态"></a>3、查看<code>node</code>状态</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> kubectl get nodes</span><br>NAME         STATUS   ROLES    AGE     VERSION<br>k8s-master   Ready    master   4h10m   v1.15.3<br>k8s-node1    Ready    &lt;none&gt;   3h55m   v1.15.3<br>k8s-node2    Ready    &lt;none&gt;   3h31m   v1.15.3<br>k8s-node3    Ready    &lt;none&gt;   3h20m   v1.15.3<br></code></pre></td></tr></table></figure><h4 id="4、安装calicoctl"><a href="#4、安装calicoctl" class="headerlink" title="4、安装calicoctl"></a>4、安装calicoctl</h4><p>将<code>calicoctl</code>安装为<code>Kubernetes pod</code><br>使用<code>Kubernetes API</code>数据存储区</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> kubectl apply -f https://docs.projectcalico.org/v3.8/manifests/calicoctl.yaml</span><br></code></pre></td></tr></table></figure><p>然后，您可以使用kubectl运行命令，如下所示</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> kubectl <span class="hljs-built_in">exec</span> -ti -n kube-system calicoctl -- /calicoctl get profiles -o wide</span><br>NAME                                                 LABELS   <br>kns.default                                          map[]    <br>kns.kube-node-lease                                  map[]    <br>kns.kube-public                                      map[]    <br>kns.kube-system                                      map[]    <br>ksa.default.default                                  map[]    <br>ksa.kube-node-lease.default                          map[]    <br>ksa.kube-public.default                              map[]    <br>ksa.kube-system.attachdetach-controller              map[]    <br>ksa.kube-system.bootstrap-signer                     map[]    <br>ksa.kube-system.calico-kube-controllers              map[]    <br>ksa.kube-system.calico-node                          map[]    <br>ksa.kube-system.calicoctl                            map[]    <br>ksa.kube-system.certificate-controller               map[]    <br>ksa.kube-system.clusterrole-aggregation-controller   map[]    <br>ksa.kube-system.coredns                              map[]    <br>ksa.kube-system.cronjob-controller                   map[]    <br>ksa.kube-system.daemon-set-controller                map[]    <br>ksa.kube-system.default                              map[]    <br>ksa.kube-system.deployment-controller                map[]    <br>ksa.kube-system.disruption-controller                map[]    <br>ksa.kube-system.endpoint-controller                  map[]    <br>ksa.kube-system.expand-controller                    map[]    <br>ksa.kube-system.generic-garbage-collector            map[]    <br>ksa.kube-system.horizontal-pod-autoscaler            map[]    <br>ksa.kube-system.job-controller                       map[]    <br>ksa.kube-system.kube-proxy                           map[]    <br>ksa.kube-system.namespace-controller                 map[]    <br>ksa.kube-system.node-controller                      map[]    <br>ksa.kube-system.persistent-volume-binder             map[]    <br>ksa.kube-system.pod-garbage-collector                map[]    <br>ksa.kube-system.pv-protection-controller             map[]    <br>ksa.kube-system.pvc-protection-controller            map[]    <br>ksa.kube-system.replicaset-controller                map[]    <br>ksa.kube-system.replication-controller               map[]    <br>ksa.kube-system.resourcequota-controller             map[]    <br>ksa.kube-system.service-account-controller           map[]    <br>ksa.kube-system.service-controller                   map[]    <br>ksa.kube-system.statefulset-controller               map[]    <br>ksa.kube-system.token-cleaner                        map[]    <br>ksa.kube-system.ttl-controller                       map[]    <br></code></pre></td></tr></table></figure><h4 id="5、设置别名"><a href="#5、设置别名" class="headerlink" title="5、设置别名"></a>5、设置别名</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> <span class="hljs-built_in">alias</span> calicoctl=<span class="hljs-string">&quot;kubectl exec -i -n kube-system calicoctl /calicoctl -- &quot;</span></span><br><span class="hljs-meta">#</span><span class="bash"> <span class="hljs-built_in">echo</span> <span class="hljs-string">&#x27;alias calicoctl=&quot;kubectl exec -i -n kube-system calicoctl /calicoctl -- &quot;&#x27;</span> &gt;&gt; .bashrc</span> <br><span class="hljs-meta">#</span><span class="bash"> tail -n 1 .bashrc</span> <br>alias calicoctl=&quot;kubectl exec -i -n kube-system calicoctl /calicoctl -- &quot;<br></code></pre></td></tr></table></figure><h4 id="6、使命别名执行命令"><a href="#6、使命别名执行命令" class="headerlink" title="6、使命别名执行命令"></a>6、使命别名执行命令</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> calicoctl get nodes</span><br>NAME         <br>k8s-master   <br>k8s-node1    <br>k8s-node2    <br>k8s-node3    <br></code></pre></td></tr></table></figure><h4 id="7、测试coredns"><a href="#7、测试coredns" class="headerlink" title="7、测试coredns"></a>7、测试coredns</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> kubectl run curl --image=radial/busyboxplus:curl -it</span> <br>kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.<br>If you don&#x27;t see a command prompt, try pressing enter.<br>[ root@curl-6bf6db5c4f-dqw9x:/ ]$ nslookup kubernetes.default<br>Server:    10.96.0.10<br>Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local<br><br>Name:      kubernetes.default<br>Address 1: 10.96.0.1 kubernetes.default.svc.cluster.local<br>[ root@curl-6bf6db5c4f-dqw9x:/ ]$ <br></code></pre></td></tr></table></figure><h3 id="安装Dashboard"><a href="#安装Dashboard" class="headerlink" title="安装Dashboard"></a>安装Dashboard</h3><h4 id="1、下载kubernetes-dashboard-yaml文件"><a href="#1、下载kubernetes-dashboard-yaml文件" class="headerlink" title="1、下载kubernetes-dashboard.yaml文件"></a>1、下载<code>kubernetes-dashboard.yaml</code>文件</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> wget https://raw.githubusercontent.com/kubernetes/dashboard/v1.10.1/src/deploy/recommended/kubernetes-dashboard.yaml</span><br></code></pre></td></tr></table></figure><h4 id="2、科学上网"><a href="#2、科学上网" class="headerlink" title="2、科学上网"></a>2、科学上网</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> vim kubernetes-dashboard.yaml</span><br><span class="hljs-meta">#</span><span class="bash"> 修改镜像名称</span><br>......<br>containers:<br>- args:<br>  - --auto-generate-certificates<br>  image: registry.aliyuncs.com/google_containers/kubernetes-dashboard-amd64:v1.10.1   # 修改成阿里源<br>  imagePullPolicy: IfNotPresent<br>......<br><span class="hljs-meta">#</span><span class="bash"> 修改Service为NodePort类型</span><br>......<br>selector:<br>  k8s-app: kubernetes-dashboard<br>type: NodePort<br>......<br></code></pre></td></tr></table></figure><h4 id="3、安装Dashboard"><a href="#3、安装Dashboard" class="headerlink" title="3、安装Dashboard"></a>3、安装<code>Dashboard</code></h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> kubectl apply -f kubernetes-dashboard.yaml</span><br><span class="hljs-meta">#</span><span class="bash"> kubectl get pods -n kube-system -l k8s-app=kubernetes-dashboard</span><br>NAME                                    READY   STATUS    RESTARTS   AGE<br>kubernetes-dashboard-5dc4c54b55-c7sv2   1/1     Running   0          62s<br>[root@k8s-master ~]# kubectl get pods,svc -n kube-system -l k8s-app=kubernetes-dashboard<br>NAME                                        READY   STATUS    RESTARTS   AGE<br>pod/kubernetes-dashboard-5dc4c54b55-c7sv2   1/1     Running   0          81s<br><br>NAME                           TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)         AGE<br>service/kubernetes-dashboard   NodePort   10.107.132.165   &lt;none&gt;        443:30114/TCP   80s<br></code></pre></td></tr></table></figure><h4 id="4、创建一个具有全局所有权限的用户来登录Dashboard"><a href="#4、创建一个具有全局所有权限的用户来登录Dashboard" class="headerlink" title="4、创建一个具有全局所有权限的用户来登录Dashboard"></a>4、创建一个具有全局所有权限的用户来登录<code>Dashboard</code></h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> vim rbac.yml</span><br>apiVersion: v1<br>kind: ServiceAccount<br>metadata:<br>  name: admin-user<br>  namespace: kube-system<br>---<br>apiVersion: rbac.authorization.k8s.io/v1<br>kind: ClusterRoleBinding<br>metadata:<br>  name: admin-user<br>roleRef:<br>  apiGroup: rbac.authorization.k8s.io<br>  kind: ClusterRole<br>  name: cluster-admin<br>subjects:<br>- kind: ServiceAccount<br>  name: admin-user<br>  namespace: kube-system<br></code></pre></td></tr></table></figure><p>直接创建</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> kubectl apply -f rbac.yml</span><br><span class="hljs-meta">#</span><span class="bash"> kubectl get secret -n kube-system|grep admin</span><br>admin-user-token-wpqck                           kubernetes.io/service-account-token   3      2m38s<br></code></pre></td></tr></table></figure><h4 id="6、获取登录token"><a href="#6、获取登录token" class="headerlink" title="6、获取登录token"></a>6、获取登录<code>token</code></h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk <span class="hljs-string">&#x27;&#123;print $1&#125;&#x27;</span>)|grep token:</span><br>token:      eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi11c2VyLXRva2VuLXdwcWNrIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImFkbWluLXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiIwZmUzNTRiYS1mODkxLTQ0M2UtOWUxMy0wNjE3NWEzNWI5NzMiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06YWRtaW4tdXNlciJ9.Xqq3V8MmFp5ssCw4Hhzxy9YmdiVxMLsMSXnMaklOkVjHiX4B4kVGED2IDmwTh3wEi2VqYxjkRVm-b_JY2AmCfoaPMUtb4R9OZj414volWgAH1CX4VbFB0o1yr6aP3i8SKmQlQfIweEqYUWDGB5zv3ml1u0WjUGyVJeiXY0GxDAtiROelsKaK4gHGsUlIFZ8izW0xmuGDnT9hvCuztqbGsb78bFmQfAL2dKNB12rLOg01EwF0g88jvtfHiv80yX18ulP46ZK0MFx4YhTrLRjq09rAqCSnMGWPHtBSogKUQhpDkwCze61ByKlG6KM_DhFE7elQkBEj-jwvygDwvn7tMQ<br></code></pre></td></tr></table></figure><h4 id="7、使用Firefox浏览器登录Dashboard"><a href="#7、使用Firefox浏览器登录Dashboard" class="headerlink" title="7、使用Firefox浏览器登录Dashboard"></a>7、使用<code>Firefox</code>浏览器登录<code>Dashboard</code></h4><p><img src="https://ljw.howieli.cn/blog/2019-9-1/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20190830152421.png"></p><p><img src="https://ljw.howieli.cn/blog/2019-9-1/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20190830152716.png"></p>]]></content>
    
    
    <categories>
      
      <category>kubernetes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>kubernetes</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>harbor搭建</title>
    <link href="/posts/harbor%E6%90%AD%E5%BB%BA/"/>
    <url>/posts/harbor%E6%90%AD%E5%BB%BA/</url>
    
    <content type="html"><![CDATA[<h3 id="harbor介绍"><a href="#harbor介绍" class="headerlink" title="harbor介绍"></a>harbor介绍</h3><p>Harbor是一个用于存储和分发Docker镜像的企业级Registry服务器，通过添加一些企业必需的功能特性，例如安全、标识和管理等，扩展了开源Docker Distribution。作为一个企业级私有Registry服务器，Harbor提供了更好的性能和安全。提升用户使用Registry构建和运行环境传输镜像的效率。Harbor支持安装在多个Registry节点的镜像资源复制，镜像全部保存在私有Registry中， 确保数据和知识产权在公司内部网络中管控。另外，Harbor也提供了高级的安全特性，诸如用户管理，访问控制和活动审计等。</p><a id="more"></a><h3 id="配置网络yum源"><a href="#配置网络yum源" class="headerlink" title="配置网络yum源"></a>配置网络yum源</h3><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs css"><span class="hljs-selector-tag">cd</span> /<span class="hljs-selector-tag">etc</span>/<span class="hljs-selector-tag">yum</span><span class="hljs-selector-class">.repos</span><span class="hljs-selector-class">.d</span><br><span class="hljs-selector-tag">rm</span> <span class="hljs-selector-tag">-rf</span> *<br><span class="hljs-selector-tag">cd</span><br><span class="hljs-selector-tag">curl</span> <span class="hljs-selector-tag">-o</span> /<span class="hljs-selector-tag">etc</span>/<span class="hljs-selector-tag">yum</span><span class="hljs-selector-class">.repos</span><span class="hljs-selector-class">.d</span>/<span class="hljs-selector-tag">CentOS-Base</span><span class="hljs-selector-class">.repo</span> <span class="hljs-selector-tag">http</span>://<span class="hljs-selector-tag">mirrors</span><span class="hljs-selector-class">.aliyun</span><span class="hljs-selector-class">.com</span>/<span class="hljs-selector-tag">repo</span>/<span class="hljs-selector-tag">Centos-7</span><span class="hljs-selector-class">.repo</span><br><span class="hljs-selector-tag">wget</span> <span class="hljs-selector-tag">-O</span> /<span class="hljs-selector-tag">etc</span>/<span class="hljs-selector-tag">yum</span><span class="hljs-selector-class">.repos</span><span class="hljs-selector-class">.d</span>/<span class="hljs-selector-tag">epel</span><span class="hljs-selector-class">.repo</span> <span class="hljs-selector-tag">http</span>://<span class="hljs-selector-tag">mirrors</span><span class="hljs-selector-class">.aliyun</span><span class="hljs-selector-class">.com</span>/<span class="hljs-selector-tag">repo</span>/<span class="hljs-selector-tag">epel-7</span><span class="hljs-selector-class">.repo</span><br></code></pre></td></tr></table></figure><h3 id="安装docker"><a href="#安装docker" class="headerlink" title="安装docker"></a>安装docker</h3><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs awk"><span class="hljs-comment"># 安装必要的一些系统工具</span><br>yum install -y yum-utils device-mapper-persistent-data lvm2<br><span class="hljs-comment"># 添加软件源信息</span><br>yum-config-manager --add-repo http:<span class="hljs-regexp">//mi</span>rrors.aliyun.com<span class="hljs-regexp">/docker-ce/</span>linux<span class="hljs-regexp">/centos/</span>docker-ce.repo<br><span class="hljs-comment"># 更新并安装 Docker-CE</span><br>yum makecache fast<br>yum -y install docker-ce<br><span class="hljs-comment"># 开启Docker服务</span><br>systemctl enable docker<br>systemctl restart docker<br>docker version<br></code></pre></td></tr></table></figure><h3 id="安装docker-compose"><a href="#安装docker-compose" class="headerlink" title="安装docker-compose"></a>安装docker-compose</h3><p>github上地址<a href="https://github.com/docker/compose">https://github.com/docker/compose</a></p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs awk">wget https:<span class="hljs-regexp">//gi</span>thub.com<span class="hljs-regexp">/docker/</span>compose<span class="hljs-regexp">/releases/</span>download<span class="hljs-regexp">/1.24.0/</span>docker-compose-Linux-x86_64<br>mv  docker-compose-Linux-x86_64 <span class="hljs-regexp">/usr/</span>local<span class="hljs-regexp">/bin/</span>docker-compose<br>chmod +x <span class="hljs-regexp">/usr/</span>local<span class="hljs-regexp">/bin/</span>docker-compose<br><span class="hljs-regexp">/usr/</span>local<span class="hljs-regexp">/bin/</span>docker-compose version<br></code></pre></td></tr></table></figure><h3 id="安装harbor"><a href="#安装harbor" class="headerlink" title="安装harbor"></a>安装harbor</h3><p>项目地址<a href="https://github.com/goharbor/harbor">https://github.com/goharbor/harbor</a></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs shell">wget https://storage.googleapis.com/harbor-releases/release-1.7.0/harbor-offline-installer-v1.7.5.tgz<br>tar  xf harbor-offline-installer-v1.7.5.tgz -C /usr/share/<br>[root@harbor ~]# ll /usr/share/harbor/<br>total 572840<br>drwxr-xr-x. 3 root root        23 May 14 16:08 common<br>-rw-r--r--. 1 root root       939 Apr  1 12:07 docker-compose.chartmuseum.yml<br>-rw-r--r--. 1 root root       975 Apr  1 12:07 docker-compose.clair.yml<br>-rw-r--r--. 1 root root      1434 Apr  1 12:07 docker-compose.notary.yml<br>-rw-r--r--. 1 root root      5608 Apr  1 12:07 docker-compose.yml<br>-rw-r--r--. 1 root root      8033 Apr  1 12:07 harbor.cfg<br>-rw-r--r--. 1 root root 585234819 Apr  1 12:08 harbor.v1.7.5.tar.gz<br>-rwxr-xr-x. 1 root root      5739 Apr  1 12:07 install.sh<br>-rw-r--r--. 1 root root     11347 Apr  1 12:07 LICENSE<br>-rw-r--r--. 1 root root   1263409 Apr  1 12:07 open_source_license<br>-rwxr-xr-x. 1 root root     36337 Apr  1 12:07 prepare<br></code></pre></td></tr></table></figure><h3 id="修改harbor配置文件harbor-cfg"><a href="#修改harbor配置文件harbor-cfg" class="headerlink" title="修改harbor配置文件harbor.cfg"></a>修改harbor配置文件harbor.cfg</h3><p>在刚才解压完的目录下有harbor配置文件: harbor.cfg, 这里有几处必要配置需要修改:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> hostname设置访问地址，可以使用ip、域名，不可以设置为127.0.0.1或localhost</span><br>hostname = 192.168.1.109<br><br><span class="hljs-meta">#</span><span class="bash"> 访问协议，默认是http，也可以设置https，如果设置https，则nginx ssl需要设置on</span><br>ui_url_protocol = http<br><br><span class="hljs-meta">#</span><span class="bash"> mysql数据库root用户默认密码root123，实际使用时修改下</span><br>db_password = root123<br><br><span class="hljs-meta">#</span><span class="bash"> 启动Harbor后，管理员UI登录的密码，默认是Harbor12345</span><br>harbor_admin_password = Harbor12345<br><br><span class="hljs-meta">#</span><span class="bash"> 是否开启自注册</span><br>self_registration = on<br><br><span class="hljs-meta">#</span><span class="bash"> Token有效时间，默认30分钟</span><br>token_expiration = 30<br></code></pre></td></tr></table></figure><h3 id="开始安装"><a href="#开始安装" class="headerlink" title="开始安装"></a>开始安装</h3><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs awk">cd <span class="hljs-regexp">/usr/</span>share<span class="hljs-regexp">/harbor/</span><br>./install.sh<br></code></pre></td></tr></table></figure><p><img src="https://ljw.howieli.cn/blog/2019-05-14/harborlogin.png"><br><img src="https://ljw.howieli.cn/blog/2019-05-14/harbor1.png"></p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs awk">cat <span class="hljs-regexp">/etc/</span>docker/daemon.json <br>&#123;<br>  <span class="hljs-string">&quot;insecure-registries&quot;</span>: [<span class="hljs-string">&quot;10.122.52.227&quot;</span>]<br>&#125;<br><span class="hljs-comment"># 重启docker服务</span><br>systemctl restart docker<br>systemctl restart docker<br></code></pre></td></tr></table></figure><h3 id="将镜像push到Harbor"><a href="#将镜像push到Harbor" class="headerlink" title="将镜像push到Harbor"></a>将镜像push到Harbor</h3><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">docker</span> images|grep centos<br><span class="hljs-attribute">centos</span>                          latest              <span class="hljs-number">9</span>f<span class="hljs-number">38484</span>d<span class="hljs-number">220</span>f        <span class="hljs-number">2</span> months ago        <span class="hljs-number">202</span>MB<br><br><span class="hljs-attribute">docker</span> tag centos:latest <span class="hljs-number">10.122.52.227</span>/library/centos:latest<br><br><span class="hljs-comment"># docker images|grep centos</span><br><span class="hljs-attribute">10</span>.<span class="hljs-number">122</span>.<span class="hljs-number">52</span>.<span class="hljs-number">227</span>/library/centos    latest              <span class="hljs-number">9</span>f<span class="hljs-number">38484</span>d<span class="hljs-number">220</span>f        <span class="hljs-number">2</span> months ago        <span class="hljs-number">202</span>MB<br><span class="hljs-attribute">centos</span>                          latest              <span class="hljs-number">9</span>f<span class="hljs-number">38484</span>d<span class="hljs-number">220</span>f        <span class="hljs-number">2</span> months ago        <span class="hljs-number">202</span>MB<br><br><span class="hljs-attribute">docker</span> push <span class="hljs-number">10.122.52.227</span>/library/centos:latest<br></code></pre></td></tr></table></figure><p><img src="https://ljw.howieli.cn/blog/2019-05-14/login.png"><br><img src="https://ljw.howieli.cn/blog/2019-05-14/harbor2.png"></p>]]></content>
    
    
    <categories>
      
      <category>docker</category>
      
    </categories>
    
    
    <tags>
      
      <tag>docker</tag>
      
      <tag>harbor</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>使用Kubeadm安装最新版Kubernetes 1.12</title>
    <link href="/posts/%E4%BD%BF%E7%94%A8Kubeadm%E5%AE%89%E8%A3%85%E6%9C%80%E6%96%B0%E7%89%88Kubernetes-1-12/"/>
    <url>/posts/%E4%BD%BF%E7%94%A8Kubeadm%E5%AE%89%E8%A3%85%E6%9C%80%E6%96%B0%E7%89%88Kubernetes-1-12/</url>
    
    <content type="html"><![CDATA[<h3 id="使用Kubeadm安装最新版Kubernetes-1-12"><a href="#使用Kubeadm安装最新版Kubernetes-1-12" class="headerlink" title="使用Kubeadm安装最新版Kubernetes 1.12"></a>使用Kubeadm安装最新版Kubernetes 1.12</h3><blockquote><p>kubeadm是Kubernetes官方提供的用于快速安装Kubernetes集群的工具，伴随Kubernetes每个版本的发布都会同步更新，kubeadm会对集群配置方面的一些实践做调整，通过实验kubeadm可以学习到Kubernetes官方在集群配置上一些新的最佳实践。</p></blockquote><a id="more"></a><h3 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h3><h4 id="系统配置"><a href="#系统配置" class="headerlink" title="系统配置"></a>系统配置</h4><p>我这有三台centos7.5主机如下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">172.20.16.243  k8smaster<br>172.20.14.51   node01<br>172.20.14.92   node02<br></code></pre></td></tr></table></figure><p>操作系统都为centos7.5</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> cat /etc/redhat-release</span> <br>CentOS Linux release 7.5.1804 (Core)<br><span class="hljs-meta">#</span><span class="bash"> uname -a</span><br>Linux k8smaster 3.10.0-862.14.4.el7.x86_64 #1 SMP Wed Sep 26 15:12:11 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux<br></code></pre></td></tr></table></figure><h4 id="环境说明"><a href="#环境说明" class="headerlink" title="环境说明"></a>环境说明</h4><p>测试使用的Kubernetes集群可由一个master主机及一个以上（建议至少两个）node主机组成，本测试环境将由k8smaster、node01和node02三个独立的主机组成，它们分别拥有8核心的CPU及8G的内存资源。</p><h4 id="初始环境"><a href="#初始环境" class="headerlink" title="初始环境"></a>初始环境</h4><p>1.</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> yum -y update</span><br></code></pre></td></tr></table></figure><p>2.借助于NTP服务设定各节点时间精确同步；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">yum -y install ntpd<br></code></pre></td></tr></table></figure><p>3.配置hosts解析;</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> cat /etc/hosts</span><br>127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4<br>::1         localhost localhost.localdomain localhost6 localhost6.localdomain6<br>172.20.16.243  k8smaster<br>172.20.14.51   node01<br>172.20.14.92   node02<br></code></pre></td></tr></table></figure><p>4.关闭各节点的iptables或firewalld服务，并确保它们被禁止随系统引导过程启动；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell">systemctl stop firewalld.service <br>systemctl stop iptables.service<br>systemctl disable firewalld.service<br>systemctl disable iptables.service<br></code></pre></td></tr></table></figure><p>5.各节点禁用SELinux；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">sed -i &#x27;s@^\(SELINUX=\).*@\1disabled@&#x27; /etc/sysconfig/selinux<br>setenforce 0<br></code></pre></td></tr></table></figure><p>6.禁用Swap设备;</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> swapoff -a</span><br>而后编辑/etc/fstab配置文件，注释用于挂载Swap设备的所有行;<br><span class="hljs-meta">#</span><span class="bash"> free -m</span> <br>              total        used        free      shared  buff/cache   available<br>Mem:           7805         750        6061          18         993        6655<br>Swap:             0           0           0<br></code></pre></td></tr></table></figure><p>7.若要使用ipvs模型的proxy，各节点还需要载入ipvs相关的各模块；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> cat /etc/sysconfig/modules/ipvs.modules</span><br><span class="hljs-meta">#</span><span class="bash">!/bin/bash</span><br>ipvs_modules_dir=&quot;/usr/lib/modules/$(uname -r)/kernel/net/netfilter/ipvs&quot;<br>for i in $(ls $ipvs_modules_dir | sed  -r &#x27;s@(.*).ko.xz@\1@&#x27;); do<br>    /sbin/modinfo -F filename $i  &amp;&gt; /dev/null<br>    if [ $? -eq 0 ]; then<br>        /sbin/modprobe $i<br>    fi<br>done<br><span class="hljs-meta">#</span><span class="bash"> chmod +x /etc/sysconfig/modules/ipvs.modules</span> <br><span class="hljs-meta">#</span><span class="bash"> bash /etc/sysconfig/modules/ipvs.modules</span> <br></code></pre></td></tr></table></figure><h3 id="安装k8s相关的软件包-在各节点执行"><a href="#安装k8s相关的软件包-在各节点执行" class="headerlink" title="安装k8s相关的软件包(在各节点执行)"></a>安装k8s相关的软件包(在各节点执行)</h3><h4 id="获取docker-ce的配置仓库文件"><a href="#获取docker-ce的配置仓库文件" class="headerlink" title="获取docker-ce的配置仓库文件"></a>获取docker-ce的配置仓库文件</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> wget https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo -O /etc/yum.repos.d/docker.repo</span><br></code></pre></td></tr></table></figure><h4 id="配置kubernetes的yum仓库"><a href="#配置kubernetes的yum仓库" class="headerlink" title="配置kubernetes的yum仓库"></a>配置kubernetes的yum仓库</h4><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs ini"><span class="hljs-comment"># cat /etc/yum.repos.d/kubernetes.repo </span><br><span class="hljs-section">[kubernetes]</span><br><span class="hljs-attr">name</span>=Kubernetes<br><span class="hljs-attr">baseurl</span>=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x<span class="hljs-number">86_64</span>/<br><span class="hljs-attr">gpgcheck</span>=<span class="hljs-number">1</span><br><span class="hljs-attr">gpgkey</span>=https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg<br><span class="hljs-attr">enabled</span>=<span class="hljs-number">1</span><br></code></pre></td></tr></table></figure><h4 id="安装相关软件包"><a href="#安装相关软件包" class="headerlink" title="安装相关软件包"></a>安装相关软件包</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> yum install -y --<span class="hljs-built_in">setopt</span>=obsoletes=0 docker-ce-17.03.2.ce docker-ce-selinux-17.03.2.ce</span><br><span class="hljs-meta">#</span><span class="bash"> yum install kubelet kubeadm kubectl</span><br></code></pre></td></tr></table></figure><h3 id="配置并启动docker服务-在各节点执行"><a href="#配置并启动docker服务-在各节点执行" class="headerlink" title="配置并启动docker服务(在各节点执行)"></a>配置并启动docker服务(在各节点执行)</h3><p>1.编辑systemctl的Docker启动文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">sed -i &quot;13i ExecStartPost=/usr/sbin/iptables -P FORWARD ACCEPT&quot; /usr/lib/systemd/system/docker.service<br>systemctl daemon-reload<br>systemctl start docker.service<br></code></pre></td></tr></table></figure><p>2.配置Docker源</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell">mkdir -p /etc/docker<br>echo -e &#x27;&#123;\n&quot;insecure-registries&quot;:[&quot;k8s.gcr.io&quot;, &quot;gcr.io&quot;, &quot;quay.io&quot;]\n&#125;&#x27; &gt;/etc/docker/daemon.json<br>systemctl daemon-reload<br>systemctl restart docker<br></code></pre></td></tr></table></figure><p>3.设置docker和kubelet服务开机自启动</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> systemctl <span class="hljs-built_in">enable</span> docker kubelet</span><br></code></pre></td></tr></table></figure><h3 id="初始化master节点"><a href="#初始化master节点" class="headerlink" title="初始化master节点"></a>初始化master节点</h3><p>在运行初始化命令之前先运行如下命令单独获取相关的镜像文件，而后再运行后面的kubeadm init命令，以便于观察到镜像文件的下载过程(可选择)。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> kubeadm config images pull</span><br></code></pre></td></tr></table></figure><p>使用kubeadm初始化集群，选择k8smaster作为Master Node，在k8smaster上执行下面的命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">kubeadm init --kubernetes-version=v1.12.1 \<br>    --pod-network-cidr=10.244.0.0/16 \<br>--service-cidr=10.96.0.0/12 <br></code></pre></td></tr></table></figure><p>命令中的各选项简单说明如下：<br>    (1) –kubernetes-version选项的版本号用于指定要部署的Kubenretes程序版本，它需要与当前的kubeadm支持的版本保持一致；<br>    (2) –pod-network-cidr选项用于指定分Pod分配使用的网络地址，它通常应该与要部署使用的网络插件（例如flannel、calico等）的默认设定保持一致，10.244.0.0/16是flannel默认使用的网络；<br>    (3) –service-cidr用于指定为Service分配使用的网络地址，它由kubernetes管理，默认即为10.96.0.0/12.<br>命令运行结束后，请记录最后的kubeadm join命令输出的最后提示的操作步骤,这是初始化node节点要用到的。</p><h3 id="初始化kubectl"><a href="#初始化kubectl" class="headerlink" title="初始化kubectl"></a>初始化kubectl</h3><p>kubectl是kube-apiserver的命令行客户端程序，实现了除系统部署之外的几乎全部的管理操作，是kubernetes管理员使用最多的命令之一。kubectl需经由API server认证及授权后方能执行相应的管理操作，kubeadm部署的集群为其生成了一个具有管理员权限的认证配置文件/etc/kubernetes/admin.conf，它可由kubectl通过默认的“$HOME/.kube/config”的路径进行加载。当然，用户也可在kubectl命令上使用–kubeconfig选项指定一个别的位置<br>复制认证为Kubernetes系统管理员的配置文件至目标用户（例如当前用户root）的家目录下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">mkdir -p $HOME/.kube<br>sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config<br>sudo chown $(id -u):$(id -g) $HOME/.kube/config<br></code></pre></td></tr></table></figure><h3 id="添加flannel网络插件"><a href="#添加flannel网络插件" class="headerlink" title="添加flannel网络插件"></a>添加flannel网络插件</h3><p>Kubernetes系统上Pod网络的实现依赖于第三方插件进行，这类插件有近数十种之多，较为著名的有flannel、calico、canal和kube-router等，简单易用的实现是为CoreOS提供的flannel项目。下面的命令用于在线部署flannel至Kubernetes系统之上：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml</span><br></code></pre></td></tr></table></figure><p>检查flannel的pod状态</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash">  kubectl get pods -n kube-system -l app=flannel</span><br>NAME                          READY   STATUS    RESTARTS   AGE<br>kube-flannel-ds-amd64-95wzl   1/1     Running   1          22h<br></code></pre></td></tr></table></figure><p>验证master节点是否就绪</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> kubectl get nodes</span><br>NAME        STATUS   ROLES    AGE   VERSION<br>k8smaster   Ready    master   22h   v1.12.1<br></code></pre></td></tr></table></figure><h3 id="添加node节点到集群中-在node01和node02上执行"><a href="#添加node节点到集群中-在node01和node02上执行" class="headerlink" title="添加node节点到集群中(在node01和node02上执行)"></a>添加node节点到集群中(在node01和node02上执行)</h3><p>添加node节点</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> kubeadm join 172.20.16.243:6443 --token 7l22o0.x1rwp6ffh30tppyz --discovery-token-ca-cert-hash sha256:20efefe8d3ef236219dab4c09e26d933fd9b8754099467babaa668076a230f49</span><br></code></pre></td></tr></table></figure><p>检查node节点是否加入集群(在master节点上执行)</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> kubectl get nodes</span><br>NAME        STATUS   ROLES    AGE   VERSION<br>k8smaster   Ready    master   22h   v1.12.1<br>node01      Ready    &lt;none&gt;   20h   v1.12.1<br>node02      Ready    &lt;none&gt;   20h   v1.12.1<br></code></pre></td></tr></table></figure><h3 id="如何从集群中移除Node"><a href="#如何从集群中移除Node" class="headerlink" title="如何从集群中移除Node"></a>如何从集群中移除Node</h3><p>如果需要从集群中移除node02这个Node执行下面的命令：<br>在master节点上执行：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> kubectl drain node02 --delete-local-data --force --ignore-daemonsets</span><br><span class="hljs-meta">#</span><span class="bash"> kubectl delete node node02</span><br></code></pre></td></tr></table></figure><p>在node02上执行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs shell">kubeadm reset<br>ifconfig cni0 down<br>ip link delete cni0<br>ifconfig flannel.1 down<br>ip link delete flannel.1<br>rm -rf /var/lib/cni/<br></code></pre></td></tr></table></figure><h3 id="查看k8s相关的服务是否正常"><a href="#查看k8s相关的服务是否正常" class="headerlink" title="查看k8s相关的服务是否正常"></a>查看k8s相关的服务是否正常</h3><p>几乎所有的kubernetes组件本身也在pod里运行，执行以下命令。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> kubectl get pod --all-namespaces -o wide</span><br>NAMESPACE     NAME                                READY   STATUS    RESTARTS   AGE   IP              NODE        NOMINATED NODE<br>default       myapp-69587444dd-6jdr5              1/1     Running   1          20h   10.244.2.3      node02      &lt;none&gt;<br>kube-system   coredns-576cbf47c7-5wfbp            1/1     Running   30         23h   10.244.0.10     k8smaster   &lt;none&gt;<br>kube-system   coredns-576cbf47c7-qfb87            1/1     Running   30         23h   10.244.0.11     k8smaster   &lt;none&gt;<br>kube-system   etcd-k8smaster                      1/1     Running   1          23h   172.20.16.243   k8smaster   &lt;none&gt;<br>kube-system   kube-apiserver-k8smaster            1/1     Running   1          23h   172.20.16.243   k8smaster   &lt;none&gt;<br>kube-system   kube-controller-manager-k8smaster   1/1     Running   1          23h   172.20.16.243   k8smaster   &lt;none&gt;<br>kube-system   kube-flannel-ds-amd64-95wzl         1/1     Running   1          23h   172.20.16.243   k8smaster   &lt;none&gt;<br>kube-system   kube-flannel-ds-amd64-lc6x7         1/1     Running   1          20h   172.20.14.92    node02      &lt;none&gt;<br>kube-system   kube-flannel-ds-amd64-xdvxf         1/1     Running   1          20h   172.20.14.51    node01      &lt;none&gt;<br>kube-system   kube-proxy-9cq4d                    1/1     Running   1          23h   172.20.16.243   k8smaster   &lt;none&gt;<br>kube-system   kube-proxy-wczgd                    1/1     Running   1          20h   172.20.14.51    node01      &lt;none&gt;<br>kube-system   kube-proxy-xnxxm                    1/1     Running   1          20h   172.20.14.92    node02      &lt;none&gt;<br>kube-system   kube-scheduler-k8smaster            1/1     Running   1          23h   172.20.16.243   k8smaster   &lt;none&gt;<br>kube-system   tiller-deploy-6f6fd74b68-qndkl      1/1     Running   0          19m   10.244.1.2      node01      &lt;none&gt;<br></code></pre></td></tr></table></figure><p>kubelet是唯一没有以容器形式运行的kubernetes组件，在centos7中可以通过systemd服务运行：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> systemctl status kubelet.service</span><br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>kubernetes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>kubernetes</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>kolla部署openstack对接华为san存储</title>
    <link href="/posts/kolla%E9%83%A8%E7%BD%B2openstack%E5%AF%B9%E6%8E%A5%E5%8D%8E%E4%B8%BAsan%E5%AD%98%E5%82%A8/"/>
    <url>/posts/kolla%E9%83%A8%E7%BD%B2openstack%E5%AF%B9%E6%8E%A5%E5%8D%8E%E4%B8%BAsan%E5%AD%98%E5%82%A8/</url>
    
    <content type="html"><![CDATA[<h3 id="kolla部署openstack对接华为san存储"><a href="#kolla部署openstack对接华为san存储" class="headerlink" title="kolla部署openstack对接华为san存储"></a>kolla部署openstack对接华为san存储</h3><p>openstack O版对接华为SAN存储（kolla部署模式）</p><a id="more"></a><h3 id="修改globals-yml文件"><a href="#修改globals-yml文件" class="headerlink" title="修改globals.yml文件"></a>修改globals.yml文件</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> grep enable_multipathd /etc/kolla/globals.yml</span> <br>enable_multipathd: &quot;yes&quot;<br></code></pre></td></tr></table></figure><h3 id="修改multinode文件"><a href="#修改multinode文件" class="headerlink" title="修改multinode文件"></a>修改multinode文件</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> vim multinode</span><br><span class="hljs-meta">#</span><span class="bash"> Multipathd</span><br>[multipathd:children]<br>control<br>compute<br></code></pre></td></tr></table></figure><h3 id="修改config下的cinder-conf文件"><a href="#修改config下的cinder-conf文件" class="headerlink" title="修改config下的cinder.conf文件"></a>修改config下的cinder.conf文件</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> cat /etc/kolla/config/cinder.conf</span> <br>[DEFAULT]<br>enabled_backends = rbd-1,18000V1_FC,18000V1_FC_SAS01,18000V1_FC_SAS02<br>host = drkmcontrol<br><br>[18000V1_FC]<br>volume_driver = cinder.volume.drivers.huawei.huawei_driver.HuaweiFCDriver<br>cinder_huawei_conf_file = /etc/cinder/cinder_huawei_conf.xml<br>volume_backend_name = 18000V1_FC<br><br>[18000V1_FC_SAS01]<br>volume_driver = cinder.volume.drivers.huawei.huawei_driver.HuaweiFCDriver<br>cinder_huawei_conf_file = /etc/cinder/cinder_huawei_sas01_conf.xml<br>volume_backend_name = 18000V1_FC_SAS01<br><br>[18000V1_FC_SAS02]<br>volume_driver = cinder.volume.drivers.huawei.huawei_driver.HuaweiFCDriver<br>cinder_huawei_conf_file = /etc/cinder/cinder_huawei_sas02_conf.xml<br>volume_backend_name = 18000V1_FC_SAS02<br></code></pre></td></tr></table></figure><h3 id="编写xml文件"><a href="#编写xml文件" class="headerlink" title="编写xml文件"></a>编写xml文件</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> cat /etc/kolla/cinder-volume/cinder_huawei_sas01_conf.xml</span> <br>&lt;?xml version=&#x27;1.0&#x27; encoding=&#x27;UTF-8&#x27;?&gt;<br>&lt;config&gt;<br>    &lt;Storage&gt;<br>        &lt;Product&gt;18000&lt;/Product&gt;<br>        &lt;Protocol&gt;FC&lt;/Protocol&gt;<br>        &lt;RestURL&gt;https://10.15.188.100:8088/deviceManager/rest/&lt;/RestURL&gt;<br>        &lt;UserName&gt;openstack&lt;/UserName&gt;<br>        &lt;UserPassword&gt;VFR$5tgb3$&lt;/UserPassword&gt;<br>    &lt;/Storage&gt;<br>    &lt;LUN&gt; <br>        &lt;StoragePool&gt;StoragePool_SMB0_1;StoragePool_SMB0_2;StoragePool_SMB0_3;StoragePool_SMB0_4&lt;/StoragePool&gt;<br>    &lt;/LUN&gt;<br>&lt;/config&gt;<br><br><br><span class="hljs-meta">#</span><span class="bash"> cat /etc/kolla/cinder-volume/cinder_huawei_sas02_conf.xml</span> <br>&lt;?xml version=&#x27;1.0&#x27; encoding=&#x27;UTF-8&#x27;?&gt;<br>&lt;config&gt;<br>    &lt;Storage&gt;<br>        &lt;Product&gt;18000&lt;/Product&gt;<br>        &lt;Protocol&gt;FC&lt;/Protocol&gt;<br>        &lt;RestURL&gt;https://10.15.188.100:8088/deviceManager/rest/&lt;/RestURL&gt;<br>        &lt;UserName&gt;openstack&lt;/UserName&gt;<br>        &lt;UserPassword&gt;VFR$5tgb3$&lt;/UserPassword&gt;<br>    &lt;/Storage&gt;<br>    &lt;LUN&gt; <br>        &lt;StoragePool&gt;StoragePool_SMB0_5;StoragePool_SMB1_1;StoragePool_SMB1_2&lt;/StoragePool&gt;<br>    &lt;/LUN&gt;<br>&lt;/config&gt;<br><br><br><span class="hljs-meta">#</span><span class="bash"> cat /etc/kolla/cinder-volume/cinder_huawei_conf.xml</span> <br>&lt;?xml version=&#x27;1.0&#x27; encoding=&#x27;UTF-8&#x27;?&gt;<br>&lt;config&gt;<br>    &lt;Storage&gt;<br>        &lt;Product&gt;18000&lt;/Product&gt;<br>        &lt;Protocol&gt;FC&lt;/Protocol&gt;<br>        &lt;RestURL&gt;https://10.15.188.100:8088/deviceManager/rest/&lt;/RestURL&gt;<br>        &lt;UserName&gt;openstack&lt;/UserName&gt;<br>        &lt;UserPassword&gt;VFR$5tgb3$&lt;/UserPassword&gt;<br>    &lt;/Storage&gt;<br>    &lt;LUN&gt; <br>        &lt;StoragePool&gt;StoragePool_SMB0_8&lt;/StoragePool&gt;<br>    &lt;/LUN&gt;<br>&lt;/config&gt;<br></code></pre></td></tr></table></figure><h3 id="修改cinder-volume-json-j2-增加以下内容"><a href="#修改cinder-volume-json-j2-增加以下内容" class="headerlink" title="修改cinder-volume.json.j2,增加以下内容"></a>修改cinder-volume.json.j2,增加以下内容</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> vim /usr/share/kolla-ansible/ansible/roles/cinder/templates/cinder-volume.json.j2</span><br>        &#123;<br>            &quot;source&quot;: &quot;&#123;&#123; container_config_directory &#125;&#125;/cinder_*_conf.xml&quot;,<br>            &quot;dest&quot;: &quot;/etc/cinder/&quot;,<br>            &quot;owner&quot;: &quot;cinder&quot;,<br>            &quot;perm&quot;: &quot;0600&quot;<br>        &#125;,<br></code></pre></td></tr></table></figure><h3 id="利用ansible下发配置文件"><a href="#利用ansible下发配置文件" class="headerlink" title="利用ansible下发配置文件"></a>利用ansible下发配置文件</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> ansible -i multinode control -m copy -a <span class="hljs-string">&quot;src=/etc/kolla/cinder-volume/cinder_huawei_* dest=/etc/kolla/cinder-volume/&quot;</span></span><br></code></pre></td></tr></table></figure><h3 id="部署cinder"><a href="#部署cinder" class="headerlink" title="部署cinder"></a>部署cinder</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> kolla-ansible -i multinode reconfigure -t cinder</span><br><br><span class="hljs-meta">#</span><span class="bash"> cinder service-list</span><br>+------------------+------------------------------+------+---------+-------+----------------------------+-----------------+<br>| Binary           | Host                         | Zone | Status  | State | Updated_at                 | Disabled Reason |<br>+------------------+------------------------------+------+---------+-------+----------------------------+-----------------+<br>| cinder-backup    | drkmcontrol                  | nova | enabled | up    | 2018-06-27T02:13:58.000000 | -               |<br>| cinder-scheduler | drkmcontrol                  | nova | enabled | up    | 2018-06-27T02:13:52.000000 | -               |<br>| cinder-volume    | drkmcontrol@18000V1_FC       | nova | enabled | up    | 2018-06-27T02:13:58.000000 | -               |<br>| cinder-volume    | drkmcontrol@18000V1_FC_SAS01 | nova | enabled | up    | 2018-06-27T02:13:51.000000 | -               |<br>| cinder-volume    | drkmcontrol@18000V1_FC_SAS02 | nova | enabled | up    | 2018-06-27T02:13:51.000000 | -               |<br>| cinder-volume    | drkmcontrol@rbd-1            | nova | enabled | up    | 2018-06-27T02:13:56.000000 | -               |<br>+------------------+------------------------------+------+---------+-------+----------------------------+-----------------+<br></code></pre></td></tr></table></figure><h3 id="查看HBA卡ww暗号"><a href="#查看HBA卡ww暗号" class="headerlink" title="查看HBA卡ww暗号"></a>查看HBA卡ww暗号</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@drkm01070104u config]# cat /sys/class/fc_host/host2/port_id <br>0x3f0000<br>[root@drkm01070104u config]# cat /sys/class/fc_host/host1/port_id <br>0x350000<br><br></code></pre></td></tr></table></figure><h3 id="配置cinder多后端"><a href="#配置cinder多后端" class="headerlink" title="配置cinder多后端"></a>配置cinder多后端</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> cinder type-create Ceph</span><br><br><span class="hljs-meta">#</span><span class="bash"> cinder type-list</span><br>+--------------------------------------+------------------+-------------+-----------+<br>| ID                                   | Name             | Description | Is_Public |<br>+--------------------------------------+------------------+-------------+-----------+<br>| 2996c5a8-e312-42b8-b7f1-ca649a32eadb | 标准云盘FC01-SAN |             | True      |<br>| 8f77dc78-689d-40ae-a673-dba37eaf7004 | 标准云盘FC02-SAN |             | True      |<br>| ac318ed1-7de2-4ebb-8233-c35301ba77bd | Ceph             |             | True      |<br>| bb5298dc-3790-4f06-8e26-b28c3fc7b56e | 高性能FC-SAN     |             | True      |<br>+--------------------------------------+------------------+-------------+-----------+<br><br><span class="hljs-meta">#</span><span class="bash"> cinder type-key Ceph <span class="hljs-built_in">set</span> volume_backend_name=rbd-1</span><br><span class="hljs-meta">#</span><span class="bash"> cinder  extra-specs-list</span><br>+--------------------------------------+------------------+---------------------------------------------+<br>| ID                                   | Name             | extra_specs                                 |<br>+--------------------------------------+------------------+---------------------------------------------+<br>| 2996c5a8-e312-42b8-b7f1-ca649a32eadb | 标准云盘FC01-SAN | &#123;&#x27;volume_backend_name&#x27;: &#x27;18000V1_FC_SAS01&#x27;&#125; |<br>| 8f77dc78-689d-40ae-a673-dba37eaf7004 | 标准云盘FC02-SAN | &#123;&#x27;volume_backend_name&#x27;: &#x27;18000V1_FC_SAS02&#x27;&#125; |<br>| ac318ed1-7de2-4ebb-8233-c35301ba77bd | Ceph             | &#123;&#x27;volume_backend_name&#x27;: &#x27;rbd-1&#x27;&#125;            |<br>| bb5298dc-3790-4f06-8e26-b28c3fc7b56e | 高性能FC-SAN     | &#123;&#x27;volume_backend_name&#x27;: &#x27;18000V1_FC&#x27;&#125;       |<br>+--------------------------------------+------------------+---------------------------------------------+<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>openstack</category>
      
    </categories>
    
    
    <tags>
      
      <tag>openstack</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>docker网络</title>
    <link href="/posts/docker%E7%BD%91%E7%BB%9C/"/>
    <url>/posts/docker%E7%BD%91%E7%BB%9C/</url>
    
    <content type="html"><![CDATA[<h3 id="docker四种单节点网络"><a href="#docker四种单节点网络" class="headerlink" title="docker四种单节点网络"></a>docker四种单节点网络</h3><p><code>Docker</code>在创建容器时有四种网络模式，<code>bridge</code>为默认不需要用<code>-–net</code>去指定，其他三种模式需要在创建容器时使用<code>–-net</code>去指定。<br>四种单节点网络模式如下：<br><code>none</code>模式，使用<code>–-net=none</code>指定；<br><code>host</code>模式，使用<code>–-net=host</code>指定；<br><code>bridge</code>模式，使用<code>–-net=bridge</code>指定，默认设置；<br><code>container</code>模式，使用<code>–-net=container</code>:容器名称或ID指定。<br>在<code>Docker</code>安装时会自动在<code>host</code>上创建三个网络,我们可以通过<code>docker network ls</code>查看：<br><img src="https://ljw.howieli.cn/blog/2018-06-12/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20180612122600.png"></p><a id="more"></a><h3 id="none网络"><a href="#none网络" class="headerlink" title="none网络"></a>none网络</h3><p><code>none</code> 网络就是什么都没有的网络。挂在这个网络下的容器除了 <code>lo</code>，没有其他任何网卡。容器创建时，可以通过 <code>--network=none</code> 指定使用 <code>none</code> 网络。<br><img src="https://ljw.howieli.cn/blog/2018-06-12/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20180612155934.png"><br>此模式下创建容器是不会为容器配置任何网络参数的，如：容器网卡、<code>IP</code>、通信路由等，全部需要自己去配置。<br>其应用场景，封闭意味着隔离，一些对安全性要求高并且不需要联网的应用可以使用 <code>none</code> 网络，比如唯一用途是生成随机密码，就可以放到 <code>none</code> 网络中避免密码被窃取。</p><h3 id="host网络"><a href="#host网络" class="headerlink" title="host网络"></a>host网络</h3><p>连接到 <code>host</code> 网络的容器共享 <code>Docker host</code> 的网络栈，容器的网络配置与 host 完全一样。可以通过 <code>--network=host</code>指定使用 <code>host</code> 网络。<br><code>Host</code> 模式并没有为容器创建一个隔离的网络环境。而之所以称之为<code>host</code>模式，是因为该模式下的 <code>Docker</code> 容器会和 <code>host</code> 宿主机共享同一个网络 <code>namespace</code>，故 <code>Docker Container</code>可以和宿主机一样，使用宿主机的<code>eth0</code>，实现和外界的通信。换言之，<code>Docker Container</code>的 <code>IP</code> 地址即为宿主机 <code>eth0</code> 的 <code>IP</code> 地址。其特点包括：</p><ul><li>这种模式下的容器没有隔离的 <code>network namespace</code></li><li>容器的 <code>IP</code> 地址同 <code>Docker host</code> 的 <code>IP</code> 地址</li><li>需要注意容器中服务的端口号不能与 <code>Docker host</code> 上已经使用的端口号相冲突</li><li><code>host</code> 模式能够和其它模式共存</li></ul><p><img src="https://ljw.howieli.cn/blog/2018-06-12/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20180612160912.png"></p><p>直接使用 <code>Docker host</code> 的网络最大的好处就是性能，如果容器对网络传输效率有较高要求，则可以选择 <code>host</code> 网络。当然不便之处就是牺牲一些灵活性，比如要考虑端口冲突问题，<code>Docker host</code> 上已经使用的端口就不能再用了.<br><code>Docker host</code> 的另一个用途是让容器可以直接配置 <code>host</code> 网路。比如某些跨 <code>host</code> 的网络解决方案，其本身也是以容器方式运行的，这些方案需要对网络进行配置，比如管理 <code>iptables</code>。</p><h3 id="bridge网络"><a href="#bridge网络" class="headerlink" title="bridge网络"></a>bridge网络</h3><p><code>Docker</code> 安装时会创建一个命名为 <code>docker0</code> 的 <code>linux bridge</code>。如果不指定<code>--network</code>，创建的容器默认都会挂到 <code>docker0</code> 上。<br><img src="https://ljw.howieli.cn/blog/2018-06-12/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20180612165740.png"><br>当前 <code>docker0</code> 上没有任何其他网络设备，我们创建一个容器看看有什么变化。<br><img src="https://ljw.howieli.cn/blog/2018-06-12/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20180612170225.png"></p><p>当创建httpd容器后，一个新的网络接口<code>veth303a751</code>被挂到了<code>docker0</code>上，<code>veth303a751</code>就是新创建的虚拟网卡。当<code>stop</code>掉<code>httpd</code>容器虚拟网卡消失。<br>下图是<code>docker bridge</code>模式网络架构图<br><img src="https://ljw.howieli.cn/blog/2018-06-12/docker0.png"></p><p><code>host</code>网络关键就是<code>NAT</code>，我们可以查看一下<code>docker host</code>上的<code>iptables</code>规则<br><img src="https://ljw.howieli.cn/blog/2018-06-12/docker3.png"><br>其含义是：如果网桥 <code>docker0</code> 收到来自 <code>172.17.0.0/16</code> 网段的外出包，把它交给 <code>MASQUERADE</code> 处理。而 <code>MASQUERADE</code> 的处理方式是将包的源地址替换成 <code>host</code> 的地址发送出去，即做了一次网络地址转换（<code>NAT</code>）。</p><h3 id="container网络"><a href="#container网络" class="headerlink" title="container网络"></a>container网络</h3><p>这个模式指定新创建的容器和已经存在的一个容器共享一个<code>Network Namespace</code>，而不是和宿主机共享。新创建的容器不会创建自己的网卡，配置自己的<code>IP</code>，而是和一个指定的容器共享IP、端口范围等。同样，两个容器除了网络方面，其他的如文件系统、进程列表等还是隔离的。两个容器的进程可以通过<code>lo</code>网卡设备通信。<br>使用<code>–-net=container:CONIAINER ID</code>模式启动容器：<br><img src="https://ljw.howieli.cn/blog/2018-06-12/docker1.png"></p><p><img src="https://ljw.howieli.cn/blog/2018-06-12/docker2.png"></p>]]></content>
    
    
    <categories>
      
      <category>docker</category>
      
    </categories>
    
    
    <tags>
      
      <tag>docker</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ceph-for-openstack-storage</title>
    <link href="/posts/ceph-for-openstack-storage/"/>
    <url>/posts/ceph-for-openstack-storage/</url>
    
    <content type="html"><![CDATA[<h2 id="openstack对接ceph存储"><a href="#openstack对接ceph存储" class="headerlink" title="openstack对接ceph存储"></a><code>openstack</code>对接<code>ceph</code>存储</h2><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><ul><li>截至<code>2018</code>年<code>5</code>月<code>3</code>日，<code>OpenStack</code>也已经伴随着我们走过了<code>8</code>个年头了，并且成为了云计算领域中最火热的项目之一，逐渐成为<code>IaaS</code>的事实标准，私有云项目的部署首选；</li><li><code>OpenStack</code>发展如此的迅速，以至于部署规模愈发的庞大，此时就该思量<code>OpenStack</code>集群的部署支持以及持续可扩展性；</li><li><code>OpenStack</code>和<code>Ceph</code>的集成更让开源项目锦上添花，<code>Ceph</code>作为优秀的分布式存储系统，实现对<code>OpenStack</code>相关子项目进行集成或替代，目前在<code>OpenStack</code>中扮演者非常重要的角色；</li></ul><a id="more"></a><h2 id="基础配置"><a href="#基础配置" class="headerlink" title="基础配置"></a>基础配置</h2><ul><li><code>Ceph</code>已经成为<code>OpenStack</code>后端存储标配，<code>OpenStack</code>作为<code>IaaS</code>系统，涉及到存储的部分主要是块存储服务模块、对象存储服务模块、镜像管理模块和计算服务模块，对应为其中的<code>Cinder</code>、<code>Swift</code>、<code>Glance</code>和<code>Nova</code>四个项目；</li><li>配置前，请将<code>OpenStack</code>中已存在的<code>VM</code>、<code>Image</code>与<code>Vloume</code>清理掉；</li></ul><h3 id="创建相应的Pool"><a href="#创建相应的Pool" class="headerlink" title="创建相应的Pool"></a>创建相应的<code>Pool</code></h3><ul><li>若少于<code>5</code>个<code>OSD</code>, 设置<code>pg_num</code>为<code>128</code>；</li><li><code>5 ~ 10</code>个<code>OSD</code>，设置<code>pg_num</code>为<code>512</code>；</li><li><code>10 ~ 50</code>个<code>OSD</code>，设置<code>pg_num</code>为<code>4096</code>；</li><li>超过<code>50</code>个<code>OSD</code>, 根据(<code>PG数 = OSD数 * 100/ 副本数 / POOL数</code>)来计算；</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ ceph osd pool create images 1024<br>$ ceph osd pool create volumes 1024<br>$ ceph osd pool create vms 1024<br>$ ceph osd pool create backups 1024<br><br>$ ceph osd pool create images.cache 1024<br>$ ceph osd pool create volumes.cache 1024<br>$ ceph osd pool create vms.cache 1024<br>$ ceph osd pool create backups.cache 1024<br></code></pre></td></tr></table></figure><h3 id="删除相应的Pool-备用"><a href="#删除相应的Pool-备用" class="headerlink" title="删除相应的Pool(备用)"></a>删除相应的<code>Pool</code>(备用)</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ ceph osd pool delete images images --yes-i-really-really-mean-it<br>$ ceph osd pool delete volumes volumes --yes-i-really-really-mean-it<br>$ ceph osd pool delete vms vms --yes-i-really-really-mean-it<br>$ ceph osd pool delete backups backups --yes-i-really-really-mean-it<br><br>$ ceph osd pool delete images.cache images.cache --yes-i-really-really-mean-it<br>$ ceph osd pool delete volumes.cache volumes.cache --yes-i-really-really-mean-it<br>$ ceph osd pool delete vms.cache vms.cache --yes-i-really-really-mean-it<br>$ ceph osd pool delete backups.cache backups.cache --yes-i-really-really-mean-it<br></code></pre></td></tr></table></figure><h3 id="更新相应Pool的属性值-备用"><a href="#更新相应Pool的属性值-备用" class="headerlink" title="更新相应Pool的属性值(备用)"></a>更新相应<code>Pool</code>的属性值(备用)</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ ceph osd pool <span class="hljs-built_in">set</span> images pg_num 512<br>$ ceph osd pool <span class="hljs-built_in">set</span> volumes pg_num 512<br>$ ceph osd pool <span class="hljs-built_in">set</span> vms pg_num 512<br>$ ceph osd pool <span class="hljs-built_in">set</span> backups pg_num 512<br><br>$ ceph osd pool <span class="hljs-built_in">set</span> images pgp_num 512<br>$ ceph osd pool <span class="hljs-built_in">set</span> volumes pgp_num 512<br>$ ceph osd pool <span class="hljs-built_in">set</span> vms pgp_num 512<br>$ ceph osd pool <span class="hljs-built_in">set</span> backups pgp_num 512<br></code></pre></td></tr></table></figure><h3 id="将不同的Pool加入到对应的Crush-Map中"><a href="#将不同的Pool加入到对应的Crush-Map中" class="headerlink" title="将不同的Pool加入到对应的Crush Map中"></a>将不同的<code>Pool</code>加入到对应的<code>Crush Map</code>中</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 后备存储-SATA盘</span><br>$ ceph osd pool <span class="hljs-built_in">set</span> images crush_ruleset 0<br>$ ceph osd pool <span class="hljs-built_in">set</span> volumes crush_ruleset 0<br>$ ceph osd pool <span class="hljs-built_in">set</span> vms crush_ruleset 0<br>$ ceph osd pool <span class="hljs-built_in">set</span> backups crush_ruleset 0<br><br><span class="hljs-comment"># 缓存存储-SSD盘</span><br>$ ceph osd pool <span class="hljs-built_in">set</span> images.cache crush_ruleset 1<br>$ ceph osd pool <span class="hljs-built_in">set</span> volumes.cache crush_ruleset 1<br>$ ceph osd pool <span class="hljs-built_in">set</span> vms.cache crush_ruleset 1<br>$ ceph osd pool <span class="hljs-built_in">set</span> backups.cache crush_ruleset 1<br></code></pre></td></tr></table></figure><h3 id="将缓存层与后备存储池相关联"><a href="#将缓存层与后备存储池相关联" class="headerlink" title="将缓存层与后备存储池相关联"></a>将缓存层与后备存储池相关联</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ ceph osd tier add images images.cache<br>$ ceph osd tier add volumes volumes.cache<br>$ ceph osd tier add vms vms.cache<br>$ ceph osd tier add backups backups.cache<br></code></pre></td></tr></table></figure><h3 id="设置缓存模式-热存储回写"><a href="#设置缓存模式-热存储回写" class="headerlink" title="设置缓存模式(热存储回写)"></a>设置缓存模式(热存储回写)</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ ceph osd tier cache-mode images.cache writeback<br>$ ceph osd tier cache-mode volumes.cache writeback<br>$ ceph osd tier cache-mode vms.cache writeback<br>$ ceph osd tier cache-mode backups.cache writeback<br></code></pre></td></tr></table></figure><h3 id="高速缓存层覆盖后备存储池"><a href="#高速缓存层覆盖后备存储池" class="headerlink" title="高速缓存层覆盖后备存储池"></a>高速缓存层覆盖后备存储池</h3><ul><li>直接将客户端流量引导到缓存池</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ ceph osd tier set-overlay images images.cache<br>$ ceph osd tier set-overlay volumes volumes.cache<br>$ ceph osd tier set-overlay vms vms.cache<br>$ ceph osd tier set-overlay backups backups.cache<br></code></pre></td></tr></table></figure><h3 id="启用缓存存储池的命中集跟踪"><a href="#启用缓存存储池的命中集跟踪" class="headerlink" title="启用缓存存储池的命中集跟踪"></a>启用缓存存储池的命中集跟踪</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ ceph osd pool <span class="hljs-built_in">set</span> images.cache hit_set_type bloom<br>$ ceph osd pool <span class="hljs-built_in">set</span> volumes.cache hit_set_type bloom<br>$ ceph osd pool <span class="hljs-built_in">set</span> vms.cache hit_set_type bloom<br>$ ceph osd pool <span class="hljs-built_in">set</span> backups.cache hit_set_type bloom<br></code></pre></td></tr></table></figure><h3 id="为缓存存储池的保留的命中集数量"><a href="#为缓存存储池的保留的命中集数量" class="headerlink" title="为缓存存储池的保留的命中集数量"></a>为缓存存储池的保留的命中集数量</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ ceph osd pool <span class="hljs-built_in">set</span> images.cache hit_set_count 10<br>$ ceph osd pool <span class="hljs-built_in">set</span> volumes.cache hit_set_count 10<br>$ ceph osd pool <span class="hljs-built_in">set</span> vms.cache hit_set_count 10<br>$ ceph osd pool <span class="hljs-built_in">set</span> backups.cache hit_set_count 10<br></code></pre></td></tr></table></figure><h3 id="为缓存存储池保留的命中集有效期"><a href="#为缓存存储池保留的命中集有效期" class="headerlink" title="为缓存存储池保留的命中集有效期"></a>为缓存存储池保留的命中集有效期</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ ceph osd pool <span class="hljs-built_in">set</span> images.cache hit_set_period 14400<br>$ ceph osd pool <span class="hljs-built_in">set</span> volumes.cache hit_set_period 14400<br>$ ceph osd pool <span class="hljs-built_in">set</span> vms.cache hit_set_period 14400<br>$ ceph osd pool <span class="hljs-built_in">set</span> backups.cache hit_set_period 14400<br></code></pre></td></tr></table></figure><h3 id="缓存层回写数据"><a href="#缓存层回写数据" class="headerlink" title="缓存层回写数据"></a>缓存层回写数据</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 最大达到50G时，回写数据</span><br>$ ceph osd pool <span class="hljs-built_in">set</span> images.cache target_max_bytes 53687091200<br>$ ceph osd pool <span class="hljs-built_in">set</span> volumes.cache target_max_bytes 53687091200<br>$ ceph osd pool <span class="hljs-built_in">set</span> vms.cache target_max_bytes 53687091200<br>$ ceph osd pool <span class="hljs-built_in">set</span> backups.cache target_max_bytes 53687091200<br><br><span class="hljs-comment"># 最大达到100万时，回写数据</span><br>$ ceph osd pool <span class="hljs-built_in">set</span> images.cache target_max_objects 1000000<br>$ ceph osd pool <span class="hljs-built_in">set</span> volumes.cache target_max_objects 1000000<br>$ ceph osd pool <span class="hljs-built_in">set</span> vms.cache target_max_objects 1000000<br>$ ceph osd pool <span class="hljs-built_in">set</span> backups.cache target_max_objects 1000000<br></code></pre></td></tr></table></figure><h3 id="保留访问记录"><a href="#保留访问记录" class="headerlink" title="保留访问记录"></a>保留访问记录</h3><ul><li>取值越高，消耗的内存就越多；</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ ceph osd pool <span class="hljs-built_in">set</span> images.cache min_read_recency_for_promote 2<br>$ ceph osd pool <span class="hljs-built_in">set</span> images.cache min_write_recency_for_promote 2<br>$ ceph osd pool <span class="hljs-built_in">set</span> volumes.cache min_read_recency_for_promote 2<br>$ ceph osd pool <span class="hljs-built_in">set</span> volumes.cache min_write_recency_for_promote 2<br>$ ceph osd pool <span class="hljs-built_in">set</span> vms.cache min_read_recency_for_promote 2<br>$ ceph osd pool <span class="hljs-built_in">set</span> vms.cache min_write_recency_for_promote 2<br>$ ceph osd pool <span class="hljs-built_in">set</span> backups.cache min_read_recency_for_promote 2<br>$ ceph osd pool <span class="hljs-built_in">set</span> backups.cache min_write_recency_for_promote 2<br></code></pre></td></tr></table></figure><h2 id="认证配置"><a href="#认证配置" class="headerlink" title="认证配置"></a>认证配置</h2><h3 id="创建认证密钥"><a href="#创建认证密钥" class="headerlink" title="创建认证密钥"></a>创建认证密钥</h3><h4 id="Nova用户"><a href="#Nova用户" class="headerlink" title="Nova用户"></a>Nova用户</h4><ul><li>允许访问<code>volumes</code>、<code>vms</code>、<code>images</code>池；</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ ceph auth get-or-create client.nova mon <span class="hljs-string">&#x27;allow r&#x27;</span> osd <span class="hljs-string">&#x27;allow class-read object_prefix rbd_children, allow rwx pool=volumes, allow rwx pool=vms, allow rwx pool=images ,allow rwx pool=volumes.cache, allow rwx pool=vms.cache, allow rwx pool=images.cache&#x27;</span><br></code></pre></td></tr></table></figure><h4 id="Cinder用户"><a href="#Cinder用户" class="headerlink" title="Cinder用户"></a>Cinder用户</h4><ul><li>允许访问<code>volumes</code>、<code>backups</code>池；</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ceph auth get-or-create client.cinder mon <span class="hljs-string">&#x27;allow r&#x27;</span> osd <span class="hljs-string">&#x27;allow class-read object_prefix rbd_children, allow rwx pool=volumes, allow rwx pool=volumes.cache ,allow rwx pool=backups ,allow rwx pool=backups.cache&#x27;</span><br></code></pre></td></tr></table></figure><h4 id="Glance用户"><a href="#Glance用户" class="headerlink" title="Glance用户"></a>Glance用户</h4><ul><li>允许访问<code>images</code>池；</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ ceph auth get-or-create client.glance mon <span class="hljs-string">&#x27;allow r&#x27;</span> osd <span class="hljs-string">&#x27;allow class-read object_prefix rbd_children, allow rwx pool=images, allow rwx pool=images.cache&#x27;</span><br></code></pre></td></tr></table></figure><h3 id="获取认证密钥"><a href="#获取认证密钥" class="headerlink" title="获取认证密钥"></a>获取认证密钥</h3><h4 id="Controller节点"><a href="#Controller节点" class="headerlink" title="Controller节点"></a><code>Controller</code>节点</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ ceph auth get-or-create client.nova | ssh root@controller tee /etc/ceph/ceph.client.nova.keyring<br>$ ssh root@controller chown nova:nova /etc/ceph/ceph.client.nova.keyring<br><br>$ ceph auth get-or-create client.cinder | ssh root@controller tee /etc/ceph/ceph.client.cinder.keyring<br>$ ssh root@controller chown cinder:cinder /etc/ceph/ceph.client.cinder.keyring<br><br>$ ceph auth get-or-create client.glance | ssh root@controller tee /etc/ceph/ceph.client.glance.keyring<br>$ ssh root@controller chown glance:glance /etc/ceph/ceph.client.glance.keyring<br></code></pre></td></tr></table></figure><h4 id="Compute节点"><a href="#Compute节点" class="headerlink" title="Compute节点"></a><code>Compute</code>节点</h4><ul><li>请使用<code>Compute</code>节点的主机名代替<code>&#123;ComputeNode&#125;</code>；</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ ceph auth get-or-create client.nova | ssh root@&#123;ComputeNode&#125; tee /etc/ceph/ceph.client.nova.keyring<br>$ ssh root@&#123;ComputeNode&#125; chown nova:nova /etc/ceph/ceph.client.nova.keyring<br></code></pre></td></tr></table></figure><h2 id="OpenStack配置"><a href="#OpenStack配置" class="headerlink" title="OpenStack配置"></a><code>OpenStack</code>配置</h2><h3 id="配置Nova服务"><a href="#配置Nova服务" class="headerlink" title="配置Nova服务"></a>配置<code>Nova</code>服务</h3><h4 id="配置libvirt认证"><a href="#配置libvirt认证" class="headerlink" title="配置libvirt认证"></a>配置<code>libvirt</code>认证</h4><h5 id="获取密钥"><a href="#获取密钥" class="headerlink" title="获取密钥"></a>获取密钥</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ ceph auth get-key client.nova | tee ~/nova.key<br></code></pre></td></tr></table></figure><h5 id="生成UUID，用于认证"><a href="#生成UUID，用于认证" class="headerlink" title="生成UUID，用于认证"></a>生成<code>UUID</code>，用于认证</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ UUID=$(cat /etc/ceph/ceph.conf | grep <span class="hljs-string">&quot;fsid&quot;</span> | awk -F <span class="hljs-string">&quot; = &quot;</span> <span class="hljs-string">&#x27;&#123;print $2&#125;&#x27;</span>)<br></code></pre></td></tr></table></figure><h5 id="生成secret-xml文件"><a href="#生成secret-xml文件" class="headerlink" title="生成secret.xml文件"></a>生成<code>secret.xml</code>文件</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ cat &gt; ~/secret.xml &lt;&lt;<span class="hljs-string">EOF</span><br><span class="hljs-string">&lt;secret ephemeral=&#x27;no&#x27; private=&#x27;no&#x27;&gt;</span><br><span class="hljs-string">  &lt;uuid&gt;$&#123;UUID&#125;&lt;/uuid&gt;</span><br><span class="hljs-string">  &lt;usage type=&#x27;ceph&#x27;&gt;</span><br><span class="hljs-string">        &lt;name&gt;client.nova secret&lt;/name&gt;</span><br><span class="hljs-string">  &lt;/usage&gt;</span><br><span class="hljs-string">&lt;/secret&gt;</span><br><span class="hljs-string">EOF</span><br></code></pre></td></tr></table></figure><h5 id="定义secret-xml文件"><a href="#定义secret-xml文件" class="headerlink" title="定义secret.xml文件"></a>定义<code>secret.xml</code>文件</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ virsh secret-define --file ~/secret.xml<br></code></pre></td></tr></table></figure><h5 id="列出已定义的项目"><a href="#列出已定义的项目" class="headerlink" title="列出已定义的项目"></a>列出已定义的项目</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ virsh secret-list<br></code></pre></td></tr></table></figure><h5 id="取消定义-备用"><a href="#取消定义-备用" class="headerlink" title="取消定义(备用)"></a>取消定义(备用)</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ virsh secret-undefine <span class="hljs-variable">$&#123;UUID&#125;</span><br></code></pre></td></tr></table></figure><h5 id="关联密钥与UUID"><a href="#关联密钥与UUID" class="headerlink" title="关联密钥与UUID"></a>关联密钥与<code>UUID</code></h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ virsh secret-set-value --secret <span class="hljs-variable">$&#123;UUID&#125;</span> --base64 $(cat ~/nova.key) &amp;&amp; rm -f ~/&#123;nova.key,secret.xml&#125;<br></code></pre></td></tr></table></figure><h4 id="配置Nova服务-1"><a href="#配置Nova服务-1" class="headerlink" title="配置Nova服务"></a>配置<code>Nova</code>服务</h4><h5 id="编辑配置文件"><a href="#编辑配置文件" class="headerlink" title="编辑配置文件"></a>编辑配置文件</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ vim /etc/nova/nova.conf<br></code></pre></td></tr></table></figure><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs text">[libvirt]<br>images_type = rbd<br>images_rbd_pool = vms<br>images_rbd_ceph_conf = /etc/ceph/ceph.conf<br>rbd_user = nova<br>rbd_secret_uuid = UUID<br>disk_cachemodes = &quot;network=writeback&quot;<br>hw_disk_discard = unmap<br>inject_password = true<br>inject_key = true<br>inject_partition = -2<br>live_migration_flag = &quot;VIR_MIGRATE_UNDEFINE_SOURCE,VIR_MIGRATE_PEER2PEER,VIR_MIGRATE_LIVE,VIR_MIGRATE_PERSIST_DEST,VIR_MIGRATE_TUNNELLED&quot;<br></code></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ sed -i <span class="hljs-string">&quot;s#UUID#<span class="hljs-variable">$&#123;UUID&#125;</span>#g&quot;</span> /etc/nova/nova.conf<br></code></pre></td></tr></table></figure><h5 id="重启服务"><a href="#重启服务" class="headerlink" title="重启服务"></a>重启服务</h5><ul><li><code>Ubuntu</code>系统：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ service nova-compute restart<br></code></pre></td></tr></table></figure><ul><li><code>CentOS</code>系统：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ systemctl restart libvirtd.service openstack-nova-compute.service<br></code></pre></td></tr></table></figure><h3 id="配置Glance服务"><a href="#配置Glance服务" class="headerlink" title="配置Glance服务"></a>配置<code>Glance</code>服务</h3><h4 id="编辑配置文件-1"><a href="#编辑配置文件-1" class="headerlink" title="编辑配置文件"></a>编辑配置文件</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ vim /etc/glance/glance-api.conf<br></code></pre></td></tr></table></figure><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs text">[DEFAULT]<br>show_image_direct_url = True<br><br>[glance_store]<br>stores = rbd<br>rbd_store_pool = images<br>rbd_store_user = glance<br>rbd_store_ceph_conf = /etc/ceph/ceph.conf<br>rbd_store_chunk_size = 8<br>default_store =rbd<br><br>[task]<br>task_executor = taskflow<br>work_dir=/tmp<br><br>[taskflow_executor]<br>engine_mode = serial<br>max_workers = 10<br>conversion_format=raw<br></code></pre></td></tr></table></figure><h4 id="重启服务-1"><a href="#重启服务-1" class="headerlink" title="重启服务"></a>重启服务</h4><ul><li><code>Ubuntu</code>系统：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ service glance-api restart<br>$ service glance-registry restart<br></code></pre></td></tr></table></figure><ul><li><code>CentOS</code>系统：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ systemctl restart openstack-glance-api.service openstack-glance-registry.service<br></code></pre></td></tr></table></figure><h3 id="配置Cinder服务"><a href="#配置Cinder服务" class="headerlink" title="配置Cinder服务"></a>配置<code>Cinder</code>服务</h3><ul><li>仅需在<code>Controller</code>节点安装<code>cinder</code>的所有的服务即可；</li></ul><h4 id="编辑配置文件-2"><a href="#编辑配置文件-2" class="headerlink" title="编辑配置文件"></a>编辑配置文件</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ vim /etc/cinder/cinder.conf<br></code></pre></td></tr></table></figure><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs text">[DEFAULT]<br>enabled_backends = ceph<br>backup_driver = cinder.backup.drivers.ceph<br>backup_ceph_conf = /etc/ceph/ceph.conf<br>backup_ceph_user = cinder-backup<br>backup_ceph_chunk_size = 134217728<br>backup_ceph_pool = backups<br>backup_ceph_stripe_unit = 0<br>backup_ceph_stripe_count = 0<br>restore_discard_excess_bytes = true<br><br>[ceph]<br>volume_driver = cinder.volume.drivers.rbd.RBDDriver<br>rbd_pool = volumes<br>rbd_ceph_conf = /etc/ceph/ceph.conf<br>rbd_flatten_volume_from_snapshot = false<br>rbd_max_clone_depth = 5<br>rbd_store_chunk_size = 4<br>rados_connect_timeout = -1<br>glance_api_version = 2<br>rbd_user = cinder<br>rbd_secret_uuid = UUID<br></code></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ sed -i <span class="hljs-string">&quot;s#UUID#<span class="hljs-variable">$&#123;UUID&#125;</span>#g&quot;</span> /etc/cinder/cinder.conf<br></code></pre></td></tr></table></figure><h4 id="重启服务-2"><a href="#重启服务-2" class="headerlink" title="重启服务"></a>重启服务</h4><ul><li><code>Ubuntu</code>系统：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ service cinder-api restart<br>$ service cinder-scheduler restart<br>$ service cinder-volume restart<br>$ service cinder-backup restart<br>$ service tgt restart<br></code></pre></td></tr></table></figure><ul><li><code>CentOS</code>系统：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ systemctl restart openstack-cinder-volume.service target.service<br></code></pre></td></tr></table></figure><h2 id="测试操作"><a href="#测试操作" class="headerlink" title="测试操作"></a>测试操作</h2><h3 id="下载镜像"><a href="#下载镜像" class="headerlink" title="下载镜像"></a>下载镜像</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ wget http://download.cirros-cloud.net/0.3.5/cirros-0.3.5-x86_64-disk.img<br></code></pre></td></tr></table></figure><h3 id="上传镜像"><a href="#上传镜像" class="headerlink" title="上传镜像"></a>上传镜像</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ openstack image create <span class="hljs-string">&quot;cirros&quot;</span> --file cirros-0.3.5-x86_64-disk.img --disk-format qcow2 --container-format bare --public<br></code></pre></td></tr></table></figure><h3 id="获取镜像"><a href="#获取镜像" class="headerlink" title="获取镜像"></a>获取镜像</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ openstack image list<br></code></pre></td></tr></table></figure><h3 id="获取Ceph的存储使用率"><a href="#获取Ceph的存储使用率" class="headerlink" title="获取Ceph的存储使用率"></a>获取<code>Ceph</code>的存储使用率</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ ceph df<br></code></pre></td></tr></table></figure><hr>]]></content>
    
    
    <categories>
      
      <category>openstack</category>
      
    </categories>
    
    
    <tags>
      
      <tag>openstack</tag>
      
      <tag>ceph</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>kolla 多region搭建</title>
    <link href="/posts/kolla-%E5%A4%9Aregion%E6%90%AD%E5%BB%BA/"/>
    <url>/posts/kolla-%E5%A4%9Aregion%E6%90%AD%E5%BB%BA/</url>
    
    <content type="html"><![CDATA[<h3 id="kolla部署openstack多region"><a href="#kolla部署openstack多region" class="headerlink" title="kolla部署openstack多region"></a>kolla部署openstack多region</h3><h3 id="openstack多region基础知识"><a href="#openstack多region基础知识" class="headerlink" title="openstack多region基础知识"></a>openstack多region基础知识</h3><p>我们都知道<code>Region</code>是<code>OpenStack</code>里面用于隔离资源的一个重要概念。简单来说，一个<code>Region</code>对应一套完整的<code>OpenStack</code>环境，而<code>Region</code>和<code>Region</code>之间可以是跨机房的集群，也可以是一个大规模物理机集群分割后的集群。<code>OpenStack</code>在设计之初就是支持多<code>Region</code>的情况，由于<code>Region</code>之间资源（<code>Mariadb</code>,<code>RabbitMQ</code>等）的独立的，所以他们之间并不存在资源交互开销的情况。<br>简单的讲所谓<code>openstack</code>多<code>region</code>，就是多套<code>openstack</code>共享一个<code>keystone</code>和<code>horizon</code>。每个区域一套<code>openstack</code>环境，可以分布在不同的地理位置，只要网络可达就行。个人认为目的就是为了提供环境隔离的功能，选择启虚拟机的时候可以根据自己所处的位置就近选择。<br><img src="https://ljw.howieli.cn/blog/2018-4-3/%E5%A4%9Aregion1.png"></p><a id="more"></a><h3 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h3><p>我这里有两台<code>kolla</code>环境的部署节点分别是<code>172.18.24.150</code>，<code>172.18.24.130</code>两个台虚拟机。<br>| column | column |<br>|——–|————|<br>|RegionOne|172.18.24.150|<br>|RegionTWO|172.18.24.130|<br><code>kolla</code>环境部署节点安装请参考<a href="https://www.lijiawang.org/posts/kolla%20queens%20on%20centos7.41.html">kolla搭建</a>，这次我用<code>kolla</code>部署<code>openstack</code>的<code>queens</code>版本。</p><h3 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h3><h4 id="用Keystone和Horizon部署第一个区域"><a href="#用Keystone和Horizon部署第一个区域" class="headerlink" title="用Keystone和Horizon部署第一个区域"></a>用<code>Keystone</code>和<code>Horizon</code>部署第一个区域</h4><p>我这这里把<code>keystone</code>,<code>horizon</code>这两个服务放到<code>RegionOne</code>虚拟机上。修改配置<code>/etc/kolla/globals.yml</code>文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> egrep <span class="hljs-string">&quot;^[^#]&quot;</span> /etc/kolla/globals.yml</span><br>---<br>kolla_base_distro: &quot;centos&quot;<br>kolla_install_type: &quot;source&quot;<br>openstack_release: &quot;queens&quot;<br>kolla_internal_vip_address: &quot;172.18.24.150&quot;<br>docker_namespace: &quot;kolla&quot;<br>network_interface: &quot;eth0&quot;<br>neutron_external_interface: &quot;eth1&quot;<br>enable_haproxy: &quot;no&quot;<br>enable_grafana: &quot;yes&quot;<br>enable_horizon_ironic: &quot;&#123;&#123; enable_ironic | bool &#125;&#125;&quot;<br>ironic_dnsmasq_dhcp_range:<br>tempest_image_id:<br>tempest_flavor_ref_id:<br>tempest_public_network_id:<br>tempest_floating_network_name:<br>openstack_region_name: &quot;RegionOne&quot;<br>multiple_regions_names:<br>    - &quot;&#123;&#123; openstack_region_name &#125;&#125;&quot;<br>    - &quot;RegionTwo&quot;<br></code></pre></td></tr></table></figure><p>部署<code>RegionOne</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> kolla-ansible deploy</span><br></code></pre></td></tr></table></figure><h4 id="部署其他区域"><a href="#部署其他区域" class="headerlink" title="部署其他区域"></a>部署其他区域</h4><p>修改<code>RegionTWO</code>的<code>/etc/kolla/globals.yml</code>文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> egrep <span class="hljs-string">&quot;^[^#]&quot;</span> /etc/kolla/globals.yml</span><br>---<br>kolla_base_distro: &quot;centos&quot;<br>kolla_install_type: &quot;source&quot;<br>openstack_release: &quot;queens&quot;<br>kolla_internal_vip_address: &quot;172.18.24.130&quot;<br>docker_namespace: &quot;kolla&quot;<br>network_interface: &quot;eth0&quot;<br>neutron_external_interface: &quot;eth1&quot;<br>nova_console: &quot;novnc&quot;<br>enable_keystone: &quot;no&quot;<br>enable_haproxy: &quot;no&quot;<br>enable_horizon: &quot;no&quot;<br>nova_compute_virt_type: &quot;qemu&quot;<br>tempest_image_id:<br>tempest_flavor_ref_id:<br>tempest_public_network_id:<br>tempest_floating_network_name:<br><span class="hljs-meta">#</span><span class="bash"> kolla_internal_fqdn_r1是指RegionOne中kolla_internal_fqdn的值</span><br>kolla_internal_fqdn_r1: 172.18.24.150<br>keystone_admin_url: &quot;&#123;&#123; admin_protocol &#125;&#125;://&#123;&#123; kolla_internal_fqdn_r1 &#125;&#125;:&#123;&#123; keystone_admin_port &#125;&#125;&quot;<br>keystone_internal_url: &quot;&#123;&#123; internal_protocol &#125;&#125;://&#123;&#123; kolla_internal_fqdn_r1 &#125;&#125;:&#123;&#123; keystone_public_port &#125;&#125;&quot;<br>openstack_auth:<br>    auth_url: &quot;&#123;&#123; admin_protocol &#125;&#125;://&#123;&#123; kolla_internal_fqdn_r1 &#125;&#125;:&#123;&#123; keystone_admin_port &#125;&#125;&quot;<br>    username: &quot;admin&quot;<br>    password: &quot;&#123;&#123; keystone_admin_password &#125;&#125;&quot;<br>    project_name: &quot;admin&quot;<br>    domain_name: &quot;default&quot;<br>openstack_region_name: &quot;RegionTwo&quot;<br></code></pre></td></tr></table></figure><h4 id="RegionTWO上的openstack组件连接RegionOne上的keystone"><a href="#RegionTWO上的openstack组件连接RegionOne上的keystone" class="headerlink" title="RegionTWO上的openstack组件连接RegionOne上的keystone"></a>RegionTWO上的openstack组件连接RegionOne上的keystone</h4><p>需要修改<code>cinder</code>，<code>nova</code>，<code>neutron</code>，<code>glance</code>等配置文件才能联系<code>RegionOne</code>的<code>Keystone</code>。在<code>RegionTWO</code>虚拟机上创建<code>/etc/kolla/config/</code>目录,然后穿件需要连接<code>Keystone</code>服务的配置文件。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> mkdir -p /etc/kolla/config/</span><br><span class="hljs-meta">#</span><span class="bash"> <span class="hljs-built_in">cd</span> /etc/kolla/config/</span><br><span class="hljs-meta">#</span><span class="bash"> tree</span><br>.<br>├── glance.conf<br>├── heat.conf<br>├── neutron.conf<br>└── nova.conf<br></code></pre></td></tr></table></figure><p>更新配置文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> cat glance.conf</span><br>[keystone_authtoken]<br>auth_uri = &#123;&#123; keystone_internal_url &#125;&#125;<br>auth_url = &#123;&#123; keystone_admin_url &#125;&#125;<br><span class="hljs-meta">#</span><span class="bash"> cat nova.conf</span><br>[keystone_authtoken]<br>auth_uri = &#123;&#123; keystone_internal_url &#125;&#125;<br>auth_url = &#123;&#123; keystone_admin_url &#125;&#125;<br>[placement]<br>auth_url = &#123;&#123; keystone_admin_url &#125;&#125;<br><span class="hljs-meta">#</span><span class="bash"> cat neutron.conf</span><br>[keystone_authtoken]<br>auth_uri = &#123;&#123; keystone_internal_url &#125;&#125;<br>auth_url = &#123;&#123; keystone_admin_url &#125;&#125;<br><span class="hljs-meta">#</span><span class="bash"> cat heat.conf</span><br>[keystone_authtoken]<br>auth_uri = &#123;&#123; keystone_internal_url &#125;&#125;<br>auth_url = &#123;&#123; keystone_admin_url &#125;&#125;<br>[trustee]<br>auth_uri = &#123;&#123; keystone_internal_url &#125;&#125;<br>auth_url = &#123;&#123; keystone_internal_url &#125;&#125;<br><br>[ec2authtoken]<br>auth_uri = &#123;&#123; keystone_internal_url &#125;&#125;<br><br>[clients_keystone]<br>auth_uri = &#123;&#123; keystone_internal_url &#125;&#125;<br></code></pre></td></tr></table></figure><p>部署<code>RegionTWO</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> kolla-ansible deploy</span><br></code></pre></td></tr></table></figure><h3 id="Dashboard"><a href="#Dashboard" class="headerlink" title="Dashboard"></a>Dashboard</h3><p>如果以上过程顺利，恭喜你，你的<code>OpenStack</code>现在也支持多区域了。<br><img src="https://ljw.howieli.cn/blog/2018-4-3/%E5%A4%9Aregion.png"><br><img src="https://ljw.howieli.cn/blog/2018-4-3/%E5%A4%9Aregion2.png"></p><h3 id="Ocata版本多region搭建"><a href="#Ocata版本多region搭建" class="headerlink" title="Ocata版本多region搭建"></a>Ocata版本多region搭建</h3><p>如果您用的是<code>openstack</code>的<code>Ocata</code>版本，需要打一下<code>patch</code>，在部署，<a href="https://review.openstack.org/#/q/project:openstack/kolla-ansible+region">这里可以看到所有多region相关bug</a>，除了下面这个没放进去，都<code>merge</code>到<code>ocata</code>。需要把这个<code>patch</code>自己打进去即可。<br><a href="https://review.openstack.org/#/c/431658/">https://review.openstack.org/#/c/431658/</a></p>]]></content>
    
    
    <categories>
      
      <category>openstack</category>
      
    </categories>
    
    
    <tags>
      
      <tag>openstack</tag>
      
      <tag>kolla</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>kolla queens on centos7.4</title>
    <link href="/posts/kolla-queens-on-centos7-4/"/>
    <url>/posts/kolla-queens-on-centos7-4/</url>
    
    <content type="html"><![CDATA[<h3 id="kolla-queens-on-centos7-4"><a href="#kolla-queens-on-centos7-4" class="headerlink" title="kolla queens on centos7.4"></a>kolla queens on centos7.4</h3><p><img src="https://ljw.howieli.cn/blog/2018-03-29/WX20180329-171345.png"></p><a id="more"></a><h3 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h3><p>我这里用<code>workstation</code> 创建了一个虚拟机，安装<code>centos7.4</code>系统，这台虚拟机上有两张网卡，一张做<code>openstack</code>管理网，一张做为虚拟机的业务网卡。</p><h3 id="确定环境信息"><a href="#确定环境信息" class="headerlink" title="确定环境信息"></a>确定环境信息</h3><ul><li><p>确认系统版本信息</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> cat /etc/redhat-release</span><br>CentOS Linux release 7.4.1708 (Core)<br><span class="hljs-meta">#</span><span class="bash"> uname -r</span><br>3.10.0-693.5.2.el7.x86_64<br></code></pre></td></tr></table></figure></li><li><p> 确认网卡个数和状态</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> ip a</span><br>1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1<br>    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00<br>    inet 127.0.0.1/8 scope host lo<br>       valid_lft forever preferred_lft forever<br>    inet6 ::1/128 scope host<br>       valid_lft forever preferred_lft forever<br>2: ens33: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000<br>    link/ether 00:0c:29:a7:8c:91 brd ff:ff:ff:ff:ff:ff<br>    inet 192.168.254.15/24 brd 192.168.254.255 scope global dynamic ens33<br>       valid_lft 1555sec preferred_lft 1555sec<br>    inet6 fe80::5057:38c5:6b65:b5/64 scope link<br>       valid_lft forever preferred_lft forever<br>3: ens34: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000<br>    link/ether 00:0c:29:a7:8c:9b brd ff:ff:ff:ff:ff:ff<br>    inet6 fe80::20c:29ff:fea7:8c9b/64 scope link<br>       valid_lft forever preferred_lft forever<br></code></pre></td></tr></table></figure><p>上面可以看出有两张网卡<code>ens33</code>和<code>ens34</code>，这里我用<code>ens33</code>做管理网，<code>ens34</code>做业务网，这里不需要配置<code>ip</code>，把<code>ens34</code>网卡<code>up</code>起来就好。</p></li><li><p>查看主机名</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> hostname</span><br>queens<br></code></pre></td></tr></table></figure><h3 id="环境初始化"><a href="#环境初始化" class="headerlink" title="环境初始化"></a>环境初始化</h3></li><li><p>关闭<code>NetworkManager</code>,<code>firewalld</code>,<code>selinux</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> systemctl stop NetworkManager</span><br><span class="hljs-meta">#</span><span class="bash"> systemctl <span class="hljs-built_in">disable</span> NetworkManager</span><br>Removed symlink /etc/systemd/system/multi-user.target.wants/NetworkManager.service.<br>Removed symlink /etc/systemd/system/dbus-org.freedesktop.NetworkManager.service.<br>Removed symlink /etc/systemd/system/dbus-org.freedesktop.nm-dispatcher.service.<br><span class="hljs-meta">#</span><span class="bash"> systemctl stop firewalld</span><br><span class="hljs-meta">#</span><span class="bash"> systemctl <span class="hljs-built_in">disable</span> firewalld</span><br>Removed symlink /etc/systemd/system/multi-user.target.wants/firewalld.service.<br>Removed symlink /etc/systemd/system/dbus-org.fedoraproject.FirewallD1.service.<br><span class="hljs-meta">#</span><span class="bash"> sed -i <span class="hljs-string">&quot;s/SELINUX=enforcing/SELINUX=disabled/&quot;</span> /etc/selinux/config</span><br><span class="hljs-meta">#</span><span class="bash"> setenforce 0</span><br><span class="hljs-meta">#</span><span class="bash"> getenforce</span><br>Permissive<br></code></pre></td></tr></table></figure></li><li><p>查看是否开启了虚拟化</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> egrep <span class="hljs-string">&quot;vmx|svm&quot;</span> /proc/cpuinfo</span><br></code></pre></td></tr></table></figure><h3 id="安装基础软件包"><a href="#安装基础软件包" class="headerlink" title="安装基础软件包"></a>安装基础软件包</h3><p>配置配置<code>epel</code>源安装基础包</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> yum install epel-release</span><br><span class="hljs-meta">#</span><span class="bash"> yum install axel vim git curl wget lrzsz gcc  python-devel python-pip</span><br></code></pre></td></tr></table></figure><h3 id="安装配置docker"><a href="#安装配置docker" class="headerlink" title="安装配置docker"></a>安装配置<code>docker</code></h3></li><li><p>安装<code>docker</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> wget -O /etc/yum.repos.d/docker-ce.repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo</span><br><span class="hljs-meta">#</span><span class="bash"> yum install -y docker-ce</span><br></code></pre></td></tr></table></figure></li><li><p>配置<code>docker</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> mkdir /etc/systemd/system/docker.service.d</span><br><span class="hljs-meta">#</span><span class="bash"> tee /etc/systemd/system/docker.service.d/kolla.conf &lt;&lt; <span class="hljs-string">&#x27;EOF&#x27;</span></span><br>[Service]<br>MountFlags=shared<br>EOF<br><span class="hljs-meta">#</span><span class="bash"> vim /usr/lib/systemd/system/docker.service</span><br><span class="hljs-meta">#</span><span class="bash"> ExecStart=/usr/bin/dockerd</span><br>ExecStart=/usr/bin/dockerd --registry-mirror=http://f2d6cb40.m.daocloud.io --storage-driver=overlay2<br></code></pre></td></tr></table></figure><p>这里<code>docker</code>的文件系统我用<code>overlay2</code></p></li><li><p>启动<code>docker</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> systemctl daemon-reload</span><br><span class="hljs-meta">#</span><span class="bash"> systemctl restart docker</span><br><span class="hljs-meta">#</span><span class="bash"> systemctl <span class="hljs-built_in">enable</span> docker</span><br>Created symlink from /etc/systemd/system/multi-user.target.wants/docker.service to /usr/lib/systemd/system/docker.service.<br><span class="hljs-meta">#</span><span class="bash"> systemctl status docker</span><br><span class="hljs-meta">#</span><span class="bash"> docker info</span><br></code></pre></td></tr></table></figure><h3 id="安装ansible"><a href="#安装ansible" class="headerlink" title="安装ansible"></a>安装<code>ansible</code></h3><p><code>ansible</code>版本必须在2.0以上</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> yum -y install ansible -y</span><br></code></pre></td></tr></table></figure><h3 id="下载kolla-ansible，并安装配置"><a href="#下载kolla-ansible，并安装配置" class="headerlink" title="下载kolla-ansible，并安装配置"></a>下载<code>kolla-ansible</code>，并安装配置</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> git <span class="hljs-built_in">clone</span> https://github.com/openstack/kolla-ansible -b stable/queens</span><br><span class="hljs-meta">#</span><span class="bash"> <span class="hljs-built_in">cd</span> kolla-ansible/</span><br><span class="hljs-meta">#</span><span class="bash"> cp -r etc/kolla/ /etc/kolla/</span><br><span class="hljs-meta">#</span><span class="bash"> pip install . -i https://pypi.tuna.tsinghua.edu.cn/simple</span><br></code></pre></td></tr></table></figure><h3 id="配置globals-yml文件"><a href="#配置globals-yml文件" class="headerlink" title="配置globals.yml文件"></a>配置<code>globals.yml</code>文件</h3><figure class="highlight dts"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs dts"><span class="hljs-meta"># egrep <span class="hljs-string">&quot;^[^#]&quot;</span> /etc/kolla/globals.yml</span><br>---<br><span class="hljs-symbol">kolla_base_distro:</span> <span class="hljs-string">&quot;centos&quot;</span><br><span class="hljs-symbol">kolla_install_type:</span> <span class="hljs-string">&quot;source&quot;</span><br><span class="hljs-symbol">openstack_release:</span> <span class="hljs-string">&quot;queens&quot;</span><br><span class="hljs-symbol">kolla_internal_vip_address:</span> <span class="hljs-string">&quot;192.168.254.11&quot;</span><br><span class="hljs-symbol">docker_namespace:</span> <span class="hljs-string">&quot;kolla&quot;</span><br><span class="hljs-symbol">network_interface:</span> <span class="hljs-string">&quot;ens33&quot;</span><br><span class="hljs-symbol">neutron_external_interface:</span> <span class="hljs-string">&quot;ens34&quot;</span><br><span class="hljs-symbol">enable_haproxy:</span> <span class="hljs-string">&quot;no&quot;</span><br><span class="hljs-symbol">nova_compute_virt_type:</span> <span class="hljs-string">&quot;qemu&quot;</span><br><span class="hljs-symbol">ironic_dnsmasq_dhcp_range:</span><br><span class="hljs-symbol">tempest_image_id:</span><br><span class="hljs-symbol">tempest_flavor_ref_id:</span><br><span class="hljs-symbol">tempest_public_network_id:</span><br><span class="hljs-symbol">tempest_floating_network_name:</span><br></code></pre></td></tr></table></figure><p>说明：这里我直接在<a href="https://hub.docker.com/search/?isAutomated=0&isOfficial=0&page=1&pullCount=0&q=kolla&starCount=0">docker hub</a>上拉镜像。如果是在虚拟机里安装 <code>Kolla</code>，希望可以在 <code>OpenStack</code> 平台上创建虚拟机，那么你需要在 <code>globals.yml</code> 文件中把 <code>nova_compute_virt_type</code> 配置项设置为 <code>qemu</code>，默认是 <code>KVM</code>。</p></li></ul><h3 id="安装kolla"><a href="#安装kolla" class="headerlink" title="安装kolla"></a>安装<code>kolla</code></h3><ul><li><p>生成密码文件</p><figure class="highlight vala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs vala"><span class="hljs-meta"># kolla-genpwd</span><br></code></pre></td></tr></table></figure></li><li><p>编辑 <code>/etc/kolla/passwords.yml</code> 文件，配置 <code>keystone</code> 管理员用户的密码。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">keystone_admin_password: password<br></code></pre></td></tr></table></figure><p>同时，也是登录 <code>Dashboard，admin</code> 使用的密码，你可以根据自己需要进行修改。</p></li><li><p>运行 <code>prechecks</code> 检查配置是否正确，如果有错误，可以先忽略。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> kolla-ansible prechecks</span><br></code></pre></td></tr></table></figure></li><li><p>从<code>docker hub</code>上<code>pull</code>镜像</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> kolla-ansible pull</span><br></code></pre></td></tr></table></figure></li><li><p>部署<code>openstack</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> kolla-ansible deploy</span><br></code></pre></td></tr></table></figure></li><li><p>创建环境变量文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> kolla-ansible post-deploy</span><br></code></pre></td></tr></table></figure><p>这样就创建了<code>/etc/kolla/admin-openrc.sh</code> 环境变量文件。</p></li><li><p>安装 <code>OpenStack Client</code> 端</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> pip install python-openstackclient</span><br></code></pre></td></tr></table></figure></li><li><p>编辑<code>init-runonce</code>文件,设置<code>public network</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> vim /usr/share/kolla-ansible/init-runonce</span><br>EXT_NET_CIDR=&#x27;10.10.20.0/24&#x27;<br>EXT_NET_RANGE=&#x27;start=10.10.20.110,end=10.10.20.254&#x27;<br>EXT_NET_GATEWAY=&#x27;10.10.20.1&#x27;<br></code></pre></td></tr></table></figure></li><li><p>加载<code>OpenStack CLI</code>所需的环境变量</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> <span class="hljs-built_in">source</span> /etc/kolla/admin-openrc.sh</span><br></code></pre></td></tr></table></figure></li><li><p>初始化部署</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> <span class="hljs-built_in">cd</span> /usr/share/kolla-ansible/ &amp;&amp; ./init-runonce</span><br></code></pre></td></tr></table></figure></li><li><p>登陆<code>Dashboard</code><br>用浏览器访问<code>192.168.254.15</code>登陆<code>Dashboard</code><br><img src="https://ljw.howieli.cn/blog/2018-03-29/WX20180329-170306.png"><br><img src="https://ljw.howieli.cn/blog/2018-03-29/WX20180329-170337.png"></p></li><li><p>如果，在部署过程中失败了，亦或是变更了配置信息，需要重新部署，则执行如下命令，清除掉已部署的 <code>Docker</code> 容器，即 <code>OpenStack</code> 服务</p><figure class="highlight gradle"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs gradle"># kolla-ansible destroy -i <span class="hljs-regexp">/home/mu</span>ltinode --yes-i-really-really-mean-it<br></code></pre></td></tr></table></figure></li><li><p>除此外，还有一些小工具，在需要时，可以使用</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> 可用于从系统中移除部署的容器:</span><br>tools/cleanup-containers<br><span class="hljs-meta">#</span><span class="bash"> 可用于移除由于网络变化引发的 Docker 启动的neutron-agents 主机：</span><br>tools/cleanup-host<br><span class="hljs-meta">#</span><span class="bash"> 可用于从本地缓存中移除所有的 Docker image</span><br>tools/cleanup-images<br></code></pre></td></tr></table></figure><h3 id="故障诊断与排除"><a href="#故障诊断与排除" class="headerlink" title="故障诊断与排除"></a>故障诊断与排除</h3><p>通过 <code>Kolla</code> 和 <code>Ansible</code> 部署或运行 <code>OpenStack</code> 环境时，如果出现问题，通常可以使用如下一些方法来排查/解决</p></li><li><p>查看指定容器（即指定的服务）的输出日志信息</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> docker logs container_name</span><br></code></pre></td></tr></table></figure></li><li><p>查看指定服务的日志<br>直接 <code>CD</code> 到主机的 <code>/var/lib/docker/volumes/kollalogs/data/</code> 目录下，查看指定服务的日志信息。</p></li><li><p>查看容器的状态</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> docker ps</span><br></code></pre></td></tr></table></figure><p>可以使用 <code>docker ps -a</code> 命令查看到安装的 <code>OpenStack</code> 所有服务的容器</p></li><li><p>进入某个容器</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> docker <span class="hljs-built_in">exec</span> -it -u root container_name bash</span><br></code></pre></td></tr></table></figure></li></ul>]]></content>
    
    
    <categories>
      
      <category>openstack</category>
      
    </categories>
    
    
    <tags>
      
      <tag>openstack</tag>
      
      <tag>kolla</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>cinder删除lvm卷很慢的问题</title>
    <link href="/posts/cinder%E5%88%A0%E9%99%A4lvm%E5%8D%B7%E5%BE%88%E6%85%A2%E7%9A%84%E9%97%AE%E9%A2%98/"/>
    <url>/posts/cinder%E5%88%A0%E9%99%A4lvm%E5%8D%B7%E5%BE%88%E6%85%A2%E7%9A%84%E9%97%AE%E9%A2%98/</url>
    
    <content type="html"><![CDATA[<h3 id="cinder删除lvm卷很慢的问题"><a href="#cinder删除lvm卷很慢的问题" class="headerlink" title="cinder删除lvm卷很慢的问题"></a>cinder删除lvm卷很慢的问题</h3><p>最近搞<code>kolla</code>部署<code>cinder</code>，使用<code>lvm</code>做后端，在<code>cinder-volume</code>服务删除数据卷，数据卷会删除的很慢，在删除这段时间发现宿主机的<code>load</code>飙高，但是检查虚拟机负载没有异常。</p><a id="more"></a><h3 id="查找原因"><a href="#查找原因" class="headerlink" title="查找原因"></a>查找原因</h3><p>使用<code>iotop</code>查看<br><img src="https://ljw.howieli.cn/blog/2018-03-20/WechatIMG33.jpeg"></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">DEBUG oslo_concurrency.processutils [req-beeeb583-c07e-4f23-8b63-1f76fc729c9a 0a266a36e53e4469a376f5baa4375064 0a266a36e53e4469a376f5baa4375064 - default default] CMD &quot;sudo cinder-rootwrap /etc/cinder/rootwrap.conf dd count=0 if=/dev/zero of=/dev/mapper/cinder--volumes-volume--beeeb583--c07e--4f23--8b63--1f76fc729c9a  oflag=direct&quot; returned: 0 in 0.127s execute /usr/lib/python2.7/site-packages/oslo_concurrency/processutils.py:374<br></code></pre></td></tr></table></figure><p>从日志里面看到，<code>volume</code>服务调用<code>dd</code>命令对数据卷进行数据清空，同时采用<code>oflag=direct</code>参数直接操作块设备。所以，<code>cinder</code>在删除卷的时间长短和卷的大小有着直接关系。（删除期间<code>iowait</code>值会增高，怪不得<code>load</code>也会飙高）</p><h3 id="原因分析"><a href="#原因分析" class="headerlink" title="原因分析"></a>原因分析</h3><p><code>cinder</code>在删除卷之前要对其做<code>dd</code>操作，是因为<code>LVM</code>在<code>lvremove</code>后重新<code>lvcreate</code>，新的<code>lv</code>里面还保留着老的数据，原来直接删除的<code>lv</code>并不会抹除卷上的数据。<code>cinder</code>之所以要用<code>dd zero</code>,就是为了避免租户<code>A</code>删除卷后，数据不会串到租户<code>B</code>新创建的数据卷上。这样一方面保证租户<code>A</code>的数据安全性，另一方面也避免租户<code>B</code>在使用数据卷的产生的疑惑。</p><h3 id="处理问题"><a href="#处理问题" class="headerlink" title="处理问题"></a>处理问题</h3><ul><li><p>关闭volume安全删除</p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs ini"><span class="hljs-comment"># Allowed values: none, zero, shred</span><br><span class="hljs-attr">volume_clear</span> = none<br></code></pre></td></tr></table></figure><p><code>volume_clear</code>采用<code>shred</code>永久删除,</p></li><li><p>置零<code>volume</code>头部<code>100MB</code>左右数据</p><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs pgsql"># Size <span class="hljs-keyword">in</span> MiB <span class="hljs-keyword">to</span> wipe at <span class="hljs-keyword">start</span> <span class="hljs-keyword">of</span> <span class="hljs-built_in">old</span> volumes. <span class="hljs-number">0</span> =&gt; <span class="hljs-keyword">all</span> (<span class="hljs-type">integer</span> <span class="hljs-keyword">value</span>)<br>volume_clear_size = <span class="hljs-number">100</span><br></code></pre></td></tr></table></figure></li><li><p>调整<code>dd</code>进程<code>io</code>调度优先策略</p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs ini"><span class="hljs-comment"># The flag to pass to ionice to alter the i/o priority of the process used to</span><br><span class="hljs-comment"># zero a volume after deletion, for example &quot;-c3&quot; for idle only priority.</span><br><span class="hljs-comment"># (string value)</span><br><span class="hljs-attr">volume_clear_ionice</span> = -c3<br></code></pre></td></tr></table></figure><h4 id="保证数据绝对安全"><a href="#保证数据绝对安全" class="headerlink" title="保证数据绝对安全"></a>保证数据绝对安全</h4><p><code>volume_clear</code>采用<code>shred</code>永久删除，根据服务器情况适当调整<code>volume_clear_ionice</code>的值。<br>shred会用一些随机内容覆盖文件所在的节点和数据块，<code>cinder</code>在这里条用<code>shred</code>默认参数采用<code>-n 3</code>即重写<code>3</code>次。</p><h4 id="数据相对安全的同时，降低数据卷删除时间"><a href="#数据相对安全的同时，降低数据卷删除时间" class="headerlink" title="数据相对安全的同时，降低数据卷删除时间"></a>数据相对安全的同时，降低数据卷删除时间</h4><p><code>volume_clear</code>采用<code>zero</code>填充，根据情况设置<code>volume_clear_size</code>大小，我们都知道，磁盘的开头部分保存着文件系统的元数据以及索引，清空这部分可以一定程度上保证数据的安全。</p></li></ul><p>至于<code>volume_clear=none</code>这种直接删除<code>lv</code>而不清空数据的方式，我就不建议采用了</p>]]></content>
    
    
    <categories>
      
      <category>openstack</category>
      
    </categories>
    
    
    <tags>
      
      <tag>openstack</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ceph写流程分析</title>
    <link href="/posts/ceph%E5%86%99%E6%B5%81%E7%A8%8B%E5%88%86%E6%9E%90/"/>
    <url>/posts/ceph%E5%86%99%E6%B5%81%E7%A8%8B%E5%88%86%E6%9E%90/</url>
    
    <content type="html"><![CDATA[<h3 id="ceph写流程分析"><a href="#ceph写流程分析" class="headerlink" title="ceph写流程分析"></a>ceph写流程分析</h3><blockquote><p>最近年底了，工作也不太忙了，这几天看看ceph，这几天总结的ceph写流程分析笔记（基于jewel版本&gt;10.2.0），现分享出来，欢迎指点。</p></blockquote><p><img src="https://ljw.howieli.cn/blog/2018-02-09/ceph.jpg"></p><a id="more"></a><h3 id="rbd到OSD映射关系"><a href="#rbd到OSD映射关系" class="headerlink" title="rbd到OSD映射关系"></a>rbd到OSD映射关系</h3><p>客户端使用<code>RBD</code>设备，使用<code>librbd</code>、<code>librados</code>库进行访问管理块设备。</p><ul><li><p>1、创建 一个<code>pool</code>，为这个<code>pool</code>指定<code>pg</code>的数量，同时在这个<code>pool</code>中指明保存数据的副本数（通常为<code>3</code>个副本）。</p></li><li><p>2、在这个<code>pool</code>中创建一个<code>rbd</code>设备<code>rbd0</code>，那么这个<code>rbd0</code>都会保存三份，在创建<code>rbd0</code>时必须指定<code>rbd</code>的<code>size</code>，对于这个<code>rbd0</code>的任何操作不能超过这个<code>size</code>。</p></li><li><p>3、将这个块设备进行切块，每个块的大小默认为<code>4M</code>，并且每个块都有一个名字，名字就是<code>object+</code>序号。</p></li><li><p>4、将每个<code>object</code>通过<code>pg</code>进行副本位置的分配，<code>pg</code>会寻找<code>3</code>个<code>osd</code>，把这个<code>object</code>分别保存在这三个<code>osd</code>上。最后对于<code>object</code>的存储就变成了存储一个文件 <code>rbd0.object1.file</code>。数据层次映射图如下：<br><img src="https://ljw.howieli.cn/blog/2018-02-09/WX20180209-105337.png"></p></li></ul><p>经过<code>pool</code>，<code>rbd</code>，<code>object</code>、<code>pg</code>的层层映射关系，在<code>PG</code>这一层中，已经知道存储数据的<code>3</code>个<code>OSD</code>所在位置及主从关系。<br>客户端与<code>primay OSD</code>建立<code>SOCKET</code> 通信，将要写入的数据传给<code>primary OSD</code>，由<code>primary OSD</code>再将数据发送给其他<code>replica OSD</code>数据节点。</p><h4 id="RBD保存形式"><a href="#RBD保存形式" class="headerlink" title="RBD保存形式"></a>RBD保存形式</h4><p>如下图所示，<code>Ceph</code> 系统中不同层次的组件/用户所看到的数据的形式是不一样的：<br><img src="https://ljw.howieli.cn/blog/2018-02-09/697113-20150928152213918-1722034221.jpg"></p><ul><li><code>Ceph</code> 客户端所见的是一个完整的连续的二进制数据块，其大小为创建 <code>RBD image</code> 是设置的大小或者 <code>resize</code> 的大小，客户端可以从头或者从某个位置开始写入二进制数据。</li><li><code>librados</code> 负责在 <code>RADOS</code> 中创建对象（<code>object</code>），其大小为 <code>pool</code> 的 <code>order</code> 决定，默认情况下 <code>order = 22</code> 此时 <code>object</code> 大小为 <code>4MB</code>；以及负责将客户端传入的二进制块条带化为若干个条带（<code>stripe</code>）。</li><li><code>librados</code> 控制哪个条带由哪个 <code>OSD</code> 写入（条带 —写入哪个—-&gt; <code>object</code> —-位于哪个 —-&gt; <code>OSD</code>）</li></ul><p><code>OSD</code> 负责创建在文件系统中创建文件，并将 <code>librados</code> 传入的数据写入数据。</p><ol><li><p><code>Ceph client</code> 调用 <code>librados</code> 创建一个 <code>RBD image</code>，这时候不会做存储空间分配，而是创建若干元数据对象来保存元数据信息。</p></li><li><p><code>Ceph client</code> 调用 <code>librados</code> 开始写数据。<code>librados</code> 计算条带、<code>object</code> 等，然后开始写第一个 <code>stripe</code> 到特定的目标 <code>object</code>。</p></li><li><p><code>librados</code> 根据 <code>CRUSH</code> 算法，计算出 <code>object</code> 所对应的主 <code>OSD ID</code>，并将二进制数据发给它。</p></li><li><p>主 <code>OSD</code> 负责调用文件系统接口将二进制数据写入磁盘上的文件（每个 <code>object</code> 对应一个 <code>file，file</code> 的内容是一个或者多个 <code>stripe</code>）。</p></li><li><p>主 <code>ODS</code> 完成数据写入后，它使用 <code>CRUSH</code> 算啊计算出第二个<code>OSD</code>（<code>secondary OSD</code>）和第三个<code>OSD</code>（<code>tertiary OSD</code>）的位置，然后向这两个 <code>OSD</code> 拷贝对象。都完成后，它向 <code>ceph client</code>反馈该 <code>object</code> 保存完毕。</p><p><code>Ceph client</code> 向一个 <code>RBD image</code> 写入二进制数据（假设 pool 的拷贝份数为 3）<br><img src="https://ljw.howieli.cn/blog/2018-02-09/WX20180209-110527.png"></p></li></ol><h3 id="客户的写流程操作"><a href="#客户的写流程操作" class="headerlink" title="客户的写流程操作"></a>客户的写流程操作</h3><p>在客户端使用 rbd 时一般有两种方法：</p><ul><li>第一种 是 <code>Kernel rbd</code>。就是创建了<code>rbd</code>设备后，把<code>rbd</code>设备<code>map</code>到内核中，形成一个虚拟的块设备，这时这个块设备同其他通用块设备一样，一般的设备文件为<code>/dev/rbd0</code>，后续直接使用这个块设备文件就可以了，可以把 <code>/dev/rbd0</code> 格式化后 <code>mount</code> 到某个目录，也可以直接作为裸设备使用。这时对<code>rbd</code>设备的操作都通过<code>kernel rbd</code>操作方法进行的。 </li><li>第二种是 <code>librbd</code> 方式。就是创建了<code>rbd</code>设备后，这时可以使用<code>librbd</code>、<code>librados</code>库进行访问管理块设备。这种方式不会<code>map</code>到内核，直接调用<code>librbd</code>提供的接口，可以实现对<code>rbd</code>设备的访问和管理，但是不会在客户端产生块设备文件。</li></ul><p>应用写入<code>rbd</code>块设备的过程：</p><ol><li>应用调用 <code>librbd</code> 接口或者对<code>linux</code> 内核虚拟块设备写入二进制块。下面以 <code>librbd</code> 为例。</li><li><code>librbd</code> 对二进制块进行分块，默认块大小为 <code>4M</code>，每一块都有名字，成为一个对象</li><li><code>librbd</code> 调用 <code>librados</code> 将对象写入 <code>Ceph</code> 集群</li><li><code>librados</code> 向主 <code>OSD</code> 写入分好块的二进制数据块 (先建立<code>TCP/IP</code>连接，然后发送消息给 <code>OSD</code>，<code>OSD</code> 接收后写入其磁盘)</li><li>主 <code>OSD</code> 负责同时向一个或者多个次 <code>OSD</code> 写入副本。注意这里是写到日志（<code>Journal</code>）就返回，因此，使用<code>SSD</code>作为<code>Journal</code>的话，可以提高响应速度，做到服务器端对客户端的快速同步返回写结果（<code>ack</code>）。</li><li>当主次<code>OSD</code>都写入完成后，主 <code>OSD</code> 向客户端返回写入成功。</li><li>当一段时间（也许得几秒钟）后<code>Journal</code> 中的数据向磁盘写入成功后，<code>Ceph</code>通过事件通知客户端数据写入磁盘成功（<code>commit</code>），此时，客户端可以将写缓存中的数据彻底清除掉了。</li><li>默认地，<code>Ceph</code> 客户端会缓存写入的数据直到收到集群的<code>commit</code>通知。如果此阶段内（在写方法返回到收到<code>commit</code>通知之间）<code>OSD</code> 出故障导致数据写入文件系统失败，<code>Ceph</code> 将会允许客户端重做尚未提交的操作（<code>replay</code>）。因此，<code>PG</code> 有个状态叫 <code>replay：“The placement group is waiting for clients to replay operations after an OSD crashed.”</code>。<br><img src="https://ljw.howieli.cn/blog/2018-02-09/697113-20160507205321796-1810415144.jpg"><br>也就是，文件系统负责文件处理，<code>librbd</code> 负责块处理，<code>librados</code> 负责对象处理，<code>OSD</code> 负责将数据写入在<code>Journal</code>和磁盘中。</li></ol><h3 id="osd端写操作处理流程"><a href="#osd端写操作处理流程" class="headerlink" title="osd端写操作处理流程"></a>osd端写操作处理流程</h3><p>而对于写操作而言，由于要保证数据写入的同步性就会复杂很多：</p><ol><li><p>首先客户端会将数据发送给主osd，</p></li><li><p>主<code>osd</code>同样要先进行写操作预处理，完成后它要发送写消息给其他的从<code>osd</code>，让他们对副本<code>pg</code>进行更改，</p></li><li><p>从<code>osd</code>通过<code>FileJournal</code>完成写操作到<code>Journal</code>中后发送消息告诉主<code>osd</code>说完成，进入<code>5</code></p></li><li><p>当主<code>osd</code>收到所有的从<code>osd</code>完成写操作的消息后，会通过<code>FileJournal</code>完成自身的写操作到<code>Journal</code>中。完成后会通知客户端，已经完成了写操作。</p></li><li><p>主<code>osd</code>，从<code>osd</code>的线程开始工作调用<code>Filestore</code>将<code>Journal</code>中的数据写入到底层文件系统中。</p></li></ol><p>今天就先总结这么多，总结的不太好，请谅解</p>]]></content>
    
    
    <categories>
      
      <category>ceph</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ceph</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>手动构建Openstack镜像</title>
    <link href="/posts/%E6%89%8B%E5%8A%A8%E6%9E%84%E5%BB%BAOpenstack%E9%95%9C%E5%83%8F/"/>
    <url>/posts/%E6%89%8B%E5%8A%A8%E6%9E%84%E5%BB%BAOpenstack%E9%95%9C%E5%83%8F/</url>
    
    <content type="html"><![CDATA[<h3 id="手动构建Openstack镜像"><a href="#手动构建Openstack镜像" class="headerlink" title="手动构建Openstack镜像"></a>手动构建Openstack镜像</h3><p>本文以centos7镜像为例，详细介绍手动制作Openstack镜像的步骤，镜像支持一下几个功能：</p><ul><li>支持密码注入功能(nova boot时通过–admin-pass参数指定设置初始密码）</li><li>支持根分区自动调整(根分区自动调整为flavor disk大小，而不是原始镜像分区大小)</li><li>支持动态修改密码(使用nova set-password命令可以修改管理员密码)</li></ul><p>镜像的宿主机操作系统为CentOs7.4，开启了VT功能(使用kvm-ok命令验证)并安装了libvirt系列工具，包括virsh、virt-manager、libguestfs-tools等。</p><a id="more"></a><h3 id="下载iso镜像"><a href="#下载iso镜像" class="headerlink" title="下载iso镜像"></a>下载iso镜像</h3><p>下载centos7.4.iso镜像，可以选择中国镜像源，相对国外镜像源下载速度会快很多，这里我已经下载好了。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> qemu-img info CentOS-7-x86_64-DVD-1708.iso</span> <br>image: CentOS-7-x86_64-DVD-1708.iso<br>file format: raw<br>virtual size: 4.2G (4521459712 bytes)<br>disk size: 4.2G<br></code></pre></td></tr></table></figure><h3 id="创建虚拟机"><a href="#创建虚拟机" class="headerlink" title="创建虚拟机"></a>创建虚拟机</h3><p>首先创建一个qcow2格式镜像文件，用于虚拟机的根磁盘，大小10G就够了。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> qemu-img create -f qcow2 centos.qcow2 10G</span><br>Formatting &#x27;centos.qcow2&#x27;, fmt=qcow2 size=10737418240 encryption=off cluster_size=65536 lazy_refcounts=off <br></code></pre></td></tr></table></figure><p>使用以下脚本创建并启动虚拟机：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-attribute">NAME</span>=centos<br><span class="hljs-attribute">ROOT_DISK</span>=centos.qcow2<br><span class="hljs-attribute">CDROM</span>=`pwd`/CentOS-7-x86_64-DVD-1708.iso<br>virt-install --virt-type kvm --name <span class="hljs-variable">$NAME</span> --ram 1024 \<br>--disk <span class="hljs-variable">$ROOT_DISK</span>,<span class="hljs-attribute">format</span>=qcow2 \<br>--network <span class="hljs-attribute">network</span>=default \<br>--graphics vnc,<span class="hljs-attribute">listen</span>=0.0.0.0 --noautoconsole \<br><span class="hljs-attribute">--os-type</span>=linux <span class="hljs-attribute">--os-variant</span>=rhel7 \<br><span class="hljs-attribute">--cdrom</span>=<span class="hljs-variable">$CDROM</span><br></code></pre></td></tr></table></figure><p>启动完成后，使用vnc client连接或者使用virt-manager、virt-viewer连接,这里我直接用virt-manager。</p><h3 id="安装OS"><a href="#安装OS" class="headerlink" title="安装OS"></a>安装OS</h3><p>进入虚拟机控制台可以看到CentOS的启动菜单，选择Install Centos 7，继续选择语言后将进入INSTALLION SUMMARY，其中大多数配置默认即可，SOFTWARE SELECTION选择Minimal Install，INSTALLATION DESTINATION需要选择手动配置分区，我们只需要一个根分区即可，不需要swap分区，文件系统选择ext4或者xfs，存储驱动选择Virtio Block Device，如图：<br><img src="https://ljw.howieli.cn/blog/2018-1-12/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20180112113728.png"></p><p>配置完成后就可以开始安装了，在CONFIGURATION中设置root临时密码，只需要暂时记住这个临时密码，制作完后cloud-init会重新设置root初始密码。</p><p>大约几分钟后，即可自动完成安装配置工作，最后点击右下角的reboot重启退出虚拟机。</p><h3 id="配置OS"><a href="#配置OS" class="headerlink" title="配置OS"></a>配置OS</h3><p>安装好操作系统后，需要进行配置才能作为glance镜像使用，启动虚拟机</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> virsh list --all</span><br> Id    名称                         状态<br>----------------------------------------------------<br> -     centos                         关闭<br><span class="hljs-meta">#</span><span class="bash"> virsh start centos</span><br></code></pre></td></tr></table></figure><p>如果云主机不需要支持root ssh远程登录，需要关闭root远程ssh登录功能，修改配置文件/etc/ssh/sshd_config并修改PermitRootLogin值为no，默认是开启的（这里我不做更改，如果更改需要重启ssh服务生效）。</p><p>为了加快安装速度，可以配置为本地软件源仓库，若没有本地镜像仓库，则选择国内的软件源，相对官网的速度下载要快。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> yum -y install wget</span><br><span class="hljs-meta">#</span><span class="bash"> <span class="hljs-built_in">cd</span> /etc/yum.repos.d/ &amp;&amp; rm -rf * &amp;&amp; <span class="hljs-built_in">cd</span></span><br><span class="hljs-meta">#</span><span class="bash"> wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo</span><br><span class="hljs-meta">#</span><span class="bash"> wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo</span><br></code></pre></td></tr></table></figure><h3 id="acpid"><a href="#acpid" class="headerlink" title="acpid"></a>acpid</h3><p><a href="https://wiki.archlinux.org/index.php/Acpid">acpid</a>是一个用户空间的服务进程, 用来处理电源相关事件,比如将kernel中的电源事件转发给应用程序，告诉应用程序安全的退出，防止应用程序异常退出导致数据损坏。libvirt可以通过向guest虚拟机发送acpid事件触发电源操作，使虚拟机安全关机、重启等操作，相对于强制执行关闭电源操作更安全。通过acpid事件发送开关机信号即我们经常所说的软重启或者软关机。<br>为了支持软操作，虚拟机需要安装acpid服务，并设置开机自启动：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> yum -y install acpid</span><br><span class="hljs-meta">#</span><span class="bash"> systemctl <span class="hljs-built_in">enable</span> acpid</span><br></code></pre></td></tr></table></figure><p>提示：</p><ul><li>用户执行重启或者关机操作时，OpenStack会首先尝试调用libvirt的shutdown方法，即软关机。</li><li>当软关机执行失败或者超时(默认120秒)，则会调动libvirt的destroy方法，即强制关机，因此如果虚拟机关机或者重启很慢，很可能是acpid没有正常运行。</li><li>为了使虚拟机进程安全退出，减少数据损坏风险，尽量使用软操作，硬操作可能导致程序崩溃或者数据丢失。</li></ul><h3 id="console-log"><a href="#console-log" class="headerlink" title="console log"></a>console log</h3><p>当操作系统内核崩溃时会报出内核系统crash出错信息，通常启动的时候一闪而过, 而此时系统还没有起来，不能通过远程工具(比如ssh)进入系统查看，我们可以通过配置grub，把这些日志重定向到Serial Console中，这样我们就可以通过Serial console来访问错误信息，以供分析和排错使用。<br>修改配置文件/etc/default/grub，设置GRUB_CMDLINE_LINUX，：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">GRUB_CMDLINE_LINUX=&quot;crashkernel=auto console=tty0 console=ttyS0,115200n8&quot;<br></code></pre></td></tr></table></figure><p>通过这个配置，内核信息会以115200的波特率同时发送到tty0和ttyS0串行端口设备。libvirt可以通过一个普通文件模拟这个串行端口：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">serial</span> <span class="hljs-attr">type</span>=<span class="hljs-string">&#x27;file&#x27;</span>&gt;</span><br>      <span class="hljs-tag">&lt;<span class="hljs-name">source</span> <span class="hljs-attr">path</span>=<span class="hljs-string">&#x27;/var/lib/nova/instances/99579ce1-f4c4-4031-a56c-68e85a3d037a/console.log&#x27;</span>/&gt;</span><br>      <span class="hljs-tag">&lt;<span class="hljs-name">target</span> <span class="hljs-attr">port</span>=<span class="hljs-string">&#x27;0&#x27;</span>/&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">serial</span>&gt;</span><br></code></pre></td></tr></table></figure><p>这样内核产生的日志发到ttyS0，实际上写到console.log文件中。</p><p>OpenStack通过nova console-log命令可以获取该文件内容，查看错误日志。</p><h3 id="qemu-guest-agent"><a href="#qemu-guest-agent" class="headerlink" title="qemu-guest-agent"></a>qemu-guest-agent</h3><p>qemu-guest-agent是运行在虚拟机内部的一个服务，libvirt会在本地创建一个unix socket，模拟为虚拟机内部的一个串口设备，从而实现了宿主机与虚拟机通信，这种方式不依赖于TCP/IP网络，实现方式简单方便。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">channel</span> <span class="hljs-attr">type</span>=<span class="hljs-string">&#x27;unix&#x27;</span>&gt;</span><br>      <span class="hljs-tag">&lt;<span class="hljs-name">source</span> <span class="hljs-attr">mode</span>=<span class="hljs-string">&#x27;bind&#x27;</span> <span class="hljs-attr">path</span>=<span class="hljs-string">&#x27;/var/lib/libvirt/qemu/org.qemu.guest_agent.0.instance-00003c2c.sock&#x27;</span>/&gt;</span><br>      <span class="hljs-tag">&lt;<span class="hljs-name">target</span> <span class="hljs-attr">type</span>=<span class="hljs-string">&#x27;virtio&#x27;</span> <span class="hljs-attr">name</span>=<span class="hljs-string">&#x27;org.qemu.guest_agent.0&#x27;</span>/&gt;</span><br>      <span class="hljs-tag">&lt;<span class="hljs-name">address</span> <span class="hljs-attr">type</span>=<span class="hljs-string">&#x27;virtio-serial&#x27;</span> <span class="hljs-attr">controller</span>=<span class="hljs-string">&#x27;0&#x27;</span> <span class="hljs-attr">bus</span>=<span class="hljs-string">&#x27;0&#x27;</span> <span class="hljs-attr">port</span>=<span class="hljs-string">&#x27;1&#x27;</span>/&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">channel</span>&gt;</span><br></code></pre></td></tr></table></figure><p>如上宿主机的socket文件为org.qemu.guest_agent.0.instance-00003c2c.sock，在虚拟机内部为/dev/virtio-ports/org.qemu.guest_agent.0。</p><p>通过这种方式，宿主机可以发送指令写到socket文件中，虚拟机内部的qemu-guest-agent会轮询查看这个串行设备是否有指令，一旦接收到指令就可以执行对应的脚本，从而实现了宿主机控制虚拟机执行命令的功能，其中最常用的指令就是通过libvirt修改虚拟机密码。更多关于qemu-guest-agent请参考<a href="http://link.zhihu.com/?target=http://wiki.qemu.org/Features/QAPI/GuestAgent">官方文档</a>。</p><p>为了支持OpenStack平台动态修改虚拟机密码功能，我们需要手动安装qemu-guest-agent：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> yum install -y qemu-guest-agent</span><br></code></pre></td></tr></table></figure><p>修改/etc/sysconfig/qemu-ga配置文件:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs shell">virsh qemu-agent-command instance-000028d5 &#x27;&#123;&quot;execute&quot;:&quot;guest-info&quot;&#125;&#x27; | python -m json.tool | grep &#x27;name&#x27; | cut -d &#x27;:&#x27; -f 2 | tr -d &#x27;&quot;,&#x27;<br> ...<br> guest-set-user-password<br> guest-get-fsinfo<br> guest-set-vcpus<br> guest-get-vcpus<br> ...<br></code></pre></td></tr></table></figure><p>确认包含guest-set-user-password指令，支持修改管理员密码。</p><h3 id="zeroconf"><a href="#zeroconf" class="headerlink" title="zeroconf"></a>zeroconf</h3><p>zeroconf启动时会自动创建一条路由169.254.0.0/16，而虚拟机访问metadata服务的地址正好是169.254.169.254，如果启动了zeroconf服务，由于路由冲突，虚拟机不能通过169.254.169.254路由到网络节点的metadata服务了。OpenStack虚拟机通常都是通过DHCP获取IP的，因此我们并不需要zeroconf服务。为了虚拟机能够访问metadata服务，我们必须禁止zeroconf服务，关于该问题的更详细讨论可参考<a href="https://zhuanlan.zhihu.com/p/30385809">bug#983611</a>：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;NOZEROCONF=yes&quot;</span> &gt;&gt; /etc/sysconfig/network</span><br></code></pre></td></tr></table></figure><h3 id="cloud-init"><a href="#cloud-init" class="headerlink" title="cloud-init"></a>cloud-init</h3><p>接下来安装cloud-init，cloud-init是虚拟机第一次启动时执行的脚本，主要负责从metadata服务中拉取配置信息，完成虚拟机的初始化工作，比如设置主机名、初始化密码以及注入密钥等。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> yum install -y cloud-init</span><br></code></pre></td></tr></table></figure><h3 id="growpart"><a href="#growpart" class="headerlink" title="growpart"></a>growpart</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> yum update -y</span><br><span class="hljs-meta">#</span><span class="bash"> yum install -y epel-release</span><br><span class="hljs-meta">#</span><span class="bash"> yum install -y cloud-utils-growpart.x86.64</span><br><span class="hljs-meta">#</span><span class="bash"> rpm -qa kernel | sed <span class="hljs-string">&#x27;s/^kernel-//&#x27;</span>  | xargs -I &#123;&#125; dracut -f /boot/initramfs-&#123;&#125;.img &#123;&#125;</span><br></code></pre></td></tr></table></figure><p>完成以上工作后，我们的镜像配置基本结束，删除一些无用文件，清理history命令后执行关机：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> shutdown -h now</span><br></code></pre></td></tr></table></figure><h3 id="移除本地信息"><a href="#移除本地信息" class="headerlink" title="移除本地信息"></a>移除本地信息</h3><p>在宿主机上运行以下命名，移除宿主机信息，比如mac地址等。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> virt-sysprep -d centos</span><br>[   0.0] Examining the guest ...<br>[  41.1] Performing &quot;abrt-data&quot; ...<br>[  42.4] Performing &quot;backup-files&quot; ...<br>[  44.4] Performing &quot;bash-history&quot; ...<br>[  44.4] Performing &quot;blkid-tab&quot; ...<br>[  44.5] Performing &quot;crash-data&quot; ...<br>[  44.5] Performing &quot;cron-spool&quot; ...<br>[  44.5] Performing &quot;dhcp-client-state&quot; ...<br>[  44.5] Performing &quot;dhcp-server-state&quot; ...<br>[  44.5] Performing &quot;dovecot-data&quot; ...<br>[  44.5] Performing &quot;logfiles&quot; ...<br>[  44.7] Performing &quot;machine-id&quot; ...<br>[  44.7] Performing &quot;mail-spool&quot; ...<br>[  44.7] Performing &quot;net-hostname&quot; ...<br>[  44.8] Performing &quot;net-hwaddr&quot; ...<br>[  44.8] Performing &quot;pacct-log&quot; ...<br>[  44.8] Performing &quot;package-manager-cache&quot; ...<br>[  44.8] Performing &quot;pam-data&quot; ...<br>[  44.8] Performing &quot;passwd-backups&quot; ...<br>[  44.8] Performing &quot;puppet-data-log&quot; ...<br>[  44.8] Performing &quot;rh-subscription-manager&quot; ...<br>[  44.8] Performing &quot;rhn-systemid&quot; ...<br>[  44.8] Performing &quot;rpm-db&quot; ...<br>[  44.8] Performing &quot;samba-db-log&quot; ...<br>[  44.8] Performing &quot;script&quot; ...<br>[  44.8] Performing &quot;smolt-uuid&quot; ...<br>[  44.8] Performing &quot;ssh-hostkeys&quot; ...<br>[  44.8] Performing &quot;ssh-userdir&quot; ...<br>[  44.8] Performing &quot;sssd-db-log&quot; ...<br>[  44.9] Performing &quot;tmp-files&quot; ...<br>[  44.9] Performing &quot;udev-persistent-net&quot; ...<br>[  44.9] Performing &quot;utmp&quot; ...<br>[  44.9] Performing &quot;yum-uuid&quot; ...<br>[  44.9] Performing &quot;customize&quot; ...<br>[  44.9] Setting a random seed<br>[  45.4] Performing &quot;lvm-uuids&quot; ...<br></code></pre></td></tr></table></figure><p>删除虚拟机，镜像制作完成。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> virsh list --all</span><br> Id    名称                         状态<br>----------------------------------------------------<br> -     centos                         关闭<br><br><span class="hljs-meta">#</span><span class="bash"> virsh undefine centos</span><br>域 centos 已经被取消定义<br><br><span class="hljs-meta">#</span><span class="bash"> virsh list --all</span><br> Id    名称                         状态<br>----------------------------------------------------<br><br></code></pre></td></tr></table></figure><h3 id="上传镜像"><a href="#上传镜像" class="headerlink" title="上传镜像"></a>上传镜像</h3><p>镜像制作完成，上传centos.qcow2到glance服务中</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> glance image-create --file ./centos.qcow2 --disk-format qcow2 \</span><br><span class="bash">    --container-format bare --name CentOS-7.2 --progress</span><br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>openstack</category>
      
    </categories>
    
    
    <tags>
      
      <tag>openstack</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ceph ( requests are blocked ) 异常解决方法</title>
    <link href="/posts/ceph-requests-are-blocked-%E5%BC%82%E5%B8%B8%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95/"/>
    <url>/posts/ceph-requests-are-blocked-%E5%BC%82%E5%B8%B8%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h3 id="ceph-requests-are-blocked-异常解决方法"><a href="#ceph-requests-are-blocked-异常解决方法" class="headerlink" title="ceph ( requests are blocked ) 异常解决方法"></a>ceph ( requests are blocked ) 异常解决方法</h3><p><img src="https://ljw.howieli.cn/blog/2018-1-8/%E4%BF%AE%E7%90%86%E5%B7%A5.png"></p><a id="more"></a><p>最近发现客户kolla平台的ceph环境遇到下面异常错误</p><figure class="highlight tap"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs tap">(ceph-mon)[root@control01 /]<span class="hljs-comment"># ceph -s</span><br>    cluster b233a0b7-4e21-4375-bca8-e215c056cc25<br>     health HEALTH_WARN<br>           <span class="hljs-number"> 2 </span>requests are blocked &gt;<span class="hljs-number"> 32 </span>sec<br>     monmap e1:<span class="hljs-number"> 3 </span>mons at &#123;10.254.253.1=10.254.253.1:6789/0,10.254.253.2=10.254.253.2:6789/0,10.254.253.3=10.254.253.3:6789/0&#125;<br>            election epoch 26, quorum 0,1,2 10.254.253.1,10.254.253.2,10.254.253.3<br>     osdmap e380:<span class="hljs-number"> 90 </span>osds:<span class="hljs-number"> 90 </span>up,<span class="hljs-number"> 90 </span>in<br>            flags sortbitwise,require_jewel_osds<br>      pgmap v1730087:<span class="hljs-number"> 1008 </span>pgs,<span class="hljs-number"> 11 </span>pools,<span class="hljs-number"> 3502 </span>GB data,<span class="hljs-number"> 886 </span>kobjects<br>           <span class="hljs-number"> 10463 </span>GB used,<span class="hljs-number"> 235 </span>TB /<span class="hljs-number"> 245 </span>TB avail<br>               <span class="hljs-number"> 1007 </span>active+clean<br>                  <span class="hljs-number"> 1 </span>active+clean+scrubbing+deep<br>  client io<span class="hljs-number"> 1838 </span>kB/s rd,<span class="hljs-number"> 100 </span>MB/s wr,<span class="hljs-number"> 457 </span>op/s rd,<span class="hljs-number"> 929 </span>op/s wr<br></code></pre></td></tr></table></figure><p>注意: 1 requests are blocked &gt; 32 sec 有可能是在数据迁移过程中, 用户正在对该数据块进行访问, 但访问还没有完成, 数据就迁移到别的 OSD 中, 那么就会导致有请求被 block, 对用户也是有影响的</p><h3 id="解决思路"><a href="#解决思路" class="headerlink" title="解决思路"></a>解决思路</h3><p>1.寻找block的请求</p><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs livecodeserver">(ceph-mon)[root@control01 /]<span class="hljs-comment"># ceph health detail</span><br>HEALTH_WARN <span class="hljs-number">2</span> requests are blocked &gt; <span class="hljs-number">32</span> <span class="hljs-built_in">sec</span>; <span class="hljs-number">1</span> osds have slow requests<br><span class="hljs-number">2</span> ops are blocked &gt; <span class="hljs-number">4194.3</span> <span class="hljs-built_in">sec</span> <span class="hljs-keyword">on</span> <span class="hljs-title">osd</span><span class="hljs-number">.5</span><br><span class="hljs-number">1</span> osds have slow requests<br></code></pre></td></tr></table></figure><p>可以考到osd.5具有一个操作block<br>2.查找osd对应的主机</p><figure class="highlight dns"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br></pre></td><td class="code"><pre><code class="hljs dns">(ceph-mon)[root@control01 /]# ceph osd tree<br>ID  WEIGHT    TYPE NAME              UP/DOWN REWEIGHT PRIMARY-AFFINITY <br> -<span class="hljs-number">1</span> <span class="hljs-number">270.00000</span> root default                                             <br> -<span class="hljs-number">2</span>  <span class="hljs-number">27.00000</span>     host <span class="hljs-number">10.254.253.1</span>                                    <br>  <span class="hljs-number">0</span>   <span class="hljs-number">3.00000</span>         osd.<span class="hljs-number">0</span>               up  <span class="hljs-number">1.00000</span>          <span class="hljs-number">1.00000</span> <br>  <span class="hljs-number">5</span>   <span class="hljs-number">3.00000</span>         osd.<span class="hljs-number">5</span>               up  <span class="hljs-number">1.00000</span>          <span class="hljs-number">1.00000</span> <br> <span class="hljs-number">10</span>   <span class="hljs-number">3.00000</span>         osd.<span class="hljs-number">10</span>              up  <span class="hljs-number">1.00000</span>          <span class="hljs-number">1.00000</span> <br> <span class="hljs-number">15</span>   <span class="hljs-number">3.00000</span>         osd.<span class="hljs-number">15</span>              up  <span class="hljs-number">1.00000</span>          <span class="hljs-number">1.00000</span> <br> <span class="hljs-number">21</span>   <span class="hljs-number">3.00000</span>         osd.<span class="hljs-number">21</span>              up  <span class="hljs-number">1.00000</span>          <span class="hljs-number">1.00000</span> <br> <span class="hljs-number">27</span>   <span class="hljs-number">3.00000</span>         osd.<span class="hljs-number">27</span>              up  <span class="hljs-number">1.00000</span>          <span class="hljs-number">1.00000</span> <br> <span class="hljs-number">30</span>   <span class="hljs-number">3.00000</span>         osd.<span class="hljs-number">30</span>              up  <span class="hljs-number">1.00000</span>          <span class="hljs-number">1.00000</span> <br> <span class="hljs-number">36</span>   <span class="hljs-number">3.00000</span>         osd.<span class="hljs-number">36</span>              up  <span class="hljs-number">1.00000</span>          <span class="hljs-number">1.00000</span> <br> <span class="hljs-number">42</span>   <span class="hljs-number">3.00000</span>         osd.<span class="hljs-number">42</span>              up  <span class="hljs-number">1.00000</span>          <span class="hljs-number">1.00000</span> <br> -<span class="hljs-number">3</span>  <span class="hljs-number">27.00000</span>     host <span class="hljs-number">10.254.253.5</span>                                    <br>  <span class="hljs-number">2</span>   <span class="hljs-number">3.00000</span>         osd.<span class="hljs-number">2</span>               up  <span class="hljs-number">1.00000</span>          <span class="hljs-number">1.00000</span> <br>  <span class="hljs-number">6</span>   <span class="hljs-number">3.00000</span>         osd.<span class="hljs-number">6</span>               up  <span class="hljs-number">1.00000</span>          <span class="hljs-number">1.00000</span> <br> <span class="hljs-number">11</span>   <span class="hljs-number">3.00000</span>         osd.<span class="hljs-number">11</span>              up  <span class="hljs-number">1.00000</span>          <span class="hljs-number">1.00000</span> <br> <span class="hljs-number">17</span>   <span class="hljs-number">3.00000</span>         osd.<span class="hljs-number">17</span>              up  <span class="hljs-number">1.00000</span>          <span class="hljs-number">1.00000</span> <br> <span class="hljs-number">20</span>   <span class="hljs-number">3.00000</span>         osd.<span class="hljs-number">20</span>              up  <span class="hljs-number">1.00000</span>          <span class="hljs-number">1.00000</span> <br> <span class="hljs-number">25</span>   <span class="hljs-number">3.00000</span>         osd.<span class="hljs-number">25</span>              up  <span class="hljs-number">1.00000</span>          <span class="hljs-number">1.00000</span> <br> <span class="hljs-number">31</span>   <span class="hljs-number">3.00000</span>         osd.<span class="hljs-number">31</span>              up  <span class="hljs-number">1.00000</span>          <span class="hljs-number">1.00000</span> <br> <span class="hljs-number">35</span>   <span class="hljs-number">3.00000</span>         osd.<span class="hljs-number">35</span>              up  <span class="hljs-number">1.00000</span>          <span class="hljs-number">1.00000</span> <br> <span class="hljs-number">41</span>   <span class="hljs-number">3.00000</span>         osd.<span class="hljs-number">41</span>              up  <span class="hljs-number">1.00000</span>          <span class="hljs-number">1.00000</span> <br> -<span class="hljs-number">4</span>  <span class="hljs-number">27.00000</span>     host <span class="hljs-number">10.254.253.2</span>                                    <br>  <span class="hljs-number">1</span>   <span class="hljs-number">3.00000</span>         osd.<span class="hljs-number">1</span>               up  <span class="hljs-number">1.00000</span>          <span class="hljs-number">1.00000</span> <br>  <span class="hljs-number">7</span>   <span class="hljs-number">3.00000</span>         osd.<span class="hljs-number">7</span>               up  <span class="hljs-number">1.00000</span>          <span class="hljs-number">1.00000</span> <br> <span class="hljs-number">12</span>   <span class="hljs-number">3.00000</span>         osd.<span class="hljs-number">12</span>              up  <span class="hljs-number">1.00000</span>          <span class="hljs-number">1.00000</span> <br> <span class="hljs-number">16</span>   <span class="hljs-number">3.00000</span>         osd.<span class="hljs-number">16</span>              up  <span class="hljs-number">1.00000</span>          <span class="hljs-number">1.00000</span> <br> <span class="hljs-number">22</span>   <span class="hljs-number">3.00000</span>         osd.<span class="hljs-number">22</span>              up  <span class="hljs-number">1.00000</span>          <span class="hljs-number">1.00000</span> <br> <span class="hljs-number">26</span>   <span class="hljs-number">3.00000</span>         osd.<span class="hljs-number">26</span>              up  <span class="hljs-number">1.00000</span>          <span class="hljs-number">1.00000</span> <br> <span class="hljs-number">32</span>   <span class="hljs-number">3.00000</span>         osd.<span class="hljs-number">32</span>              up  <span class="hljs-number">1.00000</span>          <span class="hljs-number">1.00000</span> <br> <span class="hljs-number">37</span>   <span class="hljs-number">3.00000</span>         osd.<span class="hljs-number">37</span>              up  <span class="hljs-number">1.00000</span>          <span class="hljs-number">1.00000</span> <br> <span class="hljs-number">40</span>   <span class="hljs-number">3.00000</span>         osd.<span class="hljs-number">40</span>              up  <span class="hljs-number">1.00000</span>          <span class="hljs-number">1.00000</span> <br> -<span class="hljs-number">6</span>  <span class="hljs-number">27.00000</span>     host <span class="hljs-number">10.254.253.4</span>                                    <br>  <span class="hljs-number">3</span>   <span class="hljs-number">3.00000</span>         osd.<span class="hljs-number">3</span>               up  <span class="hljs-number">1.00000</span>          <span class="hljs-number">1.00000</span> <br>  <span class="hljs-number">8</span>   <span class="hljs-number">3.00000</span>         osd.<span class="hljs-number">8</span>               up  <span class="hljs-number">1.00000</span>          <span class="hljs-number">1.00000</span> <br> <span class="hljs-number">13</span>   <span class="hljs-number">3.00000</span>         osd.<span class="hljs-number">13</span>              up  <span class="hljs-number">1.00000</span>          <span class="hljs-number">1.00000</span> <br> <span class="hljs-number">18</span>   <span class="hljs-number">3.00000</span>         osd.<span class="hljs-number">18</span>              up  <span class="hljs-number">1.00000</span>          <span class="hljs-number">1.00000</span> <br> <span class="hljs-number">24</span>   <span class="hljs-number">3.00000</span>         osd.<span class="hljs-number">24</span>              up  <span class="hljs-number">1.00000</span>          <span class="hljs-number">1.00000</span> <br> <span class="hljs-number">29</span>   <span class="hljs-number">3.00000</span>         osd.<span class="hljs-number">29</span>              up  <span class="hljs-number">1.00000</span>          <span class="hljs-number">1.00000</span> <br> <span class="hljs-number">33</span>   <span class="hljs-number">3.00000</span>         osd.<span class="hljs-number">33</span>              up  <span class="hljs-number">1.00000</span>          <span class="hljs-number">1.00000</span> <br> <span class="hljs-number">39</span>   <span class="hljs-number">3.00000</span>         osd.<span class="hljs-number">39</span>              up  <span class="hljs-number">1.00000</span>          <span class="hljs-number">1.00000</span> <br> <span class="hljs-number">43</span>   <span class="hljs-number">3.00000</span>         osd.<span class="hljs-number">43</span>              up  <span class="hljs-number">1.00000</span>          <span class="hljs-number">1.00000</span> <br> -<span class="hljs-number">5</span>  <span class="hljs-number">27.00000</span>     host <span class="hljs-number">10.254.253.3</span>                                    <br>  <span class="hljs-number">4</span>   <span class="hljs-number">3.00000</span>         osd.<span class="hljs-number">4</span>               up  <span class="hljs-number">1.00000</span>          <span class="hljs-number">1.00000</span> <br>  <span class="hljs-number">9</span>   <span class="hljs-number">3.00000</span>         osd.<span class="hljs-number">9</span>               up  <span class="hljs-number">1.00000</span>          <span class="hljs-number">1.00000</span> <br> <span class="hljs-number">14</span>   <span class="hljs-number">3.00000</span>         osd.<span class="hljs-number">14</span>              up  <span class="hljs-number">1.00000</span>          <span class="hljs-number">1.00000</span> <br> <span class="hljs-number">19</span>   <span class="hljs-number">3.00000</span>         osd.<span class="hljs-number">19</span>              up  <span class="hljs-number">1.00000</span>          <span class="hljs-number">1.00000</span> <br> <span class="hljs-number">23</span>   <span class="hljs-number">3.00000</span>         osd.<span class="hljs-number">23</span>              up  <span class="hljs-number">1.00000</span>          <span class="hljs-number">1.00000</span> <br> <span class="hljs-number">28</span>   <span class="hljs-number">3.00000</span>         osd.<span class="hljs-number">28</span>              up  <span class="hljs-number">1.00000</span>          <span class="hljs-number">1.00000</span> <br> <span class="hljs-number">34</span>   <span class="hljs-number">3.00000</span>         osd.<span class="hljs-number">34</span>              up  <span class="hljs-number">1.00000</span>          <span class="hljs-number">1.00000</span> <br> <span class="hljs-number">38</span>   <span class="hljs-number">3.00000</span>         osd.<span class="hljs-number">38</span>              up  <span class="hljs-number">1.00000</span>          <span class="hljs-number">1.00000</span> <br> <span class="hljs-number">44</span>   <span class="hljs-number">3.00000</span>         osd.<span class="hljs-number">44</span>              up  <span class="hljs-number">1.00000</span>          <span class="hljs-number">1.00000</span> <br> -<span class="hljs-number">7</span>  <span class="hljs-number">27.00000</span>     host <span class="hljs-number">10.254.253.8</span>                                    <br> <span class="hljs-number">47</span>   <span class="hljs-number">3.00000</span>         osd.<span class="hljs-number">47</span>              up  <span class="hljs-number">1.00000</span>          <span class="hljs-number">1.00000</span> <br> <span class="hljs-number">51</span>   <span class="hljs-number">3.00000</span>         osd.<span class="hljs-number">51</span>              up  <span class="hljs-number">1.00000</span>          <span class="hljs-number">1.00000</span> <br> <span class="hljs-number">56</span>   <span class="hljs-number">3.00000</span>         osd.<span class="hljs-number">56</span>              up  <span class="hljs-number">1.00000</span>          <span class="hljs-number">1.00000</span> <br> <span class="hljs-number">61</span>   <span class="hljs-number">3.00000</span>         osd.<span class="hljs-number">61</span>              up  <span class="hljs-number">1.00000</span>          <span class="hljs-number">1.00000</span> <br> <span class="hljs-number">66</span>   <span class="hljs-number">3.00000</span>         osd.<span class="hljs-number">66</span>              up  <span class="hljs-number">1.00000</span>          <span class="hljs-number">1.00000</span> <br> <span class="hljs-number">71</span>   <span class="hljs-number">3.00000</span>         osd.<span class="hljs-number">71</span>              up  <span class="hljs-number">1.00000</span>          <span class="hljs-number">1.00000</span> <br> <span class="hljs-number">76</span>   <span class="hljs-number">3.00000</span>         osd.<span class="hljs-number">76</span>              up  <span class="hljs-number">1.00000</span>          <span class="hljs-number">1.00000</span> <br> <span class="hljs-number">81</span>   <span class="hljs-number">3.00000</span>         osd.<span class="hljs-number">81</span>              up  <span class="hljs-number">1.00000</span>          <span class="hljs-number">1.00000</span> <br> <span class="hljs-number">86</span>   <span class="hljs-number">3.00000</span>         osd.<span class="hljs-number">86</span>              up  <span class="hljs-number">1.00000</span>          <span class="hljs-number">1.00000</span> <br> -<span class="hljs-number">8</span>  <span class="hljs-number">27.00000</span>     host <span class="hljs-number">10.254.253.7</span>                                    <br> <span class="hljs-number">46</span>   <span class="hljs-number">3.00000</span>         osd.<span class="hljs-number">46</span>              up  <span class="hljs-number">1.00000</span>          <span class="hljs-number">1.00000</span> <br> <span class="hljs-number">52</span>   <span class="hljs-number">3.00000</span>         osd.<span class="hljs-number">52</span>              up  <span class="hljs-number">1.00000</span>          <span class="hljs-number">1.00000</span> <br> <span class="hljs-number">57</span>   <span class="hljs-number">3.00000</span>         osd.<span class="hljs-number">57</span>              up  <span class="hljs-number">1.00000</span>          <span class="hljs-number">1.00000</span> <br> <span class="hljs-number">62</span>   <span class="hljs-number">3.00000</span>         osd.<span class="hljs-number">62</span>              up  <span class="hljs-number">1.00000</span>          <span class="hljs-number">1.00000</span> <br> <span class="hljs-number">67</span>   <span class="hljs-number">3.00000</span>         osd.<span class="hljs-number">67</span>              up  <span class="hljs-number">1.00000</span>          <span class="hljs-number">1.00000</span> <br> <span class="hljs-number">72</span>   <span class="hljs-number">3.00000</span>         osd.<span class="hljs-number">72</span>              up  <span class="hljs-number">1.00000</span>          <span class="hljs-number">1.00000</span> <br> <span class="hljs-number">77</span>   <span class="hljs-number">3.00000</span>         osd.<span class="hljs-number">77</span>              up  <span class="hljs-number">1.00000</span>          <span class="hljs-number">1.00000</span> <br> <span class="hljs-number">82</span>   <span class="hljs-number">3.00000</span>         osd.<span class="hljs-number">82</span>              up  <span class="hljs-number">1.00000</span>          <span class="hljs-number">1.00000</span> <br> <span class="hljs-number">87</span>   <span class="hljs-number">3.00000</span>         osd.<span class="hljs-number">87</span>              up  <span class="hljs-number">1.00000</span>          <span class="hljs-number">1.00000</span> <br> -<span class="hljs-number">9</span>  <span class="hljs-number">27.00000</span>     host <span class="hljs-number">10.254.253.9</span>                                    <br> <span class="hljs-number">48</span>   <span class="hljs-number">3.00000</span>         osd.<span class="hljs-number">48</span>              up  <span class="hljs-number">1.00000</span>          <span class="hljs-number">1.00000</span> <br> <span class="hljs-number">53</span>   <span class="hljs-number">3.00000</span>         osd.<span class="hljs-number">53</span>              up  <span class="hljs-number">1.00000</span>          <span class="hljs-number">1.00000</span> <br> <span class="hljs-number">58</span>   <span class="hljs-number">3.00000</span>         osd.<span class="hljs-number">58</span>              up  <span class="hljs-number">1.00000</span>          <span class="hljs-number">1.00000</span> <br> <span class="hljs-number">63</span>   <span class="hljs-number">3.00000</span>         osd.<span class="hljs-number">63</span>              up  <span class="hljs-number">1.00000</span>          <span class="hljs-number">1.00000</span> <br> <span class="hljs-number">68</span>   <span class="hljs-number">3.00000</span>         osd.<span class="hljs-number">68</span>              up  <span class="hljs-number">1.00000</span>          <span class="hljs-number">1.00000</span> <br> <span class="hljs-number">73</span>   <span class="hljs-number">3.00000</span>         osd.<span class="hljs-number">73</span>              up  <span class="hljs-number">1.00000</span>          <span class="hljs-number">1.00000</span> <br> <span class="hljs-number">78</span>   <span class="hljs-number">3.00000</span>         osd.<span class="hljs-number">78</span>              up  <span class="hljs-number">1.00000</span>          <span class="hljs-number">1.00000</span> <br> <span class="hljs-number">83</span>   <span class="hljs-number">3.00000</span>         osd.<span class="hljs-number">83</span>              up  <span class="hljs-number">1.00000</span>          <span class="hljs-number">1.00000</span> <br> <span class="hljs-number">88</span>   <span class="hljs-number">3.00000</span>         osd.<span class="hljs-number">88</span>              up  <span class="hljs-number">1.00000</span>          <span class="hljs-number">1.00000</span> <br>-<span class="hljs-number">10</span>  <span class="hljs-number">27.00000</span>     host <span class="hljs-number">10.254.253.10</span>                                   <br> <span class="hljs-number">49</span>   <span class="hljs-number">3.00000</span>         osd.<span class="hljs-number">49</span>              up  <span class="hljs-number">1.00000</span>          <span class="hljs-number">1.00000</span> <br> <span class="hljs-number">54</span>   <span class="hljs-number">3.00000</span>         osd.<span class="hljs-number">54</span>              up  <span class="hljs-number">1.00000</span>          <span class="hljs-number">1.00000</span> <br> <span class="hljs-number">59</span>   <span class="hljs-number">3.00000</span>         osd.<span class="hljs-number">59</span>              up  <span class="hljs-number">1.00000</span>          <span class="hljs-number">1.00000</span> <br> <span class="hljs-number">64</span>   <span class="hljs-number">3.00000</span>         osd.<span class="hljs-number">64</span>              up  <span class="hljs-number">1.00000</span>          <span class="hljs-number">1.00000</span> <br> <span class="hljs-number">69</span>   <span class="hljs-number">3.00000</span>         osd.<span class="hljs-number">69</span>              up  <span class="hljs-number">1.00000</span>          <span class="hljs-number">1.00000</span> <br> <span class="hljs-number">74</span>   <span class="hljs-number">3.00000</span>         osd.<span class="hljs-number">74</span>              up  <span class="hljs-number">1.00000</span>          <span class="hljs-number">1.00000</span> <br> <span class="hljs-number">79</span>   <span class="hljs-number">3.00000</span>         osd.<span class="hljs-number">79</span>              up  <span class="hljs-number">1.00000</span>          <span class="hljs-number">1.00000</span> <br> <span class="hljs-number">84</span>   <span class="hljs-number">3.00000</span>         osd.<span class="hljs-number">84</span>              up  <span class="hljs-number">1.00000</span>          <span class="hljs-number">1.00000</span> <br> <span class="hljs-number">89</span>   <span class="hljs-number">3.00000</span>         osd.<span class="hljs-number">89</span>              up  <span class="hljs-number">1.00000</span>          <span class="hljs-number">1.00000</span> <br>-<span class="hljs-number">11</span>  <span class="hljs-number">27.00000</span>     host <span class="hljs-number">10.254.253.6</span>                                    <br> <span class="hljs-number">50</span>   <span class="hljs-number">3.00000</span>         osd.<span class="hljs-number">50</span>              up  <span class="hljs-number">1.00000</span>          <span class="hljs-number">1.00000</span> <br> <span class="hljs-number">55</span>   <span class="hljs-number">3.00000</span>         osd.<span class="hljs-number">55</span>              up  <span class="hljs-number">1.00000</span>          <span class="hljs-number">1.00000</span> <br> <span class="hljs-number">60</span>   <span class="hljs-number">3.00000</span>         osd.<span class="hljs-number">60</span>              up  <span class="hljs-number">1.00000</span>          <span class="hljs-number">1.00000</span> <br> <span class="hljs-number">65</span>   <span class="hljs-number">3.00000</span>         osd.<span class="hljs-number">65</span>              up  <span class="hljs-number">1.00000</span>          <span class="hljs-number">1.00000</span> <br> <span class="hljs-number">70</span>   <span class="hljs-number">3.00000</span>         osd.<span class="hljs-number">70</span>              up  <span class="hljs-number">1.00000</span>          <span class="hljs-number">1.00000</span> <br> <span class="hljs-number">75</span>   <span class="hljs-number">3.00000</span>         osd.<span class="hljs-number">75</span>              up  <span class="hljs-number">1.00000</span>          <span class="hljs-number">1.00000</span> <br> <span class="hljs-number">80</span>   <span class="hljs-number">3.00000</span>         osd.<span class="hljs-number">80</span>              up  <span class="hljs-number">1.00000</span>          <span class="hljs-number">1.00000</span> <br> <span class="hljs-number">85</span>   <span class="hljs-number">3.00000</span>         osd.<span class="hljs-number">85</span>              up  <span class="hljs-number">1.00000</span>          <span class="hljs-number">1.00000</span> <br> <span class="hljs-number">90</span>   <span class="hljs-number">3.00000</span>         osd.<span class="hljs-number">90</span>              up  <span class="hljs-number">1.00000</span>          <span class="hljs-number">1.00000</span> <br></code></pre></td></tr></table></figure><h3 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法"></a>解决办法</h3><figure class="highlight autoit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs autoit">[root<span class="hljs-symbol">@control01</span> ~]<span class="hljs-meta"># docker restart ceph_osd_5</span><br>ceph_osd_5<br><br>[root<span class="hljs-symbol">@control01</span> ~]<span class="hljs-meta"># docker ps|grep ceph_osd_5</span><br><span class="hljs-number">9</span>c4fe5eb0090        <span class="hljs-number">10.254</span><span class="hljs-number">.254</span><span class="hljs-number">.1</span>:<span class="hljs-number">4000</span>/<span class="hljs-number">99</span>cloud/centos-source-ceph-osd:animbus<span class="hljs-number">-5.4</span><span class="hljs-number">.0</span>                    <span class="hljs-string">&quot;kolla_start&quot;</span>            <span class="hljs-number">3</span> weeks ago         Up <span class="hljs-number">13</span> seconds <br></code></pre></td></tr></table></figure><p>系统会对该 osd 执行 recovery 操作, recovery 过程中, 会断开 block request, 那么这个 request 将会重新请求 mon 节点, 并重新获得新的 pg map, 得到最新的数据访问位置, 从而解决上述问题。</p><h3 id="查看集群状态"><a href="#查看集群状态" class="headerlink" title="查看集群状态"></a>查看集群状态</h3><figure class="highlight tap"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs tap">(ceph-mon)[root@control01 /]<span class="hljs-comment"># ceph -s</span><br>    cluster b233a0b7-4e21-4375-bca8-e215c056cc25<br>     health HEALTH_OK<br>     monmap e1:<span class="hljs-number"> 3 </span>mons at &#123;10.254.253.1=10.254.253.1:6789/0,10.254.253.2=10.254.253.2:6789/0,10.254.253.3=10.254.253.3:6789/0&#125;<br>            election epoch 26, quorum 0,1,2 10.254.253.1,10.254.253.2,10.254.253.3<br>     osdmap e387:<span class="hljs-number"> 90 </span>osds:<span class="hljs-number"> 90 </span>up,<span class="hljs-number"> 90 </span>in<br>            flags sortbitwise,require_jewel_osds<br>      pgmap v1730238:<span class="hljs-number"> 1008 </span>pgs,<span class="hljs-number"> 11 </span>pools,<span class="hljs-number"> 3498 </span>GB data,<span class="hljs-number"> 886 </span>kobjects<br>           <span class="hljs-number"> 10453 </span>GB used,<span class="hljs-number"> 235 </span>TB /<span class="hljs-number"> 245 </span>TB avail<br>               <span class="hljs-number"> 1006 </span>active+clean<br>                  <span class="hljs-number"> 2 </span>active+clean+scrubbing+deep<br>  client io<span class="hljs-number"> 1090 </span>kB/s rd,<span class="hljs-number"> 92507 </span>kB/s wr,<span class="hljs-number"> 778 </span>op/s rd,<span class="hljs-number"> 904 </span>op/s wr<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>ceph</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ceph</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>shell实现局域网IP扫描</title>
    <link href="/posts/shell%E5%AE%9E%E7%8E%B0%E5%B1%80%E5%9F%9F%E7%BD%91IP%E6%89%AB%E6%8F%8F/"/>
    <url>/posts/shell%E5%AE%9E%E7%8E%B0%E5%B1%80%E5%9F%9F%E7%BD%91IP%E6%89%AB%E6%8F%8F/</url>
    
    <content type="html"><![CDATA[<h3 id="简单shell实现局域网IP扫描"><a href="#简单shell实现局域网IP扫描" class="headerlink" title="简单shell实现局域网IP扫描"></a>简单shell实现局域网IP扫描</h3><p>简单shell实现局域网IP扫描<br>局域网主机联通性的扫描</p><a id="more"></a><h3 id="脚本"><a href="#脚本" class="headerlink" title="脚本"></a>脚本</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment">#简单shell实现局域网IP扫描</span><br><span class="hljs-comment">#Functions: 局域网主机联通性的扫描</span><br><span class="hljs-comment">#Author: ljw</span><br><br>network=<span class="hljs-variable">$1</span><br>time=$(date +%H%M%S)<br><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> $(seq <span class="hljs-variable">$2</span> <span class="hljs-variable">$3</span>)<br><span class="hljs-keyword">do</span><br>    ping -c 1 -w 2 <span class="hljs-variable">$network</span>.<span class="hljs-variable">$i</span> &gt; /dev/null<br>    <span class="hljs-keyword">if</span> [ $? -eq 0 ]; <span class="hljs-keyword">then</span><br>          arp <span class="hljs-variable">$network</span>.<span class="hljs-variable">$i</span> | grep <span class="hljs-string">&quot;:&quot;</span> | awk <span class="hljs-string">&#x27;&#123;print $1,$3&#125;&#x27;</span> &gt;&gt; <span class="hljs-variable">$time</span>.<span class="hljs-built_in">log</span><br>          <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;host <span class="hljs-variable">$network</span>.<span class="hljs-variable">$i</span> is up&quot;</span><br>   <span class="hljs-keyword">else</span><br>          <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;host <span class="hljs-variable">$network</span>.<span class="hljs-variable">$i</span> is down&quot;</span><br>   <span class="hljs-keyword">fi</span><br><span class="hljs-keyword">done</span><br></code></pre></td></tr></table></figure><h3 id="执行"><a href="#执行" class="headerlink" title="执行"></a>执行</h3><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs css">./<span class="hljs-selector-tag">netscan</span><span class="hljs-selector-class">.sh</span> 172.18.22 100 130<br></code></pre></td></tr></table></figure><h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs vim"># <span class="hljs-keyword">sh</span> netscan.<span class="hljs-keyword">sh</span> <span class="hljs-number">172.18</span>.<span class="hljs-number">22</span> <span class="hljs-number">100</span> <span class="hljs-number">130</span><br>host <span class="hljs-number">172.18</span>.<span class="hljs-number">22.100</span> <span class="hljs-keyword">is</span> <span class="hljs-keyword">up</span><br>host <span class="hljs-number">172.18</span>.<span class="hljs-number">22.101</span> <span class="hljs-keyword">is</span> <span class="hljs-keyword">up</span><br>host <span class="hljs-number">172.18</span>.<span class="hljs-number">22.102</span> <span class="hljs-keyword">is</span> <span class="hljs-keyword">up</span><br>host <span class="hljs-number">172.18</span>.<span class="hljs-number">22.103</span> <span class="hljs-keyword">is</span> <span class="hljs-keyword">up</span><br>host <span class="hljs-number">172.18</span>.<span class="hljs-number">22.104</span> <span class="hljs-keyword">is</span> down<br>host <span class="hljs-number">172.18</span>.<span class="hljs-number">22.105</span> <span class="hljs-keyword">is</span> down<br>host <span class="hljs-number">172.18</span>.<span class="hljs-number">22.106</span> <span class="hljs-keyword">is</span> down<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>shell</category>
      
    </categories>
    
    
    <tags>
      
      <tag>shell</tag>
      
      <tag>Linux</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>在kolla中配置cinder ceph多后端</title>
    <link href="/posts/%E5%9C%A8kolla%E4%B8%AD%E9%85%8D%E7%BD%AEcinder-ceph%E5%A4%9A%E5%90%8E%E7%AB%AF/"/>
    <url>/posts/%E5%9C%A8kolla%E4%B8%AD%E9%85%8D%E7%BD%AEcinder-ceph%E5%A4%9A%E5%90%8E%E7%AB%AF/</url>
    
    <content type="html"><![CDATA[<h3 id="利用kolla中配置cinder-ceph多后端"><a href="#利用kolla中配置cinder-ceph多后端" class="headerlink" title="利用kolla中配置cinder ceph多后端"></a>利用kolla中配置cinder ceph多后端</h3><p>我一台有2块硬盘，1块ssd（性能盘）和1块sata（容量盘），共有3台同样的服务器（控制计算存储融合）<br>/dev/sdb  ssd<br>/dev/sdc  sata<br><img src="https://ljw.howieli.cn/blog/2017-11-30/ceph.png"></p><a id="more"></a><h3 id="配置globals-yml-文件"><a href="#配置globals-yml-文件" class="headerlink" title="配置globals.yml 文件"></a>配置globals.yml 文件</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell">ceph_enable_cache: &quot;yes&quot;<br>enable_ceph: &quot;yes&quot;<br>enable_cinder: &quot;yes&quot;<br>cinder_backend_ceph: &quot;&#123;&#123; enable_ceph &#125;&#125;&quot;<br></code></pre></td></tr></table></figure><h3 id="修改cinder配置（代码修改都在部署节点）"><a href="#修改cinder配置（代码修改都在部署节点）" class="headerlink" title="修改cinder配置（代码修改都在部署节点）"></a>修改cinder配置（代码修改都在部署节点）</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> vim /etc/kolla/config/cinder.conf</span><br>[DEFAULT]<br>enabled_backends = rbd-1,ssd<br>[ssd]<br>volume_driver = cinder.volume.drivers.rbd.RBDDriver<br>volume_backend_name = ssd<br>rbd_pool = volumes-cache<br>rbd_ceph_conf = /etc/ceph/ceph.conf<br>rbd_flatten_volume_from_snapshot = false<br>rbd_max_clone_depth = 5<br>rbd_store_chunk_size = 4<br>rados_connect_timeout = 5<br>rbd_user = cinder<br>rbd_secret_uuid = b89a2a40-c009-47da-ba5b-7b6414a1f759<br><span class="hljs-meta">#</span><span class="bash"> rbd_secret_uuid = xxxxxxxxxxxxxxxxxxxxxxxxxxx</span><br>report_discard_supported = True<br>image_upload_use_cinder_backend = True<br></code></pre></td></tr></table></figure><h3 id="给磁盘打标签"><a href="#给磁盘打标签" class="headerlink" title="给磁盘打标签"></a>给磁盘打标签</h3><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">parted</span> /dev/sdb -s -- mklabel gpt mkpart KOLLA_CEPH_OSD_BOOTSTRAP <span class="hljs-number">1</span> -<span class="hljs-number">1</span><br><br><span class="hljs-attribute">parted</span> /dev/sdc -s -- mklabel gpt mkpart KOLLA_CEPH_OSD_CACHE_BOOTSTRAP <span class="hljs-number">1</span> -<span class="hljs-number">1</span><br></code></pre></td></tr></table></figure><h3 id="开始部署"><a href="#开始部署" class="headerlink" title="开始部署"></a>开始部署</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">kolla-ansible deploy  -i /root/multinode<br></code></pre></td></tr></table></figure><h3 id="crushmap"><a href="#crushmap" class="headerlink" title="crushmap"></a>crushmap</h3><h3 id="获取crushmap"><a href="#获取crushmap" class="headerlink" title="获取crushmap"></a>获取crushmap</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> docker <span class="hljs-built_in">exec</span> -it ceph_mon bash</span><br><span class="hljs-meta">#</span><span class="bash"> ceph osd getcrushmap -o crushmap.old</span><br></code></pre></td></tr></table></figure><h4 id="反编译：此后就可以编辑crushmap文件了"><a href="#反编译：此后就可以编辑crushmap文件了" class="headerlink" title="反编译：此后就可以编辑crushmap文件了"></a>反编译：此后就可以编辑crushmap文件了</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> crushtool -d crushmap.old -o crushmap.new</span><br></code></pre></td></tr></table></figure><h4 id="修改crushmap"><a href="#修改crushmap" class="headerlink" title="修改crushmap"></a>修改crushmap</h4><p>根据实际情况编写crushmap</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> vi crushmap.new</span><br><br><span class="hljs-meta">#</span><span class="bash"> begin crush map</span><br>tunable choose_local_tries 0<br>tunable choose_local_fallback_tries 0<br>tunable choose_total_tries 50<br>tunable chooseleaf_descend_once 1<br>tunable chooseleaf_vary_r 1<br>tunable straw_calc_version 1<br><br><span class="hljs-meta">#</span><span class="bash"> devices</span><br>device 0 osd.0<br>device 1 osd.1<br>device 2 osd.2<br>device 3 osd.3<br>device 4 osd.4<br>device 5 osd.5<br><br><span class="hljs-meta">#</span><span class="bash"> types</span><br>type 0 osd<br>type 1 host<br>type 2 chassis<br>type 3 rack<br>type 4 row<br>type 5 pdu<br>type 6 pod<br>type 7 room<br>type 8 datacenter<br>type 9 region<br>type 10 root<br><br><span class="hljs-meta">#</span><span class="bash"> buckets</span><br>host 1.1.1.163 &#123;<br>        id -2           # do not change unnecessarily<br>        # weight 0.990<br>        alg straw<br>        hash 0  # rjenkins1<br>        item osd.0 weight 0.990<br>&#125;<br>host 1.1.1.161 &#123;<br>        id -3           # do not change unnecessarily<br>        # weight 0.990<br>        alg straw<br>        hash 0  # rjenkins1<br>        item osd.1 weight 0.990<br>&#125;<br>host 1.1.1.162 &#123;<br>        id -4           # do not change unnecessarily<br>        # weight 0.990<br>        alg straw<br>        hash 0  # rjenkins1<br>        item osd.2 weight 0.990<br>&#125;<br>root default &#123;<br>        id -1           # do not change unnecessarily<br>        # weight 2.970<br>        alg straw<br>        hash 0  # rjenkins1<br>        item 1.1.1.163 weight 0.990<br>        item 1.1.1.161 weight 0.990<br>        item 1.1.1.162 weight 0.990<br>&#125;<br>host 1.1.1.163-ssd &#123;<br>        id -5           # do not change unnecessarily<br>        # weight 0.470<br>        alg straw<br>        hash 0  # rjenkins1<br>        item osd.4 weight 0.470<br>&#125;<br>host 1.1.1.161-ssd &#123;<br>        id -6           # do not change unnecessarily<br>        # weight 0.470<br>        alg straw<br>        hash 0  # rjenkins1<br>        item osd.3 weight 0.470<br>&#125;<br>host 1.1.1.162-ssd &#123;<br>        id -8           # do not change unnecessarily<br>        # weight 0.470<br>        alg straw<br>        hash 0  # rjenkins1<br>        item osd.5 weight 0.470<br>&#125;<br>root ssd &#123;<br>        id -7           # do not change unnecessarily<br>        # weight 1.410<br>        alg straw<br>        hash 0  # rjenkins1<br>        item 1.1.1.163-ssd weight 0.470<br>        item 1.1.1.161-ssd weight 0.470<br>        item 1.1.1.162-ssd weight 0.470<br>&#125;<br><br><span class="hljs-meta">#</span><span class="bash"> rules</span><br>rule replicated_ruleset &#123;<br>        ruleset 0<br>        type replicated<br>        min_size 1<br>        max_size 10<br>        step take default<br>        step chooseleaf firstn 0 type host<br>        step emit<br>&#125;<br>rule disks &#123;<br>        ruleset 1<br>        type replicated<br>        min_size 1<br>        max_size 10<br>        step take default<br>        step chooseleaf firstn 0 type host<br>        step emit<br>&#125;<br>rule ssd_releset &#123;<br>        ruleset 2<br>        type replicated<br>        min_size 1<br>        max_size 10<br>        step take ssd<br>        step chooseleaf firstn 0 type host<br>        step emit<br>&#125;<br><br><span class="hljs-meta">#</span><span class="bash"> end crush map</span><br></code></pre></td></tr></table></figure><h4 id="重新编译"><a href="#重新编译" class="headerlink" title="重新编译"></a>重新编译</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> crushtool -c crushmap.new -o crushmap.bin</span><br></code></pre></td></tr></table></figure><h4 id="导入新的crushmap"><a href="#导入新的crushmap" class="headerlink" title="导入新的crushmap"></a>导入新的crushmap</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> ceph osd setcrushmap -i crushmap.bin</span><br></code></pre></td></tr></table></figure><h4 id="查看"><a href="#查看" class="headerlink" title="查看"></a>查看</h4><figure class="highlight dns"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs dns"># ceph osd tree<br>ID WEIGHT  TYPE NAME              UP/DOWN REWEIGHT PRIMARY-AFFINITY <br>-<span class="hljs-number">7 1.40996</span> root ssd                                                 <br>-<span class="hljs-number">5 0.46999</span>     host <span class="hljs-number">1.1.1.163</span>-ssd                                   <br> <span class="hljs-number">4 0.46999</span>         osd.<span class="hljs-number">4</span>               up  <span class="hljs-number">1.00000</span>          <span class="hljs-number">1.00000</span> <br>-<span class="hljs-number">6 0.46999</span>     host <span class="hljs-number">1.1.1.161</span>-ssd                                   <br> <span class="hljs-number">3 0.46999</span>         osd.<span class="hljs-number">3</span>               up  <span class="hljs-number">1.00000</span>          <span class="hljs-number">1.00000</span> <br>-<span class="hljs-number">8 0.46999</span>     host <span class="hljs-number">1.1.1.162</span>-ssd                                   <br> <span class="hljs-number">5 0.46999</span>         osd.<span class="hljs-number">5</span>               up  <span class="hljs-number">1.00000</span>          <span class="hljs-number">1.00000</span> <br>-<span class="hljs-number">1 2.96997</span> root default                                             <br>-<span class="hljs-number">2 0.98999</span>     host <span class="hljs-number">1.1.1.163</span>                                       <br> <span class="hljs-number">0 0.98999</span>         osd.<span class="hljs-number">0</span>               up  <span class="hljs-number">1.00000</span>          <span class="hljs-number">1.00000</span> <br>-<span class="hljs-number">3 0.98999</span>     host <span class="hljs-number">1.1.1.161</span>                                       <br> <span class="hljs-number">1 0.98999</span>         osd.<span class="hljs-number">1</span>               up  <span class="hljs-number">1.00000</span>          <span class="hljs-number">1.00000</span> <br>-<span class="hljs-number">4 0.98999</span>     host <span class="hljs-number">1.1.1.162</span>                                       <br> <span class="hljs-number">2 0.98999</span>         osd.<span class="hljs-number">2</span>               up  <span class="hljs-number">1.00000</span>          <span class="hljs-number">1.00000</span><br></code></pre></td></tr></table></figure><h3 id="创建pool"><a href="#创建pool" class="headerlink" title="创建pool"></a>创建pool</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> ceph osd pool create volumes-cache 64 64</span><br></code></pre></td></tr></table></figure><h3 id="设置pool的crushmap"><a href="#设置pool的crushmap" class="headerlink" title="设置pool的crushmap"></a>设置pool的crushmap</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> ceph osd pool <span class="hljs-built_in">set</span> volumes-cache crush_ruleset 2</span><br><span class="hljs-meta">#</span><span class="bash"> ceph osd dump |grep -i pool</span><br>pool 1 &#x27;images&#x27; replicated size 1 min_size 1 crush_ruleset 1 object_hash rjenkins pg_num 128 pgp_num 128 last_change 285 flags hashpspool stripe_width 0<br>pool 2 &#x27;volumes&#x27; replicated size 1 min_size 1 crush_ruleset 1 object_hash rjenkins pg_num 128 pgp_num 128 last_change 281 flags hashpspool stripe_width 0<br>pool 3 &#x27;backups&#x27; replicated size 1 min_size 1 crush_ruleset 1 object_hash rjenkins pg_num 128 pgp_num 128 last_change 165 flags hashpspool stripe_width 0<br>pool 4 &#x27;vms&#x27; replicated size 1 min_size 1 crush_ruleset 1 object_hash rjenkins pg_num 128 pgp_num 128 last_change 44 flags hashpspool stripe_width 0<br>pool 5 &#x27;gnocchi&#x27; replicated size 1 min_size 1 crush_ruleset 1 object_hash rjenkins pg_num 128 pgp_num 128 last_change 35 flags hashpspool stripe_width 0<br>pool 6 &#x27;volumes-cache&#x27; replicated size 3 min_size 2 crush_ruleset 2 object_hash rjenkins pg_num 64 pgp_num 64 last_change 62 flags hashpspool stripe_width 0<br></code></pre></td></tr></table></figure><h3 id="创建两个cinder卷类型"><a href="#创建两个cinder卷类型" class="headerlink" title="创建两个cinder卷类型"></a>创建两个cinder卷类型</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> <span class="hljs-built_in">source</span> /etc/kolla/admin-openrc.sh</span><br><span class="hljs-meta">#</span><span class="bash"> cinder type-create SATA</span><br><span class="hljs-meta">#</span><span class="bash"> cinder type-create SSD</span><br><span class="hljs-meta">#</span><span class="bash"> cinder type-list</span><br>+--------------------------------------+------+-------------+-----------+<br>| ID                                   | Name | Description | Is_Public |<br>+--------------------------------------+------+-------------+-----------+<br>| 8c1079e5-90a3-4f6d-bdb7-2f25b70bc2c8 | SSD  |             | True      |<br>| a605c569-1e88-486d-bd8e-7aba43ce1ef2 | SATA |             | True      |<br>+--------------------------------------+------+-------------+-----------+<br></code></pre></td></tr></table></figure><h3 id="设置卷类型的key键值"><a href="#设置卷类型的key键值" class="headerlink" title="设置卷类型的key键值"></a>设置卷类型的key键值</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs shell">cinder type-key SSD set volume_backend_name=ssd<br>cinder type-key SATA set volume_backend_name=rbd-1<br><span class="hljs-meta">#</span><span class="bash"> cinder  extra-specs-list</span><br>+--------------------------------------+------+----------------------------------+<br>| ID                                   | Name | extra_specs                      |<br>+--------------------------------------+------+----------------------------------+<br>| 8c1079e5-90a3-4f6d-bdb7-2f25b70bc2c8 | SSD  | &#123;&#x27;volume_backend_name&#x27;: &#x27;ssd&#x27;&#125;   |<br>| a605c569-1e88-486d-bd8e-7aba43ce1ef2 | SATA | &#123;&#x27;volume_backend_name&#x27;: &#x27;rbd-1&#x27;&#125; |<br>+--------------------------------------+------+----------------------------------+<br></code></pre></td></tr></table></figure><h3 id="创建云硬盘验证"><a href="#创建云硬盘验证" class="headerlink" title="创建云硬盘验证"></a>创建云硬盘验证</h3><p><img src="https://ljw.howieli.cn/blog/2017-11-30/%E5%88%9B%E5%BB%BA%E4%BA%91%E7%A1%AC%E7%9B%98.png"></p><h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> cinder create --volume_type SSD --display_name vol-ssd 1</span><br>+--------------------------------+--------------------------------------+<br>| Property                       | Value                                |<br>+--------------------------------+--------------------------------------+<br>| attachments                    | []                                   |<br>| availability_zone              | nova                                 |<br>| bootable                       | false                                |<br>| consistencygroup_id            | None                                 |<br>| created_at                     | 2017-11-30T02:58:17.000000           |<br>| description                    | None                                 |<br>| encrypted                      | False                                |<br>| id                             | e2df2b47-13ec-4503-a78b-2ab60cb5a34b |<br>| metadata                       | &#123;&#125;                                   |<br>| migration_status               | None                                 |<br>| multiattach                    | False                                |<br>| name                           | vol-ssd                              |<br>| os-vol-host-attr:host          | control@ssd#ssd                      |<br>| os-vol-mig-status-attr:migstat | None                                 |<br>| os-vol-mig-status-attr:name_id | None                                 |<br>| os-vol-tenant-attr:tenant_id   | 21c161dda51147fb9ff527aadfe1d81a     |<br>| replication_status             | None                                 |<br>| size                           | 1                                    |<br>| snapshot_id                    | None                                 |<br>| source_volid                   | None                                 |<br>| status                         | creating                             |<br>| updated_at                     | 2017-11-30T02:58:18.000000           |<br>| user_id                        | 68601348f1264a4cb69f8f8f162e3f2a     |<br>| volume_type                    | SSD                                  |<br>+--------------------------------+--------------------------------------+<br><span class="hljs-meta">#</span><span class="bash"> cinder create --volume_type SATA --display_name vol-sata 1</span><br>+--------------------------------+--------------------------------------+<br>| Property                       | Value                                |<br>+--------------------------------+--------------------------------------+<br>| attachments                    | []                                   |<br>| availability_zone              | nova                                 |<br>| bootable                       | false                                |<br>| consistencygroup_id            | None                                 |<br>| created_at                     | 2017-11-30T02:59:07.000000           |<br>| description                    | None                                 |<br>| encrypted                      | False                                |<br>| id                             | f3f53733-348d-4ff9-a472-445aed77c111 |<br>| metadata                       | &#123;&#125;                                   |<br>| migration_status               | None                                 |<br>| multiattach                    | False                                |<br>| name                           | vol-sata                             |<br>| os-vol-host-attr:host          | None                                 |<br>| os-vol-mig-status-attr:migstat | None                                 |<br>| os-vol-mig-status-attr:name_id | None                                 |<br>| os-vol-tenant-attr:tenant_id   | 21c161dda51147fb9ff527aadfe1d81a     |<br>| replication_status             | None                                 |<br>| size                           | 1                                    |<br>| snapshot_id                    | None                                 |<br>| source_volid                   | None                                 |<br>| status                         | creating                             |<br>| updated_at                     | None                                 |<br>| user_id                        | 68601348f1264a4cb69f8f8f162e3f2a     |<br>| volume_type                    | SATA                                 |<br>+--------------------------------+--------------------------------------+<br><br><span class="hljs-meta">#</span><span class="bash"> rbd -p volumes-cache ls|grep e2df2b47-13ec-4503-a78b-2ab60cb5a34b</span><br>volume-e2df2b47-13ec-4503-a78b-2ab60cb5a34b<br><span class="hljs-meta">#</span><span class="bash"> rbd -p volumes ls|grep f3f53733-348d-4ff9-a472-445aed77c111</span><br>volume-f3f53733-348d-4ff9-a472-445aed77c111<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>openstack</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ceph</tag>
      
      <tag>cinder</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>kolla部署swift</title>
    <link href="/posts/kolla%E9%83%A8%E7%BD%B2swift/"/>
    <url>/posts/kolla%E9%83%A8%E7%BD%B2swift/</url>
    
    <content type="html"><![CDATA[<h3 id="kolla部署swift"><a href="#kolla部署swift" class="headerlink" title="kolla部署swift"></a>kolla部署swift</h3><p>kolla实现对象存储方式有两种：</p><ul><li>利用ceph_rgw实现</li><li>部署swift实现</li></ul><p>本篇博客介绍利用kolla部署swift实现openstack对象存储</p><a id="more"></a><h3 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h3><p>我这是一个all-in-one的环境,利用kolla部署了一个pike版本,部署方法请参考<a href="https://www.lijiawang.org/posts/kolla-pike-on-centos.html#more">kolla部署openstack Pike版</a><br>我这台虚拟机一共有5块盘，分别为sda,sdb,sdc,sdd,sde。sda为系统盘，sdb,sdc,sdd,为ceph的osd用，sde用于做swift</p><h3 id="打标签"><a href="#打标签" class="headerlink" title="打标签"></a>打标签</h3><p>swift要求可用的块设备用来存储，所以我事先准备了一个硬盘<code>sde</code>作为存储设备,在部署需要给这块盘打一个特殊的标签。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs shell">index=0<br>for d in sde; do<br><span class="hljs-meta">#</span><span class="bash"> 因为我这就有一块用于swift存储的盘sde。</span><br>    parted /dev/$&#123;d&#125; -s -- mklabel gpt mkpart KOLLA_SWIFT_DATA 1 -1<br>    sudo mkfs.xfs -f -L d$&#123;index&#125; /dev/$&#123;d&#125;1<br>    (( index++ ))<br>done<br></code></pre></td></tr></table></figure><h3 id="Rings"><a href="#Rings" class="headerlink" title="Rings"></a>Rings</h3><p>kolla在部署swift前，需要生成rings，这是一种二进制压缩文件，在较高的级别上，让各种快速服务知道集群中的数据在哪里，这里官方给了一个脚本，简单的配置一下即可。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> STORAGE_NODES=(192.168.0.2 192.168.0.3 192.168.0.4)</span><br><span class="hljs-meta">#</span><span class="bash"> 这里是swift走的存储网络的ip，因为我是all-in-one环境，所以IP只有一个，这里我的swift存储用网将和管理网在一起</span><br>STORAGE_NODES=(192.168.10.139)<br><span class="hljs-meta">#</span><span class="bash"> KOLLA_SWIFT_BASE_IMAGE=<span class="hljs-string">&quot;docker_registry/docker_namespace/centos-source-swift-base:openstack_release&quot;</span></span><br><span class="hljs-meta">#</span><span class="bash"> 这里按照需求填写</span><br>KOLLA_SWIFT_BASE_IMAGE=&quot;192.168.10.139:4000/lokolla/centos-source-swift-base:5.0.1&quot;<br><br>mkdir -p /etc/kolla/config/swift<br><br><span class="hljs-meta">#</span><span class="bash"> Object ring</span><br>docker run \<br>  --rm \<br>  -v /etc/kolla/config/swift/:/etc/kolla/config/swift/ \<br><span class="hljs-meta">  $</span><span class="bash">KOLLA_SWIFT_BASE_IMAGE \</span><br><span class="bash">  swift-ring-builder \</span><br><span class="bash">    /etc/kolla/config/swift/object.builder create 10 3 1</span><br><br>for node in $&#123;STORAGE_NODES[@]&#125;; do<br>    for i in &#123;0..2&#125;; do<br>      docker run \<br>        --rm \<br>        -v /etc/kolla/config/swift/:/etc/kolla/config/swift/ \<br>        $KOLLA_SWIFT_BASE_IMAGE \<br>        swift-ring-builder \<br>          /etc/kolla/config/swift/object.builder add r1z1-$&#123;node&#125;:6000/d$&#123;i&#125; 1;<br>    done<br>done<br><br><span class="hljs-meta">#</span><span class="bash"> Account ring</span><br>docker run \<br>  --rm \<br>  -v /etc/kolla/config/swift/:/etc/kolla/config/swift/ \<br><span class="hljs-meta">  $</span><span class="bash">KOLLA_SWIFT_BASE_IMAGE \</span><br><span class="bash">  swift-ring-builder \</span><br><span class="bash">    /etc/kolla/config/swift/account.builder create 10 3 1</span><br><br>for node in $&#123;STORAGE_NODES[@]&#125;; do<br>    for i in &#123;0..2&#125;; do<br>      docker run \<br>        --rm \<br>        -v /etc/kolla/config/swift/:/etc/kolla/config/swift/ \<br>        $KOLLA_SWIFT_BASE_IMAGE \<br>        swift-ring-builder \<br>          /etc/kolla/config/swift/account.builder add r1z1-$&#123;node&#125;:6001/d$&#123;i&#125; 1;<br>    done<br>done<br><br><span class="hljs-meta">#</span><span class="bash"> Container ring</span><br>docker run \<br>  --rm \<br>  -v /etc/kolla/config/swift/:/etc/kolla/config/swift/ \<br><span class="hljs-meta">  $</span><span class="bash">KOLLA_SWIFT_BASE_IMAGE \</span><br><span class="bash">  swift-ring-builder \</span><br><span class="bash">    /etc/kolla/config/swift/container.builder create 10 3 1</span><br><br>for node in $&#123;STORAGE_NODES[@]&#125;; do<br>    for i in &#123;0..2&#125;; do<br>      docker run \<br>        --rm \<br>        -v /etc/kolla/config/swift/:/etc/kolla/config/swift/ \<br>        $KOLLA_SWIFT_BASE_IMAGE \<br>        swift-ring-builder \<br>          /etc/kolla/config/swift/container.builder add r1z1-$&#123;node&#125;:6002/d$&#123;i&#125; 1;<br>    done<br>done<br><br>for ring in object account container; do<br>  docker run \<br>    --rm \<br>    -v /etc/kolla/config/swift/:/etc/kolla/config/swift/ \<br>    $KOLLA_SWIFT_BASE_IMAGE \<br>    swift-ring-builder \<br>      /etc/kolla/config/swift/$&#123;ring&#125;.builder rebalance;<br>done<br></code></pre></td></tr></table></figure><h3 id="globals-yml文件"><a href="#globals-yml文件" class="headerlink" title="globals.yml文件"></a>globals.yml文件</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> vim /etc/kolla/globals.yml</span><br><span class="hljs-meta">#</span><span class="bash">enable_ceph_rgw: <span class="hljs-string">&quot;no&quot;</span></span><br>enable_swift: &quot;yes&quot;<br><span class="hljs-meta">#</span><span class="bash">enable_ceph_rgw_keystone: <span class="hljs-string">&quot;no&quot;</span></span><br></code></pre></td></tr></table></figure><p>这里需要说下，ceph_rgw实现的对象存储和swift实现的对象存储二者不能共存。如果要想用ceph_rgw实现以上步骤均不要做直接修改globals文件后deploy即可</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> vim /etc/kolla/globals.yml</span><br>enable_ceph_rgw: &quot;yes&quot;<br><span class="hljs-meta">#</span><span class="bash"> enable_swift: <span class="hljs-string">&quot;no&quot;</span></span><br>enable_ceph_rgw_keystone: &quot;yes&quot;<br></code></pre></td></tr></table></figure><h3 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> kolla-ansible deploy</span><br></code></pre></td></tr></table></figure><h3 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> <span class="hljs-built_in">source</span> /etc/kolla/admin-openrc.sh</span> <br><span class="hljs-meta">#</span><span class="bash"> openstack container create mycontainer</span><br>+---------------------------------------+-------------+------------------------------------+<br>| account                               | container   | x-trans-id                         |<br>+---------------------------------------+-------------+------------------------------------+<br>| AUTH_2a568be969b34c288ef52a2987973145 | mycontainer | txc68d723c0c934a7282cfe-005a1e5d11 |<br>+---------------------------------------+-------------+------------------------------------+<br>[root@pike ~]# openstack object create mycontainer README.rst<br>+------------+-------------+----------------------------------+<br>| object     | container   | etag                             |<br>+------------+-------------+----------------------------------+<br>| README.rst | mycontainer | a2f288979d0af55c5e9313f525c75615 |<br>+------------+-------------+----------------------------------+<br><span class="hljs-meta">#</span><span class="bash"> openstack container show mycontainer</span><br>+--------------+---------------------------------------+<br>| Field        | Value                                 |<br>+--------------+---------------------------------------+<br>| account      | AUTH_2a568be969b34c288ef52a2987973145 |<br>| bytes_used   | 39                                    |<br>| container    | mycontainer                           |<br>| object_count | 1                                     |<br>+--------------+---------------------------------------+<br><span class="hljs-meta">#</span><span class="bash"> openstack object store account show</span><br>+------------+---------------------------------------+<br>| Field      | Value                                 |<br>+------------+---------------------------------------+<br>| Account    | AUTH_2a568be969b34c288ef52a2987973145 |<br>| Bytes      | 0                                     |<br>| Containers | 1                                     |<br>| Objects    | 0                                     |<br>+------------+---------------------------------------+<br></code></pre></td></tr></table></figure><p><img src="https://ljw.howieli.cn/blog/2017-11-29/swift.png"></p><h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h3><p><a href="https://docs.openstack.org/kolla-ansible/latest/reference/swift-guide.html#rings">https://docs.openstack.org/kolla-ansible/latest/reference/swift-guide.html#rings</a></p>]]></content>
    
    
    <categories>
      
      <category>openstack</category>
      
    </categories>
    
    
    <tags>
      
      <tag>swift</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>openstack虚拟机VIP配置步骤</title>
    <link href="/posts/openstack%E8%99%9A%E6%8B%9F%E6%9C%BAVIP%E9%85%8D%E7%BD%AE%E6%AD%A5%E9%AA%A4/"/>
    <url>/posts/openstack%E8%99%9A%E6%8B%9F%E6%9C%BAVIP%E9%85%8D%E7%BD%AE%E6%AD%A5%E9%AA%A4/</url>
    
    <content type="html"><![CDATA[<h3 id="在openstack上的虚拟机绑定vip"><a href="#在openstack上的虚拟机绑定vip" class="headerlink" title="在openstack上的虚拟机绑定vip"></a>在openstack上的虚拟机绑定vip</h3><p>有些情况下，客户想在openstack的虚拟机上配置vip搭建高可用集群，下面我就简单的说下在openstack上的虚拟机如何绑定vip</p><a id="more"></a><h3 id="操作步骤"><a href="#操作步骤" class="headerlink" title="操作步骤"></a>操作步骤</h3><p>1、导入环境变量</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> <span class="hljs-built_in">source</span> admin-openrc.sh</span><br></code></pre></td></tr></table></figure><p>2、执行命令neutron net-list查看网络，找到自己需要设置的网络，获取subnet_id和network_id</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> neutron net-list</span><br>+--------------------------------------+----------------------------------------------------+----------------------------------+-------------------------------------------------------+<br>| id                                   | name                                               | tenant_id                        | subnets                                               |<br>+--------------------------------------+----------------------------------------------------+----------------------------------+-------------------------------------------------------+<br>| 32482d56-bb40-4b7f-85df-3be3a460e441 | HA network tenant 7ba30c1e519d4d6eb8f1ace2cfbf30d3 |                                  | 860bf95f-4775-4fac-af88-db392f254416 169.254.192.0/18 |<br>| 7cc26554-2795-4a53-b053-34ec1b4c90f2 | web                                                | 7ba30c1e519d4d6eb8f1ace2cfbf30d3 | 4b1f707b-8842-4ce0-acba-4f0de304459b 192.168.1.0/24   |<br>| d0ad534f-1bcd-43b0-aa0c-edee32520020 | public                                             | 21c161dda51147fb9ff527aadfe1d81a | 9a7f07e5-e906-4622-8bc6-def64b3622ec 172.18.23.0/24   |<br>+--------------------------------------+----------------------------------------------------+----------------------------------+-------------------------------------------------------+<br></code></pre></td></tr></table></figure><p><img src="https://ljw.howieli.cn/blog/2017-11-28/net-list.png"><br>3、创建port来占用ip，保证neutron不会将此IP在分配出去，导致IP冲突问题。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs shell">neutron port-create --fixed-ip subnet_id=&lt;subnet_id&gt;,ip_address=&lt;vip&gt; &lt;network_id&gt;<br>注：<br>替换subnet_id为neutron net-list中查看到的subnet_id<br>替换vip为需要配置的vip地址<br>替换network_ID为neutron net-list中查看到的network_id<br></code></pre></td></tr></table></figure><p>具体命令如下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> neutron port-create --fixed-ip subnet_id=9a7f07e5-e906-4622-8bc6-def64b3622ec,ip_address=172.18.23.10 d0ad534f-1bcd-43b0-aa0c-edee32520020</span><br>Created a new port:<br>+-----------------------+-------------------------------------------------------------------------------------+<br>| Field                 | Value                                                                               |<br>+-----------------------+-------------------------------------------------------------------------------------+<br>| admin_state_up        | True                                                                                |<br>| allowed_address_pairs |                                                                                     |<br>| binding:host_id       |                                                                                     |<br>| binding:profile       | &#123;&#125;                                                                                  |<br>| binding:vif_details   | &#123;&#125;                                                                                  |<br>| binding:vif_type      | unbound                                                                             |<br>| binding:vnic_type     | normal                                                                              |<br>| created_at            | 2017-11-28T02:35:17Z                                                                |<br>| description           |                                                                                     |<br>| device_id             |                                                                                     |<br>| device_owner          |                                                                                     |<br>| extra_dhcp_opts       |                                                                                     |<br>| fixed_ips             | &#123;&quot;subnet_id&quot;: &quot;9a7f07e5-e906-4622-8bc6-def64b3622ec&quot;, &quot;ip_address&quot;: &quot;172.18.23.10&quot;&#125; |<br>| id                    | 7c7ccc26-9ac9-4ef7-8178-2b97218b1d63                                                |<br>| mac_address           | fa:16:3e:ea:81:a6                                                                   |<br>| name                  |                                                                                     |<br>| network_id            | d0ad534f-1bcd-43b0-aa0c-edee32520020                                                |<br>| port_security_enabled | True                                                                                |<br>| project_id            | 21c161dda51147fb9ff527aadfe1d81a                                                    |<br>| revision_number       | 5                                                                                   |<br>| security_groups       | abfba384-55f2-4eed-902a-712369be9604                                                |<br>| status                | DOWN                                                                                |<br>| tags                  |                                                                                     |<br>| tenant_id             | 21c161dda51147fb9ff527aadfe1d81a                                                    |<br>| updated_at            | 2017-11-28T02:35:18Z                                                                |<br>+-----------------------+-------------------------------------------------------------------------------------+<br></code></pre></td></tr></table></figure><p><img src="https://ljw.howieli.cn/blog/2017-11-28/port-create.png"><br>4、执行命令<code>neutron port-list</code>查看端口，找到VIP的Port ID以及需要使用VIP的虚拟机的IP对应的Port id<br>比如两台虚拟机做HA绑定vip，那么需要查看两台虚拟机的port ID和这个vip的port ID</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> neutron port-list|grep 172.18.23.10</span><br>| 7c7ccc26-9ac9-4ef7-8178-2b97218b1d63 |                                                 | 21c161dda51147fb9ff527aadfe1d81a | fa:16:3e:ea:81:a6 | &#123;&quot;subnet_id&quot;: &quot;9a7f07e5-e906-4622-8bc6-def64b3622ec&quot;, &quot;ip_address&quot;: &quot;172.18.23.10&quot;&#125;  |<br></code></pre></td></tr></table></figure><p>可以看出vip<code>172.18.23.10</code>的port id为<code>7c7ccc26-9ac9-4ef7-8178-2b97218b1d63</code>.<br>5、取消安全组对应端口的管理</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell">neutron port-update --no-security-groups &lt;Port_id&gt;<br>neutron port-update --port_security_enabled=false &lt;Port_id&gt;<br>注：<br>    替换Port_id为之前neutron port-list中找到的Port_id<br></code></pre></td></tr></table></figure><p>具有命令如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> neutron port-update --no-security-groups 7c7ccc26-9ac9-4ef7-8178-2b97218b1d63</span><br>Updated port: 7c7ccc26-9ac9-4ef7-8178-2b97218b1d63<br><span class="hljs-meta">#</span><span class="bash"> neutron port-update --port_security_enabled=<span class="hljs-literal">false</span> 7c7ccc26-9ac9-4ef7-8178-2b97218b1d63</span><br>Updated port: 7c7ccc26-9ac9-4ef7-8178-2b97218b1d63<br></code></pre></td></tr></table></figure><p>6、此时执行命令neutron port-show <Port_id><br><img src="https://ljw.howieli.cn/blog/2017-11-28/port-show.png"><br>可看到port_security_enabled的value为False，security_groups的value为空，即OK，这样两个端口就没有了安全组了。<br>7、意思就是对VIP和需要使用VIP的虚拟机都执行4、5、6步，比如配置HA，VIP+两台虚拟机，总共3个Port，都需要执行4、5、6步<br>然后就可以在这两台虚拟机上搭建keepalived集群使用<code>172.18.23.10</code>这个vip了。</p>]]></content>
    
    
    <categories>
      
      <category>openstack</category>
      
    </categories>
    
    
    <tags>
      
      <tag>openstack</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>kolla Pike on CentOS 7.4</title>
    <link href="/posts/kolla-Pike-on-CentOS-7-4/"/>
    <url>/posts/kolla-Pike-on-CentOS-7-4/</url>
    
    <content type="html"><![CDATA[<h3 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h3><p>我这里用workstation 创建了一个虚拟机，安装centos7.4系统，这台虚拟机上有两张网卡，一张做openstack管理网，一张做为虚拟机的业务网卡。</p><a id="more"></a><h3 id="确认环境信息"><a href="#确认环境信息" class="headerlink" title="确认环境信息"></a>确认环境信息</h3><p>1.确认系统版本</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> cat /etc/redhat-release</span><br>CentOS Linux release 7.4.1708 (Core)<br></code></pre></td></tr></table></figure><p>2.确认网卡个数和状态</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> ip a</span><br>1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1<br>    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00<br>    inet 127.0.0.1/8 scope host lo<br>       valid_lft forever preferred_lft forever<br>    inet6 ::1/128 scope host<br>       valid_lft forever preferred_lft forever<br>2: ens33: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000<br>    link/ether 00:0c:29:a7:8c:91 brd ff:ff:ff:ff:ff:ff<br>    inet 192.168.10.139/24 brd 192.168.10.255 scope global dynamic ens33<br>       valid_lft 1555sec preferred_lft 1555sec<br>    inet6 fe80::5057:38c5:6b65:b5/64 scope link<br>       valid_lft forever preferred_lft forever<br>3: ens34: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000<br>    link/ether 00:0c:29:a7:8c:9b brd ff:ff:ff:ff:ff:ff<br>    inet6 fe80::20c:29ff:fea7:8c9b/64 scope link<br>       valid_lft forever preferred_lft forever<br></code></pre></td></tr></table></figure><p>上面可以看出有两张网卡<code>ens33</code>和<code>ens34</code>，这里我用<code>ens33</code>做管理网，<code>ens34</code>做业务网，这里不需要配置ip，把<code>ens34</code>网卡up起来就好。</p><h3 id="环境初始化"><a href="#环境初始化" class="headerlink" title="环境初始化"></a>环境初始化</h3><p>1.更改主机名</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> hostnamectl set-hostname pike</span><br><span class="hljs-meta">#</span><span class="bash"> hostname pike</span><br><span class="hljs-meta">#</span><span class="bash"> hostname</span><br>pike<br></code></pre></td></tr></table></figure><p>2.关闭NetworkManager,firewalld,selinux</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> systemctl stop NetworkManager</span><br><span class="hljs-meta">#</span><span class="bash"> systemctl <span class="hljs-built_in">disable</span> NetworkManager</span><br>Removed symlink /etc/systemd/system/multi-user.target.wants/NetworkManager.service.<br>Removed symlink /etc/systemd/system/dbus-org.freedesktop.NetworkManager.service.<br>Removed symlink /etc/systemd/system/dbus-org.freedesktop.nm-dispatcher.service.<br><span class="hljs-meta">#</span><span class="bash"> systemctl stop firewalld</span><br><span class="hljs-meta">#</span><span class="bash"> systemctl <span class="hljs-built_in">disable</span> firewalld</span><br>Removed symlink /etc/systemd/system/multi-user.target.wants/firewalld.service.<br>Removed symlink /etc/systemd/system/dbus-org.fedoraproject.FirewallD1.service.<br><span class="hljs-meta">#</span><span class="bash"> sed -i <span class="hljs-string">&quot;s/SELINUX=enforcing/SELINUX=disabled/&quot;</span> /etc/selinux/config</span><br><span class="hljs-meta">#</span><span class="bash"> setenforce 0</span><br><span class="hljs-meta">#</span><span class="bash"> getenforce</span><br>Permissive<br></code></pre></td></tr></table></figure><p>3.查看是否开启了虚拟化</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> egrep <span class="hljs-string">&quot;vmx|svm&quot;</span> /proc/cpuinfo</span><br></code></pre></td></tr></table></figure><h3 id="安装基础包"><a href="#安装基础包" class="headerlink" title="安装基础包"></a>安装基础包</h3><p>1.配置epel源安装基础包</p><figure class="highlight vala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs vala"><span class="hljs-meta"># yum install epel-release</span><br><span class="hljs-meta"># yum install axel vim git curl wget lrzsz gcc  python-devel yum* python-pip</span><br></code></pre></td></tr></table></figure><p>2.配置docker-ce的yum源安装docker-ce,这里我用的docker版本为docker-ce17.09</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo</span><br><span class="hljs-meta">#</span><span class="bash"> yum install -y docker-ce</span><br><span class="hljs-meta">#</span><span class="bash"> docker -v</span><br>Docker version 17.09.0-ce, build afdb6d4<br></code></pre></td></tr></table></figure><p>3.配置docker</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> mkdir /etc/systemd/system/docker.service.d</span><br><span class="hljs-meta">#</span><span class="bash"> tee /etc/systemd/system/docker.service.d/kolla.conf &lt;&lt; <span class="hljs-string">&#x27;EOF&#x27;</span></span><br>[Service]<br>MountFlags=shared<br>EOF<br><span class="hljs-meta">#</span><span class="bash"> vim /usr/lib/systemd/system/docker.service</span><br><span class="hljs-meta">#</span><span class="bash"> ExecStart=/usr/bin/dockerd</span><br>ExecStart=/usr/bin/dockerd --insecure-registry 192.168.10.139:4000<br></code></pre></td></tr></table></figure><p>4.启动docker</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> systemctl daemon-reload</span><br><span class="hljs-meta">#</span><span class="bash"> systemctl restart docker</span><br><span class="hljs-meta">#</span><span class="bash"> systemctl <span class="hljs-built_in">enable</span> docker</span><br>Created symlink from /etc/systemd/system/multi-user.target.wants/docker.service to /usr/lib/systemd/system/docker.service.<br><span class="hljs-meta">#</span><span class="bash"> systemctl status docker</span><br><span class="hljs-meta">#</span><span class="bash"> docker info</span><br></code></pre></td></tr></table></figure><h3 id="安装ansible"><a href="#安装ansible" class="headerlink" title="安装ansible"></a>安装ansible</h3><p>ansible版本必须在2.0以上</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> yum -y install ansible -y</span><br></code></pre></td></tr></table></figure><h3 id="搭建Registry"><a href="#搭建Registry" class="headerlink" title="搭建Registry"></a>搭建Registry</h3><p>默认docker的registry是使用5000端口，对于OpenStack来说，有端口冲突，所以我将端口改成了4000。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> docker run -d -v /opt/registry:/var/lib/registry -p 4000:5000 \</span><br><span class="bash">--restart=always --name registry registry:2</span><br><span class="hljs-meta">#</span><span class="bash"> docker ps</span><br>CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                    NAMES<br>005883e5a115        registry:2          &quot;/entrypoint.sh /e...&quot;   24 seconds ago      Up 22 seconds       0.0.0.0:4000-&gt;5000/tcp   registry<br></code></pre></td></tr></table></figure><h3 id="下载kolla官方提供的Pile镜像"><a href="#下载kolla官方提供的Pile镜像" class="headerlink" title="下载kolla官方提供的Pile镜像"></a>下载kolla官方提供的Pile镜像</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> wget http://tarballs.openstack.org/kolla/images/centos-source-registry-pike.tar.gz</span><br><span class="hljs-meta">#</span><span class="bash"> du -sh centos-source-registry-pike.tar.gz</span><br>4.2Gcentos-source-registry-pike.tar.gz<br><span class="hljs-meta">#</span><span class="bash"> tar zxf centos-source-registry-pike.tar.gz -C /opt/registry/</span><br><span class="hljs-meta">#</span><span class="bash"> curl http://192.168.10.139:4000/v2/_catalog</span><br></code></pre></td></tr></table></figure><h3 id="下载kolla-ansible，并配置"><a href="#下载kolla-ansible，并配置" class="headerlink" title="下载kolla-ansible，并配置"></a>下载kolla-ansible，并配置</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> git <span class="hljs-built_in">clone</span> https://github.com/openstack/kolla-ansible -b stable/pike</span><br><span class="hljs-meta">#</span><span class="bash"> <span class="hljs-built_in">cd</span> kolla-ansible/</span><br><span class="hljs-meta">#</span><span class="bash"> cp -r etc/kolla/ /etc/kolla/</span><br><span class="hljs-meta">#</span><span class="bash"> pip install . -i https://pypi.tuna.tsinghua.edu.cn/simple</span><br></code></pre></td></tr></table></figure><h3 id="安装kolla"><a href="#安装kolla" class="headerlink" title="安装kolla"></a>安装kolla</h3><p>1.因为我是在虚拟机安装的kolla，希望可以启动虚拟机，那么需要把virt_type=qemu,默认是kvm</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> mkdir -p /etc/kolla/config/nova</span><br><span class="hljs-meta">#</span><span class="bash"> cat &lt;&lt; <span class="hljs-string">EOF &gt; /etc/kolla/config/nova/nova-compute.conf</span></span><br><span class="hljs-meta">&gt;</span><span class="bash"> [libvirt]</span><br><span class="hljs-meta">&gt;</span><span class="bash"> virt_type=qemu</span><br><span class="hljs-meta">&gt;</span><span class="bash"> cpu_mode = none</span><br><span class="hljs-meta">&gt;</span><span class="bash"> EOF</span><br></code></pre></td></tr></table></figure><p>2.ceph打标签,下面两个命令分别用来快速删除 osd disk 的分区和快速给 disk 打 ceph 标签</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> lsblk</span><br><span class="hljs-meta">#</span><span class="bash"> lsblk | grep <span class="hljs-string">&quot;^sd[^a]&quot;</span> | awk <span class="hljs-string">&#x27;&#123;print $1&#125;&#x27;</span> | <span class="hljs-keyword">while</span> <span class="hljs-built_in">read</span> line; <span class="hljs-keyword">do</span> \</span><br><span class="bash">parted /dev/<span class="hljs-variable">$line</span> -s -- mklabel gpt mkpart KOLLA_CEPH_OSD_BOOTSTRAP 1 -1; \</span><br><span class="bash"><span class="hljs-keyword">done</span></span><br><span class="hljs-meta">#</span><span class="bash"> lsblk</span><br></code></pre></td></tr></table></figure><p>3.配置GLOBALS.YML文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> cat /etc/kolla/globals.yml |grep -Ev <span class="hljs-string">&#x27;^$|^#&#x27;</span></span><br>---<br>kolla_base_distro: &quot;centos&quot;<br>kolla_install_type: &quot;source&quot;<br>openstack_release: &quot;5.0.1&quot;<br>kolla_internal_vip_address: &quot;192.168.10.139&quot;<br>docker_registry: &quot;192.168.10.139:4000&quot;<br>docker_namespace: &quot;lokolla&quot;<br>network_interface: &quot;ens33&quot;<br>neutron_external_interface: &quot;ens34&quot;<br>keepalived_virtual_router_id: &quot;52&quot;<br>enable_ceph: &quot;yes&quot;<br>enable_chrony: &quot;yes&quot;<br>enable_cinder: &quot;yes&quot;<br>enable_haproxy: &quot;no&quot;<br>enable_horizon: &quot;yes&quot;<br>tempest_image_id:<br>tempest_flavor_ref_id:<br>tempest_public_network_id:<br>tempest_floating_network_name:<br></code></pre></td></tr></table></figure><p>4.创建 /etc/kolla/config/ceph.conf</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">[global]<br>osd pool default size = 1<br>osd pool default min size = 1<br></code></pre></td></tr></table></figure><h3 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h3><p>1.生成密码,编辑/etc/kolla/passwords.yml</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> kolla-genpwd</span><br><span class="hljs-meta">#</span><span class="bash"> vim /etc/kolla/passwords.yml</span><br>keystone_admin_password: lijiawang<br></code></pre></td></tr></table></figure><p>这是登录Dashboard，admin使用的密码，你可以根据自己需要进行修改<br>2.部署检查</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> kolla-ansible prechecks</span><br></code></pre></td></tr></table></figure><p>3.部署</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> kolla-ansible deploy</span><br><span class="hljs-meta">#</span><span class="bash"> kolla-ansible post-deploy</span><br></code></pre></td></tr></table></figure><h3 id="安装openstack-client-端"><a href="#安装openstack-client-端" class="headerlink" title="安装openstack client 端"></a>安装openstack client 端</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> pip install python-openstackclient</span><br></code></pre></td></tr></table></figure><h3 id="创建网络"><a href="#创建网络" class="headerlink" title="创建网络"></a>创建网络</h3><p>1.编辑 /usr/share/kolla-ansible/init-runonce<br>网络需要根据实际情况修改,我都是用的nat模式</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">EXT_NET_CIDR=&#x27;192.168.10.0/24&#x27;<br>EXT_NET_RANGE=&#x27;start=192.168.10.150,end=192.168.10.199&#x27;<br>EXT_NET_GATEWAY=&#x27;192.168.10.2&#x27;<br></code></pre></td></tr></table></figure><p>2.运行脚本</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> <span class="hljs-built_in">source</span> /etc/kolla/admin-openrc.sh</span><br><span class="hljs-meta">#</span><span class="bash"> bash /usr/share/kolla-ansible/init-runonce</span><br></code></pre></td></tr></table></figure><p>3.创建实例绑定浮动ip，测试网络</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@pike ~]# nova list<br>+--------------------------------------+-------+--------+------------+-------------+------------------------------------+<br>| ID                                   | Name  | Status | Task State | Power State | Networks                           |<br>+--------------------------------------+-------+--------+------------+-------------+------------------------------------+<br>| cbc40238-3bee-4df1-abc0-a10fc269e6e0 | demo1 | ACTIVE | -          | Running     | demo-net=10.0.0.10, 192.168.10.154 |<br>+--------------------------------------+-------+--------+------------+-------------+------------------------------------+<br>[root@pike ~]# ping 192.168.10.154<br>PING 192.168.10.154 (192.168.10.154) 56(84) bytes of data.<br>64 bytes from 192.168.10.154: icmp_seq=1 ttl=63 time=3.91 ms<br>64 bytes from 192.168.10.154: icmp_seq=2 ttl=63 time=4.56 ms<br>64 bytes from 192.168.10.154: icmp_seq=3 ttl=63 time=2.04 ms<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>openstack</category>
      
    </categories>
    
    
    <tags>
      
      <tag>openstack</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>opensyack glance学习</title>
    <link href="/posts/opensyack-glance%E5%AD%A6%E4%B9%A0/"/>
    <url>/posts/opensyack-glance%E5%AD%A6%E4%B9%A0/</url>
    
    <content type="html"><![CDATA[<h3 id="openstack-Glance介绍"><a href="#openstack-Glance介绍" class="headerlink" title="openstack Glance介绍"></a>openstack Glance介绍</h3><p>Glance项目提供虚拟机镜像的发现，注册，取得服务。<br>Glance提供restful API可以查询虚拟机镜像的metadata，并且可以获得镜像。<br>通过Glance，虚拟机镜像可以被存储到多种存储上，比如简单的文件存储或者对象存储（比如OpenStack中swiftx项目）。<br>Glance，像所有的OpenStack项目一样，遵循以下思想：<br>1.基于组件的架构      便于快速增加新特性<br>2.高可用性                  支持大负荷<br>3.容错性                      独立的进程避免串行错误<br>4.开放标准                  对社区驱动的API提供参考实现</p><a id="more"></a><h3 id="Glance架构"><a href="#Glance架构" class="headerlink" title="Glance架构"></a>Glance架构</h3><p><img src="https://ljw.howieli.cn/blog/2017-10-26/glance%E6%9E%B6%E6%9E%84%E5%9B%BE.png"><br>以上是glance架构图<br><a href="https://docs.openstack.org/glance/latest/">glance官网</a></p><h3 id="glance服务"><a href="#glance服务" class="headerlink" title="glance服务"></a>glance服务</h3><p>我这是一个kolla环境</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@control01 ~]# docker ps |grep glance<br>1b240466af76        172.18.22.161:4000/99cloud/centos-source-glance-registry:animbus-5.3.0             &quot;kolla_start&quot;            2 weeks ago         Up 17 hours                                  glance_registry<br>3f25b6ace631        172.18.22.161:4000/99cloud/centos-source-glance-api:animbus-5.3.0                  &quot;kolla_start&quot;            2 weeks ago         Up 17 hours                                  glance_api<br></code></pre></td></tr></table></figure><h3 id="glancce-api"><a href="#glancce-api" class="headerlink" title="glancce-api"></a>glancce-api</h3><p>glance-api 是系统后台运行的服务进程。 对外提供 REST API，响应 image 查询、获取和存储的调用。</p><p>glance-api 不会真正处理请求。 如果操作是与 image metadata（元数据）相关，glance-api 会把请求转发给 glance-registry； 如果操作是与 image 自身存取相关，glance-api 会把请求转发给该 image 的 store backend。<br>在控制节点可以查看glance-api的进程</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@control01 ~]# ps -e |grep glance-api<br> 4381 ?        00:09:56 glance-api<br> 9026 ?        00:00:14 glance-api<br> 9035 ?        00:00:11 glance-api<br> 9046 ?        00:00:14 glance-api<br> 9059 ?        00:00:16 glance-api<br> 9067 ?        00:00:13 glance-api<br></code></pre></td></tr></table></figure><h3 id="glance-registry"><a href="#glance-registry" class="headerlink" title="glance-registry"></a>glance-registry</h3><p>glance-registry 是系统后台运行的服务进程。 负责处理和存取 image 的 metadata，例如 image 的大小和类型。在控制节点上可以查看 glance-registry 进程.</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@control01 ~]# ps -e |grep glance-registry<br> 2645 ?        00:00:03 glance-registry<br> 5351 ?        00:00:00 glance-registry<br> 5352 ?        00:00:00 glance-registry<br> 5353 ?        00:00:00 glance-registry<br> 5354 ?        00:00:00 glance-registry<br> 5355 ?        00:00:00 glance-registry<br></code></pre></td></tr></table></figure><h3 id="Store-backend"><a href="#Store-backend" class="headerlink" title="Store backend"></a>Store backend</h3><p>Glance 自己并不存储 image。 真正的 image 是存放在 backend 中的。 Glance 支持多种 backend，包括：</p><ul><li>A directory on a local file system（这是默认配置）</li><li>GridFS</li><li>Ceph RBD</li><li>Amazon S3</li><li>Sheepdog</li><li>OpenStack Block Storage (Cinder)</li><li>OpenStack Object Storage (Swift)</li><li>VMware ESX</li></ul><h3 id="image数据存放"><a href="#image数据存放" class="headerlink" title="image数据存放"></a>image数据存放</h3><p>image的元数据通过glance-registry存放到db中；image的chunk数据通过glance-store存放在各种 backend store 中，并从中获取。</p><h3 id="image的访问权限"><a href="#image的访问权限" class="headerlink" title="image的访问权限"></a>image的访问权限</h3><p>image 的 访问权限分为：</p><ul><li>public 公共的：可以被所有的 tenant 使用。</li><li>private 私有的/项目的：只能被 image owner 所在的 tenant 使用。</li><li>shared 共享的：一个非共有的image 可以 共享给另外的 tenant，可通过member-* 操作来实现。</li><li>protected 受保护的：protected 的 image 不能被删除。</li></ul><h3 id="image各种状态"><a href="#image各种状态" class="headerlink" title="image各种状态"></a>image各种状态</h3><ul><li>queued：没有上传 image 数据，只有db 中的元数据。</li><li>saving：正在上传 image data</li><li>active：正常状态</li><li>deleted/pending_delete： 已删除/等待删除</li><li>killed：image 元数据不正确，等待被删除。</li></ul>]]></content>
    
    
    <categories>
      
      <category>openstack</category>
      
    </categories>
    
    
    <tags>
      
      <tag>openstack</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>openstack性能测试</title>
    <link href="/posts/openstack%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/"/>
    <url>/posts/openstack%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/</url>
    
    <content type="html"><![CDATA[<h3 id="文档说明"><a href="#文档说明" class="headerlink" title="文档说明"></a>文档说明</h3><p>本文档验证在openstack ocata环境上开展自动化测试，性能测试，压力测试。</p><a id="more"></a><h3 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h3><p>openstack ocata for kolla环境一台（多节点，三个控制也做计算存储）</p><h3 id="接口测试"><a href="#接口测试" class="headerlink" title="接口测试"></a>接口测试</h3><h4 id="Tempest-介绍"><a href="#Tempest-介绍" class="headerlink" title="Tempest 介绍"></a>Tempest 介绍</h4><p>目前Tempest 的测试代码，包括计算、认证、镜像管理、网络、对象存储、块存储 六个核心模块。</p><p><img src="https://ljw.howieli.cn/blog/2017-9-8/tempest.png"></p><h4 id="Tempest-安装"><a href="#Tempest-安装" class="headerlink" title="Tempest 安装"></a>Tempest 安装</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@control01 ~]<span class="hljs-comment"># vim /etc/kolla/globals.yml</span><br>enable_tempest: <span class="hljs-string">&quot;yes&quot;</span><br>[root@control01 ~]<span class="hljs-comment"># kolla-ansible -i multinode deploy</span><br>[root@control01 ~]<span class="hljs-comment"># docker ps |grep tempest</span><br>06512704e13e        172.18.22.162:4000/99cloud/centos-source-tempest:animbus-5.2.0                     <span class="hljs-string">&quot;kolla_start&quot;</span>            8 days ago          Up 8 days                                    tempest<br></code></pre></td></tr></table></figure><h4 id="Tempest-配置"><a href="#Tempest-配置" class="headerlink" title="Tempest 配置"></a>Tempest 配置</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@control01 ~]<span class="hljs-comment"># docker exec -it -u root tempest bash</span><br>(tempest)[root@control01 tempest-16.0.1.dev358]<span class="hljs-comment">#</span><br>(tempest)[root@control01 tempest-16.0.1.dev358]<span class="hljs-comment"># vi /etc/tempest/tempest.conf</span><br>image_ref = a1958f75-ae81-4b1d-a167-213f4681856b<br>image_ref_alt = a1958f75-ae81-4b1d-a167-213f4681856b<br>flavor_ref = 2<br>flavor_ref_alt = 2<br>public_network_id = f157770d-e81c-41a3-acf1-8fc3f4ea7f31<br>floating_network_name = f157770d-e81c-41a3-acf1-8fc3f4ea7f31<br></code></pre></td></tr></table></figure><h4 id="测试执行"><a href="#测试执行" class="headerlink" title="测试执行"></a>测试执行</h4><h5 id="执行一个测试类"><a href="#执行一个测试类" class="headerlink" title="执行一个测试类"></a>执行一个测试类</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">(tempest)[root@control01 tempest-16.0.1.dev358]<span class="hljs-comment"># ostestr --pdb tempest.api.compute.servers.test_create_server | tee tempest_compute.log</span><br></code></pre></td></tr></table></figure><p>测试输出</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs shell">======<br>Totals<br>======<br>Ran: 16 tests in 334.0000 sec.<br> - Passed: 12<br> - Skipped: 4<br> - Expected Fail: 0<br> - Unexpected Success: 0<br> - Failed: 0<br>Sum of execute time for each test: 232.6021 sec.<br><br>==============<br>Worker Balance<br>==============<br> - Worker 0 (16 tests) =&gt; 0:04:47.392702<br></code></pre></td></tr></table></figure><h5 id="执行多个项目下的所有测试，并输出subunit"><a href="#执行多个项目下的所有测试，并输出subunit" class="headerlink" title="执行多个项目下的所有测试，并输出subunit"></a>执行多个项目下的所有测试，并输出subunit</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">(tempest)[root@control01 tempest-16.0.1.dev358]<span class="hljs-comment"># ostestr --no-pretty --subunit --regex &#x27;^tempest\.api\.(compute|identity|image|network|volume|object_storage)&#x27; | tee tempest_all.log</span><br></code></pre></td></tr></table></figure><p>测试输出</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs shell">Test id                                                                                                                                                               Runtime (s)<br>--------------------------------------------------------------------------------------------------------------------------------------------------------------------  -----------<br>tempest.api.volume.test_volume_delete_cascade.VolumesDeleteCascade.test_volume_from_snapshot_cascade_delete[id-59a77ede-609b-4ee8-9f68-fc3c6ffe97b5]                  304.790<br>tempest.api.image.v1.test_images.CreateRegisterImagesTest.test_register_http_image[id-6d0e13a7-515b-460c-b91f-9f4793f09816]                                           301.614<br>tempest.api.compute.servers.test_attach_interfaces.AttachInterfacesTestJSON.test_create_list_show_delete_interfaces[id-73fe8f02-590d-4bf1-b184-e9ca81065051,network]  170.781<br>tempest.api.compute.servers.test_delete_server.DeleteServersTestJSON.test_delete_server_while_in_shelved_state[id-bb0cb402-09dd-4947-b6e5-5e7e1cfa61ad]               153.487<br>tempest.api.compute.volumes.test_attach_volume.AttachVolumeTestJSON.test_list_get_volume_attachments[id-7fa563fe-f0f7-43eb-9e22-a1ece036b513]                         105.981<br>tempest.api.compute.images.test_images_oneserver.ImagesOneServerTestJSON.test_create_delete_image[id-3731d080-d4c5-4872-b41a-64d0d0021314]                             71.267<br>tempest.api.volume.test_volumes_snapshots.VolumesSnapshotTestJSON.test_snapshot_create_delete_with_volume_in_use[compute,id-8567b54c-4455-446d-a1cf-651ddeaa3ff2]      69.934<br>tempest.api.volume.test_volumes_snapshots.VolumesSnapshotTestJSON.test_snapshot_create_offline_delete_online[compute,id-5210a1de-85a0-11e6-bb21-641c676a5d61]          68.662<br>tempest.api.compute.servers.test_delete_server.DeleteServersTestJSON.test_delete_server_while_in_shutoff_state[id-546d368c-bb6c-4645-979a-83ed16f3a6be]                61.945<br>tempest.api.compute.servers.test_attach_interfaces.AttachInterfacesTestJSON.test_add_remove_fixed_ip[id-c7e0e60b-ee45-43d0-abeb-8596fd42a2f9,network,smoke]            60.422<br><br>Slowest Tests:<br></code></pre></td></tr></table></figure><h4 id="转换测试结果为html文件"><a href="#转换测试结果为html文件" class="headerlink" title="转换测试结果为html文件"></a>转换测试结果为html文件</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">(tempest)[root@control01 tempest-16.0.1.dev358]<span class="hljs-comment"># subunit2html tempest_all.log tempest.html</span><br></code></pre></td></tr></table></figure><h4 id="Tempest-执行时间很长，如何放到后台执行"><a href="#Tempest-执行时间很长，如何放到后台执行" class="headerlink" title="Tempest 执行时间很长，如何放到后台执行"></a>Tempest 执行时间很长，如何放到后台执行</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@control01 ~]<span class="hljs-comment"># screen -r tempest</span><br>[root@control01 ~]<span class="hljs-comment"># docker exec -it -u root tempest ostestr --no-pretty --subunit --regex  &#x27;^tempest\.api\.(compute|identity|image|network|volume|object_storage)&#x27; | tee tempest_all.log</span><br></code></pre></td></tr></table></figure><p><code>screen</code>窗口键入<code>ctrl</code>+<code>a</code> <code>d</code>,退出<code>screen</code><br>再次进入</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@control01 ~]<span class="hljs-comment"># screen -r tempest</span><br></code></pre></td></tr></table></figure><h4 id="执行测试后提示：No-testr-conf-config-file"><a href="#执行测试后提示：No-testr-conf-config-file" class="headerlink" title="执行测试后提示：No .testr.conf config file"></a>执行测试后提示：No .testr.conf config file</h4><p>需要切换到<code>/tempest-source/tempest-16.0.1.dev358</code>目录执行，如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">(tempest)[root@control01 tempest-16.0.1.dev358]<span class="hljs-comment"># ls .testr.conf</span><br>.testr.conf<br>(tempest)[root@control01 tempest-16.0.1.dev358]<span class="hljs-comment"># pwd</span><br>/tempest-source/tempest-16.0.1.dev358<br>(tempest)[root@control01 tempest-16.0.1.dev358]<span class="hljs-comment">#</span><br></code></pre></td></tr></table></figure><h3 id="性能测试"><a href="#性能测试" class="headerlink" title="性能测试"></a>性能测试</h3><h4 id="Rally介绍"><a href="#Rally介绍" class="headerlink" title="Rally介绍"></a>Rally介绍</h4><p><img src="https://ljw.howieli.cn/blog/2017-9-8/rally.png"></p><p>详情请看官方文档：<a href="https://wiki.openstack.org/wiki/Rally">参考文档</a></p><h4 id="安装Rally"><a href="#安装Rally" class="headerlink" title="安装Rally"></a>安装Rally</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@control01 ~]<span class="hljs-comment"># vim /etc/kolla/globals.yml</span><br>enable_rally: <span class="hljs-string">&quot;yes&quot;</span><br>[root@control01 ~]<span class="hljs-comment"># kolla-ansible -i multinode deploy</span><br>[root@control01 ~]<span class="hljs-comment"># docker ps |grep rally</span><br>b239cb679134        172.18.22.162:4000/99cloud/centos-source-rally:animbus-5.2.0                       <span class="hljs-string">&quot;kolla_start&quot;</span>            8 days ago          Up 8 days                                    rally<br></code></pre></td></tr></table></figure><h4 id="Rally-配置"><a href="#Rally-配置" class="headerlink" title="Rally 配置"></a>Rally 配置</h4><h5 id="认证配置"><a href="#认证配置" class="headerlink" title="认证配置"></a>认证配置</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@control01 ~]<span class="hljs-comment"># docker cp /etc/kolla/admin-openrc.sh rally:/</span><br>[root@control01 ~]<span class="hljs-comment"># docker exec -it -u root rally bash</span><br>(rally)[root@control01 /]<span class="hljs-comment"># source admin-openrc.sh</span><br>(rally)[root@control01 /]<span class="hljs-comment"># openstack image list</span><br></code></pre></td></tr></table></figure><h5 id="创建deployment"><a href="#创建deployment" class="headerlink" title="创建deployment"></a>创建deployment</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash">(rally)[root@control01 /]<span class="hljs-comment"># rally deployment create --fromenv --name ljw</span><br>+--------------------------------------+---------------------+------+------------------+--------+<br>| uuid                                 | created_at          | name | status           | active |<br>+--------------------------------------+---------------------+------+------------------+--------+<br>| 66dea2c5-bef5-41ac-8a83-1d80f0d3755f | 2017-09-12T05:55:54 | ljw  | deploy-&gt;finished |        |<br>+--------------------------------------+---------------------+------+------------------+--------+<br></code></pre></td></tr></table></figure><h4 id="配置测试方案"><a href="#配置测试方案" class="headerlink" title="配置测试方案"></a>配置测试方案</h4><p>测试方案：</p><ul><li>使用image cirros创建10GB大小云硬盘</li><li>然后用此与硬盘、flavor m1.tiny创建虚拟机，并发是1，执行100次，使用20个租户，每个租户下面2个用户</li></ul><p>编辑测试脚本</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs bash">(rally)[root@control01 /]<span class="hljs-comment"># vi boot-from-volume.json</span><br>&#123;% <span class="hljs-built_in">set</span> flavor_name = flavor_name or <span class="hljs-string">&quot;m1.tiny&quot;</span> %&#125;<br>&#123;% <span class="hljs-built_in">set</span> volume_type = volume_type or <span class="hljs-string">&quot;&quot;</span> %&#125;<br>&#123;<br>    <span class="hljs-string">&quot;NovaServers.boot_server_from_volume&quot;</span>: [<br>        &#123;<br>            <span class="hljs-string">&quot;args&quot;</span>: &#123;<br>                <span class="hljs-string">&quot;flavor&quot;</span>: &#123;<br>                    <span class="hljs-string">&quot;name&quot;</span>: <span class="hljs-string">&quot;&#123;&#123;flavor_name&#125;&#125;&quot;</span><br>                &#125;,<br>                <span class="hljs-string">&quot;image&quot;</span>: &#123;<br>                    <span class="hljs-string">&quot;name&quot;</span>: <span class="hljs-string">&quot;^cirros&quot;</span><br>                &#125;,<br>                <span class="hljs-string">&quot;volume_size&quot;</span>: 10,<br>                <span class="hljs-string">&quot;volume_type&quot;</span>: <span class="hljs-string">&quot;&#123;&#123;volume_type&#125;&#125;&quot;</span><br>            &#125;,<br>            <span class="hljs-string">&quot;runner&quot;</span>: &#123;<br>                <span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;constant&quot;</span>,<br>                <span class="hljs-string">&quot;times&quot;</span>: 100,<br>                <span class="hljs-string">&quot;concurrency&quot;</span>: 1<br>            &#125;,<br>            <span class="hljs-string">&quot;context&quot;</span>: &#123;<br>                <span class="hljs-string">&quot;users&quot;</span>: &#123;<br>                    <span class="hljs-string">&quot;tenants&quot;</span>: 20,<br>                    <span class="hljs-string">&quot;users_per_tenant&quot;</span>: 2<br>                &#125;,<br>                <span class="hljs-string">&quot;quotas&quot;</span>: &#123;<br>                    <span class="hljs-string">&quot;nova&quot;</span>: &#123;<br>                        <span class="hljs-string">&quot;instances&quot;</span>: 20<br>                    &#125;<br>                &#125;<br>            &#125;<br>        &#125;<br>    ]<br>&#125;<br></code></pre></td></tr></table></figure><p>导出测试报告</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">(rally)[root@control01 /]<span class="hljs-comment"># rally task report b77ccde5-ceb7-444a-b9a9-75f0ff40b480 --out boot_server.html</span><br></code></pre></td></tr></table></figure><p>从容器中导出测试报告</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@control01 ~]<span class="hljs-comment"># docker cp rally:/boot_server.html .</span><br></code></pre></td></tr></table></figure><h4 id="分析测试报告"><a href="#分析测试报告" class="headerlink" title="分析测试报告"></a>分析测试报告</h4><p>可以用Google Chrome浏览器分析<br><img src="https://ljw.howieli.cn/blog/2017-9-8/rally%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.png"></p>]]></content>
    
    
    <categories>
      
      <category>openstack</category>
      
    </categories>
    
    
    <tags>
      
      <tag>tempest</tag>
      
      <tag>rally</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>kolla搭建octavia</title>
    <link href="/posts/kolla%E6%90%AD%E5%BB%BAoctavia/"/>
    <url>/posts/kolla%E6%90%AD%E5%BB%BAoctavia/</url>
    
    <content type="html"><![CDATA[<h3 id="octavia-介绍"><a href="#octavia-介绍" class="headerlink" title="octavia 介绍"></a>octavia 介绍</h3><p>openstack octavia 是 openstack Lbaas项目分出来的一个子项目，是提供虚拟机流量负载均衡，其实oactavia就是调用nova以及neutron的api生成两台安装好了haproxy和keepalived软件的虚拟机，并连接到目标网络。octavia共有4个组件housekeeping,worker,api,health-manager。详情请参考 <a href="https://docs.openstack.org/octavia/latest/reference/introduction.html">octavia介绍</a></p><a id="more"></a><h3 id="octavia-架构图"><a href="#octavia-架构图" class="headerlink" title="octavia 架构图"></a>octavia 架构图</h3><p><img src="https://ljw.howieli.cn/blog/2017-9-8/octavia%E6%9E%B6%E6%9E%84%E5%9B%BE.png"></p><h3 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h3><p>我事先准备好了一个kolla部署的多节点的openstack环境</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> openstack service list</span><br>+----------------------------------+------------------+-------------------------+<br>| ID                               | Name             | Type                    |<br>+----------------------------------+------------------+-------------------------+<br>| 11bb0c086a484e33bf65558298a13f2d | manila           | share                   |<br>| 126fb7d3b45f44008f9b5649e7fa6772 | glance           | image                   |<br>| 1963c13ceda74eaa846ce53447aa9663 | swift            | object-store            |<br>| 1d73b2481e0e4c2996e902282f53ac94 | ceilometer       | metering                |<br>| 277ac4e4d36b4170a13dff885133d497 | cinderv3         | volumev3                |<br>| 313e8abeeb5f46968e21c47be64fe37a | panko            | event                   |<br>| 3546da190ebe4cb69a5e4054c1b226e8 | ironic-inspector | baremetal-introspection |<br>| 3559efb2c60c4532b0e20f4280d00d65 | gnocchi          | metric                  |<br>| 38fa732db46a4bf99a656cc4b29f9955 | trove            | database                |<br>| 5003dabefad4462f80758eecc883a185 | sahara           | data_processing         |<br>| 5b3ab9520859475fb874e570be8ea064 | manilav2         | sharev2                 |<br>| 659cd19c052b452a8a954bbadfd94a5a | nova             | compute                 |<br>| 72fcd0fc324248bdb8c5f08f9c65a1ea | nova_legacy      | compute_legacy          |<br>| 754afe3e3b4349a0a54d26b7d83fec5e | cinder           | volume                  |<br>| 85c1d57869754c38989f228c08896d4c | ironic           | baremetal               |<br>| 9962758f42c140c4b811940bdc4041b2 | heat             | orchestration           |<br>| b7dc29ababc54104b31ad3b999eae6b5 | heat-cfn         | cloudformation          |<br>| bee93d265f994dd3bd9e76a326df0d1b | cinderv2         | volumev2                |<br>| d6f1862a19434267a4a51fd4c8a6d665 | neutron          | network                 |<br>| ee90e0f359d84c1eb35c3cd9423f43db | placement        | placement               |<br>| f7b5b57d52494869ab20501c28237613 | cloudkitty       | rating                  |<br>| fdb4e5b1658d4d39a1f807f9d7b2b4d7 | keystone         | identity                |<br>+----------------------------------+------------------+-------------------------+<br></code></pre></td></tr></table></figure><h3 id="创建证书"><a href="#创建证书" class="headerlink" title="创建证书"></a>创建证书</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> git <span class="hljs-built_in">clone</span> https://review.openstack.org/p/openstack/octavia</span><br><span class="hljs-meta">#</span><span class="bash"> <span class="hljs-built_in">cd</span> octavia</span><br><span class="hljs-meta">#</span><span class="bash"> grep octavia_ca /etc/kolla/passwords.yml</span><br>octavia_ca_password: mEUyBHLopKk501CX30WRnPuiDmoP3I7eNQIQbC6z<br><span class="hljs-meta">#</span><span class="bash"> sed -i <span class="hljs-string">&#x27;s/foobar/mEUyBHLopKk501CX30WRnPuiDmoP3I7eNQIQbC6z/g&#x27;</span> bin/create_certificates.sh</span><br><span class="hljs-meta">#</span><span class="bash"> ./bin/create_certificates.sh cert $(<span class="hljs-built_in">pwd</span>)/etc/certificates/openssl.cnf</span><br></code></pre></td></tr></table></figure><p>然后你会得到一个文件夹</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> ls -al cert/</span><br>total 48<br>drwxr-xr-x  4 root root  219 Sep  7 09:32 .<br>drwxr-xr-x 17 root root 4096 Sep  7 09:32 ..<br>-rw-r--r--  1 root root 1294 Sep  7 09:32 ca_01.pem<br>-rw-r--r--  1 root root  989 Sep  7 09:32 client.csr<br>-rw-r--r--  1 root root 1708 Sep  7 09:32 client.key<br>-rw-r--r--  1 root root 4401 Sep  7 09:32 client-.pem<br>-rw-r--r--  1 root root 6109 Sep  7 09:32 client.pem<br>-rw-r--r--  1 root root   71 Sep  7 09:32 index.txt<br>-rw-r--r--  1 root root   21 Sep  7 09:32 index.txt.attr<br>-rw-r--r--  1 root root    0 Sep  7 09:32 index.txt.old<br>drwxr-xr-x  2 root root   20 Sep  7 09:32 newcerts<br>drwx------  2 root root   23 Sep  7 09:32 private<br>-rw-r--r--  1 root root    3 Sep  7 09:32 serial<br>-rw-r--r--  1 root root    3 Sep  7 09:32 serial.old<br></code></pre></td></tr></table></figure><p>将认证放到kolla部署节点上的/etc/kolla/octavia目录里，首先要先在/etc/kolla目录下创建一个octavia文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> mkdir /etc/kolla/config/octavia</span><br><span class="hljs-meta">#</span><span class="bash"> cp certs/&#123;private/cakey.pem,ca_01.pem,client.pem&#125; /etc/kolla/config/ocatava/</span><br><span class="hljs-meta">#</span><span class="bash"> ls -al /etc/kolla/config/octavia/</span><br>total 20<br>drwxr-xr-x 2 root root   58 Sep  8 12:37 .<br>drwxr-xr-x 5 root root 4096 Sep  8 12:36 ..<br>-rw-r--r-- 1 root root 1294 Sep  8 12:37 ca_01.pem<br>-rw-r--r-- 1 root root 1743 Sep  8 12:37 cakey.pem<br>-rw-r--r-- 1 root root 6109 Sep  8 12:37 client.pem<br></code></pre></td></tr></table></figure><h3 id="构建镜像"><a href="#构建镜像" class="headerlink" title="构建镜像"></a>构建镜像</h3><figure class="highlight vala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs vala"><span class="hljs-meta"># pwd</span><br>/root<br><span class="hljs-meta"># cd octavia/diskimage-create/</span><br><span class="hljs-meta"># ./diskimage-create.sh -i centos</span><br></code></pre></td></tr></table></figure><p>会产生amphora-x64-haproxy.qcow2镜像</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> ls amphora-x64-haproxy.qcow2</span><br>amphora-x64-haproxy.qcow2<br></code></pre></td></tr></table></figure><p>上传镜像，因为我kolla部署openstack时候后端存储用的ceph，所以首先要把镜像转换成raw格式,然后在上传镜像，上传镜像必须给镜像打一个amphora的tag。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> qemu-img convert -f qcow2 -O raw amphora-x64-haproxy.qcow2 amphora-x64-haproxy.raw</span><br><span class="hljs-meta">#</span><span class="bash"> qemu-img info amphora-x64-haproxy.raw</span><br>image: amphora-x64-haproxy.raw<br>file format: raw<br>virtual size: 2.0G (2147483648 bytes)<br>disk size: 1.5G<br><span class="hljs-meta">#</span><span class="bash"> openstack image create --container-format bare --disk-format raw --private --file amphora-x64-haproxy.raw --tag amphora amphora</span><br></code></pre></td></tr></table></figure><h3 id="准备openstack资源"><a href="#准备openstack资源" class="headerlink" title="准备openstack资源"></a>准备openstack资源</h3><p>octavia_amp_boot_network_list 网络,这里需要准备分给octavia用的网络ID,而且这个网络必须能和api互通</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> openstack network create --provider-physical-network physnet1 \</span><br><span class="bash">    --provider-network-type vlan --provider-segment 121 lb-net</span><br><span class="hljs-meta">#</span><span class="bash"> openstack subnet create --allocation-pool --network lb-net --subnet-range 10.140.1.0/24 \</span><br><span class="bash">    lb-subnet</span><br></code></pre></td></tr></table></figure><p>还需要octavia_amp_flavor_id 值,这是octavia创建的虚拟机的大小。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> openstack flavor create --disk 40 --private --ram 4096 --vcpus 2 octavia_flavor</span><br></code></pre></td></tr></table></figure><p>创建一个octavia的安全组，安全组的id值为octavia_amp_secgroup_list 值</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> openstack security group create --description <span class="hljs-string">&#x27;used by Octavia amphora instance&#x27;</span> octavia</span><br></code></pre></td></tr></table></figure><p>为octavia安全组设置规则</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> openstack security group rule create --protocol icmp f97161c4-ffda-4205-b984-d3ecad0f9ce1</span><br><span class="hljs-meta">#</span><span class="bash"> openstack security group rule create --protocol tcp --dst-port 5555 --egress f97161c4-ffda-4205-b984-d3ecad0f9ce1</span><br><span class="hljs-meta">#</span><span class="bash"> openstack security group rule create --protocol tcp --dst-port 9443 --ingress f97161c4-ffda-4205-b984-d3ecad0f9ce1</span><br></code></pre></td></tr></table></figure><p>octavia 机器启动时注入的  key, octavia_ssh_key 名字是固定的，需要和 octavia.conf 配置文件相同。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> openstack keypair create --public-key /root/.ssh/id_rsa.pub octavia_ssh_key</span><br><span class="hljs-meta">#</span><span class="bash"> 我们也可以使用 octavia 账号去建虚拟机，所以 ssh key 要使用 octavia 账号创建。</span><br><span class="hljs-meta">#</span><span class="bash"> openstack --os-username octavia --os-password &lt;octavia_keystone_password&gt; keypair create --public-key /tmp/id_rsa.pub octavia_ssh_key</span><br></code></pre></td></tr></table></figure><h3 id="更改globas-yml文件"><a href="#更改globas-yml文件" class="headerlink" title="更改globas.yml文件"></a>更改globas.yml文件</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> vim /etc/kolla/globals.yml</span><br>enable_horizon_neutron_lbaas: &quot;yes&quot;<br>enable_neutron_lbaas: &quot;yes&quot;<br>enable_octavia: yes<br><span class="hljs-meta">#</span><span class="bash"> Load balancer topology options are [ SINGLE, ACTIVE_STANDBY ]</span><br>octavia_loadbalancer_topology: &quot;ACTIVE_STANDBY&quot;<br>octavia_amp_boot_network_list: ffc00802-c88b-4a5e-a51b-60071f4045d5<br>octavia_amp_secgroup_list: f97161c4-ffda-4205-b984-d3ecad0f9ce1<br>octavia_amp_flavor_id: 54f2cc50-81d4-44d3-8e44-a01686e4b2c6<br></code></pre></td></tr></table></figure><h3 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> kolla-ansible deploy -i multinode</span><br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>openstack</category>
      
    </categories>
    
    
    <tags>
      
      <tag>octavia</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ironic裸机服务使用说明</title>
    <link href="/posts/ironic%E8%A3%B8%E6%9C%BA%E6%9C%8D%E5%8A%A1%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E/"/>
    <url>/posts/ironic%E8%A3%B8%E6%9C%BA%E6%9C%8D%E5%8A%A1%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E/</url>
    
    <content type="html"><![CDATA[<h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>“裸机功能使用说明”介绍了裸机配置中的一些技术细节，旨在帮助用户更好地了解和使用 ironic。</p><a id="more"></a><h3 id="whole-disk镜像和partition镜像区别"><a href="#whole-disk镜像和partition镜像区别" class="headerlink" title="whole disk镜像和partition镜像区别"></a>whole disk镜像和partition镜像区别</h3><hr><p>partition image部署可以支持windows系统，但是不支持自定义分区，因为分区表在制作镜像时已经被确定下来。但partition image支持按照配置中规定的root,swap分区来自动进行磁盘分区。</p><p>partition image方式部署的机器将kernel和ramdisk都存放在tftp server上，每次启动都需要访问tftp,whole disk部署的服务器启动时不需要访问tftp server。</p><hr><h3 id="noop，neutron和flat简介"><a href="#noop，neutron和flat简介" class="headerlink" title="noop，neutron和flat简介"></a>noop，neutron和flat简介</h3><hr><p>noop ：用于stand alone(不和OpenStack集成的模式)部署用的网络模式，在部署过程不进行任何的网络切换。</p><p>neutron ：通过与provide的网络集成来提供租户定义的网络，同时还将租户网络与provision network和cleaning network网络分开。</p><p>flat ：用于提供外网访问和部署的二层网络,provision network和cleaning network网络可以放在一起，也可以分开。</p><h3 id="IPMI带内（in-band）信道和带外（out-of-band）信道"><a href="#IPMI带内（in-band）信道和带外（out-of-band）信道" class="headerlink" title="IPMI带内（in band）信道和带外（out of band）信道"></a>IPMI带内（in band）信道和带外（out of band）信道</h3><hr><p>IPMI 信道分为带内信道和带外信道两种类型，但配置裸机并不是按照带内和带外信道的方式来划分驱动。</p><p>带内和带外的区别主要体现在不同行为的的管理方式上,例如检查时，通过agent发现硬件属性就是带内的管理方式，通过ilo rest接口查询就是带外的方式；部署时，通过iscsi部署是带外的部署方式，通过agent部署是带内的部署方式。</p><hr><h3 id="裸机配置状态介绍"><a href="#裸机配置状态介绍" class="headerlink" title="裸机配置状态介绍"></a>裸机配置状态介绍</h3><hr><p><img src="https://ljw.howieli.cn/blog/2017-9-8/ironic.png"><br>注册（Enroll)：所有节点的初始状态，代表节点已经存在，但在这种状态下无法对节点进行下一步操作。</p><p>当获取到节点信息以及驱动详情后，可通过调用manage API，使节点过渡到验证状态。</p><p>验证（Verifying)：ironic会对节点的驱动和证书进行验证，验证成功后节点的配置状态将过渡到可管理状态。</p><p>可管理（Manageable)：此时，可对节点进行管理（包括电源状态等）。此时节点可以：</p><pre><code>从清理中（Cleaning）到可管理，通过调用clean API；从检查中（Inspecting）到可管理，通过调用inspect API；通过清理中（Cleaning）到Available，通过调用provide API。</code></pre><p>检查（Inspecting)：检查过程将调用节点的自我检查功能来更新节点的硬件属性。如果检查失败配置状态将过渡到检查失败（inspectfail）。</p><p>清理中（Cleaning):节点通过清理确保自身能够成功过渡到可用状态（Available）。清理过程中的任务包括：</p><pre><code>清除驱动器验证硬件完整性验证节点中的配置信息和物理硬件的配置是否一致。启动Ramdisk内核</code></pre><p>当节点处于清理状态，如果使用带外信道（out of band)则控制器会进入清理步骤，如果是带内信道（in band)方式，控制器会开始为安装磁盘做环境准备。</p><p>可用（Available):处于可用状态下的节点已经进行过清理和预配置过程，在可用状态下：</p><pre><code>通过调用active API过渡到活跃状态（Active)；通过调用manage API过渡到可管理状态(Manageable)</code></pre><p>配置中（Deploying):处于配置中的节点，正在为裸机的创建作积极准备。这一过程中会运行一系列短暂任务，如：</p><pre><code>配置BIOS；部署分区、安装文件系统驱动；创建额外的资源（如：网络配置）以及其他附加</code></pre><p>的子系统要求；</p><p>活跃（Active):节点上有裸机在运行，此时节点可：</p><pre><code>通过调用rescue API过渡到维护（Rescue）状态；通过调用delete API过渡到可用状态（Available)；通过调用rebuild API过渡到活跃状态（Active).</code></pre><p>删除中（Deleting)：在删除状态下，部署在节点上的裸机会删除。</p><hr>]]></content>
    
    
    <categories>
      
      <category>openstack</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ironic</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>kolla搭建ironic裸机服务</title>
    <link href="/posts/kolla%E6%90%AD%E5%BB%BAironic%E8%A3%B8%E6%9C%BA%E6%9C%8D%E5%8A%A1/"/>
    <url>/posts/kolla%E6%90%AD%E5%BB%BAironic%E8%A3%B8%E6%9C%BA%E6%9C%8D%E5%8A%A1/</url>
    
    <content type="html"><![CDATA[<h3 id="内容说明"><a href="#内容说明" class="headerlink" title="内容说明"></a>内容说明</h3><p>  本文介绍在kolla 管理虚拟机配置流程，包括部署ironic和 裸机管理配置两部分，实现在flat 网络下管理裸机。</p><a id="more"></a><h3 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h3><p>在kolla 环境上增加裸机管理功能。</p><p>由于ironic 在管理裸机时需要一个网络来管理裸机的部署和清理。所以需要在部署ironic 前先创建一个flat类型的provision network 网络。本文档业务网络和provision network 网络共用一个。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@control01 ~]# source /etc/kolla/admin-openrc.sh<br>[root@control01 ~]# openstack network create --provider-physical-network physnet1 \<br>    --provider-network-type flat provision-net<br>[root@control01 ~]# openstack subnet create --allocation-pool start=172.18.23.1,end=172.18.23.199 \<br>    --network provision-net --subnet-range 172.18.23.0.0/24 \<br>    --gateway 172.18.151.1 provision-subnet<br></code></pre></td></tr></table></figure><h3 id="部署ironic"><a href="#部署ironic" class="headerlink" title="部署ironic"></a>部署ironic</h3><h4 id="修改globals-yml文件"><a href="#修改globals-yml文件" class="headerlink" title="修改globals.yml文件"></a>修改globals.yml文件</h4><p>ironic_cleaning_network 是上面创建的provision network网络ID。 ironic_dnsmasq_dhcp_range 可以随便定义一个，并不会用到，是为了保证ironic_dnsmasq 容器启动正常。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@control01 ~]# vim /etc/kolla/globals.yml<br>ironic_cleaning_network: &quot;f157770d-e81c-41a3-acf1-8fc3f4ea7f31&quot;<br>ironic_dnsmasq_dhcp_range: &quot;172.18.22.210,172.18.22.219&quot;<br>enable_horizon_ironic: &quot;&#123;&#123; enable_ironic | bool &#125;&#125;&quot;<br>enable_ironic: &quot;yes&quot;<br>enable_nova_serialconsole_proxy: &quot;yes&quot;<br></code></pre></td></tr></table></figure><h4 id="准备ironic-agent镜像"><a href="#准备ironic-agent镜像" class="headerlink" title="准备ironic-agent镜像"></a>准备ironic-agent镜像</h4><p>ironic-agent 镜像是在联网环境下通过diskimage-builder 生成, 参考官方文档 <a href="https://docs.openstack.org/ironic/latest/install/deploy-ramdisk.html#disk-image-builder">disk-image-builder</a>。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@control01 ~]# pip install diskimage-builder<br>[root@control01 ~]# disk-image-create ironic-agent fedora -o ironic-agent<br></code></pre></td></tr></table></figure><p>上传镜像</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@control01 ~]# source /etc/kolla/admin-openrc.sh<br>[root@control01 ~]# glance image-create --name deploy-vmlinuz \<br>    --visibility public  --disk-format aki \<br>    --container-format aki --file ./ironic-agent.vmlinuz<br>[root@control01 ~]# glance image-create --name deploy-initrd \<br>    --visibility public --disk-format ari \<br>    --container-format ari --file ./ironic-agent.initramfs<br></code></pre></td></tr></table></figure><p>把ironic-agent.initramfs ironic-agent.kernel放到/etc/kolla/config/ironic目录下。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@control01 ~]# cd /etc/kolla/config/ironic<br>[root@control01 ~]# ls<br>ironic-agent.initramfs ironic-agent.kernel<br></code></pre></td></tr></table></figure><h4 id="配置ironic-conf"><a href="#配置ironic-conf" class="headerlink" title="配置ironic.conf"></a>配置ironic.conf</h4><p>配置drivers 为支持远程console 的pxe_ipmitool_socat </p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@control01 ~]# cat /etc/kolla/config/ironic.conf<br>[DEFAULT]<br>enabled_drivers = pxe_ipmitool_socat,pxe_ipmitool<br>enabled_hardware_types = ipmi<br>enabled_network_interfaces=noop,flat<br>enabled_power_interfaces = ipmitool<br>enabled_management_interfaces = ipmitool<br><br>[neutron]<br><span class="hljs-meta">#</span><span class="bash">After deploy the OpenStack, Update the cleaning_network</span><br><span class="hljs-meta">#</span><span class="bash">cleaning_network = externel</span><br><br>[conductor]<br>sync_power_state_interval = 10<br>power_state_sync_max_retries = 20<br><span class="hljs-meta">#</span><span class="bash">power_state_change_timeout = 30</span><br>deploy_callback_timeout = 900<br>force_power_state_during_sync = true<br><br>[pxe]<br><span class="hljs-meta">#</span><span class="bash">uefi_pxe_bootfile_name = bootx64.efi</span><br><span class="hljs-meta">#</span><span class="bash">After deploy the OpenStack, Update the tftp_server</span><br><span class="hljs-meta">#</span><span class="bash">tftp_server =</span><br><span class="hljs-meta">#</span><span class="bash">pxe_append_params = coreos.autologin rd.auto=1 nofb nomodeset vga=normal console=ttyS0,115200n8</span><br>pxe_append_params = nofb nomodeset vga=normal console=tty0 console=ttyS0,115200n8<br><br>[disk_utils]<br><span class="hljs-meta">#</span><span class="bash">default interval is 1 second</span><br>iscsi_verify_attempts = 30<br><br><span class="hljs-meta">#</span><span class="bash"> cat /etc/kolla/config/ironic.conf</span><br><br>[DEFAULT]<br>enabled_drivers = pxe_ipmitool_socat<br>enabled_network_interfaces=noop,flat<br><br>[pxe]<br>pxe_append_params = nofb nomodeset vga=normal console=tty0 console=ttyS0,115200n8<br></code></pre></td></tr></table></figure><p>或</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs shell">cat /etc/kolla/config/ironic.conf<br>[DEFAULT]<br>enabled_drivers = pxe_ipmitool_socat<br>enabled_hardware_types = redfish<br>enabled_power_interfaces = ipmitool,redfish<br>enabled_management_interfaces = ipmitool,redfish<br>default_network_interface = flat<br><br>[neutron]<br><span class="hljs-meta">#</span><span class="bash">After deploy the OpenStack, Update the cleaning_network</span><br><span class="hljs-meta">#</span><span class="bash">cleaning_network = externel</span><br><br>[conductor]<br>sync_power_state_interval = 10<br>power_state_sync_max_retries = 20<br><span class="hljs-meta">#</span><span class="bash">power_state_change_timeout = 30</span><br>deploy_callback_timeout = 900<br>force_power_state_during_sync = true<br><br>[pxe]<br>pxe_append_params = coreos.autologin rd.auto=1 nofb nomodeset vga=normal console=ttyS0,115200n8<br><br>[disk_utils]<br><span class="hljs-meta">#</span><span class="bash">default interval is 1 second</span><br>iscsi_verify_attempts = 30<br></code></pre></td></tr></table></figure><h3 id="检查部署"><a href="#检查部署" class="headerlink" title="检查部署"></a>检查部署</h3><h4 id="部署检查"><a href="#部署检查" class="headerlink" title="部署检查"></a>部署检查</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@control01 ~]# kolla-ansible -i multinode prechecks<br></code></pre></td></tr></table></figure><h4 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@control01 ~]# kolla-ansible -i multinode deploy<br></code></pre></td></tr></table></figure><h4 id="部署后检查"><a href="#部署后检查" class="headerlink" title="部署后检查"></a>部署后检查</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@control01 ~]# ironic driver-list<br>+---------------------+---------------------------------+<br>| Supported driver(s) | Active host(s)                  |<br>+---------------------+---------------------------------+<br>| ipmi                | control02, control03, control01 |<br>| pxe_ipmitool        | control02, control03, control01 |<br>| pxe_ipmitool_socat  | control02, control03, control01 |<br>+---------------------+---------------------------------+<br>[root@control01 ~]# nova service-list<br>+-----+------------------+------------------+----------+---------+-------+----------------------------+-----------------+<br>| Id  | Binary           | Host             | Zone     | Status  | State | Updated_at                 | Disabled Reason |<br>+-----+------------------+------------------+----------+---------+-------+----------------------------+-----------------+<br>| 4   | nova-scheduler   | control02        | internal | enabled | up    | 2017-08-31T01:48:41.000000 | -               |<br>| 10  | nova-scheduler   | control01        | internal | enabled | up    | 2017-08-31T01:48:37.000000 | -               |<br>| 22  | nova-conductor   | control02        | internal | enabled | up    | 2017-08-31T01:48:40.000000 | -               |<br>| 31  | nova-conductor   | control01        | internal | enabled | up    | 2017-08-31T01:48:39.000000 | -               |<br>| 61  | nova-consoleauth | control02        | internal | enabled | up    | 2017-08-31T01:48:36.000000 | -               |<br>| 64  | nova-consoleauth | control01        | internal | enabled | up    | 2017-08-31T01:48:42.000000 | -               |<br>| 67  | nova-compute     | control02        | nova     | enabled | up    | 2017-08-31T01:48:40.000000 | -               |<br>| 70  | nova-compute     | control01        | nova     | enabled | up    | 2017-08-31T01:48:36.000000 | -               |<br>| 76  | nova-scheduler   | control03        | internal | enabled | up    | 2017-08-31T01:48:42.000000 | -               |<br>| 82  | nova-conductor   | control03        | internal | enabled | up    | 2017-08-31T01:48:36.000000 | -               |<br>| 91  | nova-consoleauth | control03        | internal | enabled | up    | 2017-08-31T01:48:43.000000 | -               |<br>| 94  | nova-compute     | control03        | nova     | enabled | up    | 2017-08-31T01:48:44.000000 | -               |<br>| 97  | nova-compute     | control03-ironic | nova     | enabled | up    | 2017-08-31T01:48:44.000000 | -               |<br>| 100 | nova-compute     | control02-ironic | nova     | enabled | up    | 2017-08-31T01:48:42.000000 | -               |<br>| 103 | nova-compute     | control01-ironic | nova     | enabled | up    | 2017-08-31T01:48:43.000000 | -               |<br>+-----+------------------+------------------+----------+---------+-------+----------------------------+-----------------+<br></code></pre></td></tr></table></figure><h3 id="使用ironic-管理物理服务器"><a href="#使用ironic-管理物理服务器" class="headerlink" title="使用ironic 管理物理服务器"></a>使用ironic 管理物理服务器</h3><p>每一台需要管理的裸机，都要创建为一个ironic node ，包括ipmi、服务器配置等信息。 再为ironic node 配置管理网卡的信息。</p><h4 id="创建ironic-node"><a href="#创建ironic-node" class="headerlink" title="创建ironic node"></a>创建ironic node</h4><p>创建ironic node,配置网络接口为flat</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@control01 ~]# export IRONIC_API_VERSION=1.20<br>[root@control01 ~]# ironic node-create -d pxe_ipmitool_socat --network-interface flat -n node1<br>+-------------------+--------------------------------------+<br>| Property          | Value                                |<br>+-------------------+--------------------------------------+<br>| chassis_uuid      | None                                 |<br>| driver            | pxe_ipmitool_socat                   |<br>| driver_info       | &#123;&#125;                                   |<br>| extra             | &#123;&#125;                                   |<br>| name              | node1                                |<br>| network_interface | flat                                 |<br>| properties        | &#123;&#125;                                   |<br>| resource_class    |                                      |<br>| uuid              | c78f0257-bfbf-489c-b525-a39e0b924b79 |<br>+-------------------+--------------------------------------+<br></code></pre></td></tr></table></figure><h4 id="配置ironic-node-信息"><a href="#配置ironic-node-信息" class="headerlink" title="配置ironic node 信息"></a>配置ironic node 信息</h4><h5 id="配置ipmi信息"><a href="#配置ipmi信息" class="headerlink" title="配置ipmi信息"></a>配置ipmi信息</h5><p>ipmi_username,ipmi_password,ipmi_address 为裸机ipmi信息。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@control01 ~]# NODE_UUID=$(ironic node-list|grep node1|awk -F &quot;| &quot; &#x27;&#123;print $2&#125;&#x27;)<br>[root@control01 ~]#  ironic node-update $NODE_UUID add     driver_info/ipmi_username=admin   \<br><span class="hljs-meta">&gt;</span><span class="bash">    driver_info/ipmi_password=admin     driver_info/ipmi_address=172.18.21.164 \</span><br><span class="bash">&gt;    driver_info/ipmi_terminal_port=8901</span><br>+------------------------+------------------------------------------------------------------+<br>| Property               | Value                                                            |<br>+------------------------+------------------------------------------------------------------+<br>| chassis_uuid           | None                                                             |<br>| clean_step             | &#123;&#125;                                                               |<br>| console_enabled        | False                                                            |<br>| created_at             | 2017-08-31T01:50:09+00:00                                        |<br>| driver                 | pxe_ipmitool_socat                                               |<br>| driver_info            | &#123;u&#x27;ipmi_terminal_port&#x27;: 8901, u&#x27;ipmi_address&#x27;: u&#x27;172.18.21.164&#x27;, |<br>|                        | u&#x27;ipmi_username&#x27;: u&#x27;admin&#x27;, u&#x27;ipmi_password&#x27;: u&#x27;******&#x27;&#125;         |<br>| driver_internal_info   | &#123;&#125;                                                               |<br>| extra                  | &#123;&#125;                                                               |<br>| inspection_finished_at | None                                                             |<br>| inspection_started_at  | None                                                             |<br>| instance_info          | &#123;&#125;                                                               |<br>| instance_uuid          | None                                                             |<br>| last_error             | None                                                             |<br>| maintenance            | False                                                            |<br>| maintenance_reason     | None                                                             |<br>| name                   | node1                                                            |<br>| network_interface      | flat                                                             |<br>| power_state            | None                                                             |<br>| properties             | &#123;&#125;                                                               |<br>| provision_state        | enroll                                                           |<br>| provision_updated_at   | None                                                             |<br>| raid_config            | &#123;&#125;                                                               |<br>| reservation            | control02                                                        |<br>| resource_class         |                                                                  |<br>| target_power_state     | None                                                             |<br>| target_provision_state | None                                                             |<br>| target_raid_config     | &#123;&#125;                                                               |<br>| updated_at             | 2017-08-31T01:51:40+00:00                                        |<br>| uuid                   | c78f0257-bfbf-489c-b525-a39e0b924b79                             |<br>+------------------------+------------------------------------------------------------------+<br></code></pre></td></tr></table></figure><h4 id="配置ramdisk-kernel信息"><a href="#配置ramdisk-kernel信息" class="headerlink" title="配置ramdisk ,kernel信息"></a>配置ramdisk ,kernel信息</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@control01 ~]# DEPLOY_VMLINUZ_UUID=$(glance image-list|grep deploy-vmlinuz|awk -F &quot;| &quot; &#x27;&#123;print $2&#125;&#x27;)<br>[root@control01 ~]# DEPLOY_INITRD_UUID=$(glance image-list|grep deploy-initrd|awk -F &quot;| &quot; &#x27;&#123;print $2&#125;&#x27;)<br>[root@control01 ~]# ironic node-update $NODE_UUID add     driver_info/deploy_kernel=$DEPLOY_VMLINUZ_UUID \<br><span class="hljs-meta">&gt;</span><span class="bash">     driver_info/deploy_ramdisk=<span class="hljs-variable">$DEPLOY_INITRD_UUID</span></span><br>+------------------------+-----------------------------------------------------------------------+<br>| Property               | Value                                                                 |<br>+------------------------+-----------------------------------------------------------------------+<br>| chassis_uuid           | None                                                                  |<br>| clean_step             | &#123;&#125;                                                                    |<br>| console_enabled        | False                                                                 |<br>| created_at             | 2017-08-31T01:50:09+00:00                                             |<br>| driver                 | pxe_ipmitool_socat                                                    |<br>| driver_info            | &#123;u&#x27;ipmi_terminal_port&#x27;: 8901, u&#x27;ipmi_username&#x27;: u&#x27;admin&#x27;,             |<br>|                        | u&#x27;deploy_kernel&#x27;: u&#x27;99a5c143-e231-4018-a6bf-121585eb6290&#x27;,            |<br>|                        | u&#x27;ipmi_address&#x27;: u&#x27;172.18.21.164&#x27;, u&#x27;deploy_ramdisk&#x27;:                 |<br>|                        | u&#x27;8c23e3e4-e749-4a91-9af7-63674b5f5128&#x27;, u&#x27;ipmi_password&#x27;: u&#x27;******&#x27;&#125; |<br>| driver_internal_info   | &#123;&#125;                                                                    |<br>| extra                  | &#123;&#125;                                                                    |<br>| inspection_finished_at | None                                                                  |<br>| inspection_started_at  | None                                                                  |<br>| instance_info          | &#123;&#125;                                                                    |<br>| instance_uuid          | None                                                                  |<br>| last_error             | None                                                                  |<br>| maintenance            | False                                                                 |<br>| maintenance_reason     | None                                                                  |<br>| name                   | node1                                                                 |<br>| network_interface      | flat                                                                  |<br>| power_state            | None                                                                  |<br>| properties             | &#123;&#125;                                                                    |<br>| provision_state        | enroll                                                                |<br>| provision_updated_at   | None                                                                  |<br>| raid_config            | &#123;&#125;                                                                    |<br>| reservation            | control02                                                             |<br>| resource_class         |                                                                       |<br>| target_power_state     | None                                                                  |<br>| target_provision_state | None                                                                  |<br>| target_raid_config     | &#123;&#125;                                                                    |<br>| updated_at             | 2017-08-31T01:52:16+00:00                                             |<br>| uuid                   | c78f0257-bfbf-489c-b525-a39e0b924b79                                  |<br>+------------------------+-----------------------------------------------------------------------+<br></code></pre></td></tr></table></figure><h5 id="配置服务器硬件信息"><a href="#配置服务器硬件信息" class="headerlink" title="配置服务器硬件信息"></a>配置服务器硬件信息</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@control01 ~]# ironic node-update $NODE_UUID add  properties/cpus=36  properties/memory_mb=129024 \<br><span class="hljs-meta">&gt;</span><span class="bash">     properties/local_gb=120     properties/cpu_arch=x86_64</span><br>+------------------------+-----------------------------------------------------------------------+<br>| Property               | Value                                                                 |<br>+------------------------+-----------------------------------------------------------------------+<br>| chassis_uuid           | None                                                                  |<br>| clean_step             | &#123;&#125;                                                                    |<br>| console_enabled        | False                                                                 |<br>| created_at             | 2017-08-31T01:50:09+00:00                                             |<br>| driver                 | pxe_ipmitool_socat                                                    |<br>| driver_info            | &#123;u&#x27;ipmi_terminal_port&#x27;: 8901, u&#x27;ipmi_username&#x27;: u&#x27;admin&#x27;,             |<br>|                        | u&#x27;deploy_kernel&#x27;: u&#x27;99a5c143-e231-4018-a6bf-121585eb6290&#x27;,            |<br>|                        | u&#x27;ipmi_address&#x27;: u&#x27;172.18.21.164&#x27;, u&#x27;deploy_ramdisk&#x27;:                 |<br>|                        | u&#x27;8c23e3e4-e749-4a91-9af7-63674b5f5128&#x27;, u&#x27;ipmi_password&#x27;: u&#x27;******&#x27;&#125; |<br>| driver_internal_info   | &#123;&#125;                                                                    |<br>| extra                  | &#123;&#125;                                                                    |<br>| inspection_finished_at | None                                                                  |<br>| inspection_started_at  | None                                                                  |<br>| instance_info          | &#123;&#125;                                                                    |<br>| instance_uuid          | None                                                                  |<br>| last_error             | None                                                                  |<br>| maintenance            | False                                                                 |<br>| maintenance_reason     | None                                                                  |<br>| name                   | node1                                                                 |<br>| network_interface      | flat                                                                  |<br>| power_state            | None                                                                  |<br>| properties             | &#123;u&#x27;memory_mb&#x27;: 129024, u&#x27;cpu_arch&#x27;: u&#x27;x86_64&#x27;, u&#x27;local_gb&#x27;: 120,      |<br>|                        | u&#x27;cpus&#x27;: 36&#125;                                                          |<br>| provision_state        | enroll                                                                |<br>| provision_updated_at   | None                                                                  |<br>| raid_config            | &#123;&#125;                                                                    |<br>| reservation            | control02                                                             |<br>| resource_class         |                                                                       |<br>| target_power_state     | None                                                                  |<br>| target_provision_state | None                                                                  |<br>| target_raid_config     | &#123;&#125;                                                                    |<br>| updated_at             | 2017-08-31T01:52:28+00:00                                             |<br>| uuid                   | c78f0257-bfbf-489c-b525-a39e0b924b79                                  |<br>+------------------------+-----------------------------------------------------------------------+<br></code></pre></td></tr></table></figure><h5 id="开启控制台"><a href="#开启控制台" class="headerlink" title="开启控制台"></a>开启控制台</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@control01 ~]#  ironic node-set-console-mode $NODE_UUID true<br></code></pre></td></tr></table></figure><h4 id="创建ironic-port"><a href="#创建ironic-port" class="headerlink" title="创建ironic port"></a>创建ironic port</h4><p>MAC_ADDRESS 为裸机业务网络网卡MAC地址, 可以登录裸机IPMI查询。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@control01 ~]# MAC_ADDRESS=08:9E:01:FC:48:7D<br>[root@control01 ~]#  ironic port-create -n $NODE_UUID -a $MAC_ADDRESS<br>+-----------------------+--------------------------------------+<br>| Property              | Value                                |<br>+-----------------------+--------------------------------------+<br>| address               | 08:9e:01:fc:48:7d                    |<br>| extra                 | &#123;&#125;                                   |<br>| local_link_connection | &#123;&#125;                                   |<br>| node_uuid             | c78f0257-bfbf-489c-b525-a39e0b924b79 |<br>| portgroup_uuid        |                                      |<br>| pxe_enabled           | True                                 |<br>| uuid                  | 40158d94-76c2-4893-b081-075729ea210f |<br>+-----------------------+--------------------------------------+<br></code></pre></td></tr></table></figure><h5 id="检查ironic-node"><a href="#检查ironic-node" class="headerlink" title="检查ironic node"></a>检查ironic node</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@control01 ~]# ironic node-list<br>+--------------------------------------+-------+---------------+-------------+--------------------+-------------+<br>| UUID                                 | Name  | Instance UUID | Power State | Provisioning State | Maintenance |<br>+--------------------------------------+-------+---------------+-------------+--------------------+-------------+<br>| c78f0257-bfbf-489c-b525-a39e0b924b79 | node1 | None          | None        | enroll             | False       |<br>+--------------------------------------+-------+---------------+-------------+--------------------+-------------+<br>[root@control01 ~]# ironic node-show c78f0257-bfbf-489c-b525-a39e0b924b79<br>+------------------------+--------------------------------------------------------------------------+<br>| Property               | Value                                                                    |<br>+------------------------+--------------------------------------------------------------------------+<br>| chassis_uuid           | None                                                                     |<br>| clean_step             | &#123;&#125;                                                                       |<br>| console_enabled        | False                                                                    |<br>| created_at             | 2017-08-31T01:50:09+00:00                                                |<br>| driver                 | pxe_ipmitool_socat                                                       |<br>| driver_info            | &#123;u&#x27;ipmi_terminal_port&#x27;: 8901, u&#x27;ipmi_username&#x27;: u&#x27;admin&#x27;,                |<br>|                        | u&#x27;deploy_kernel&#x27;: u&#x27;99a5c143-e231-4018-a6bf-121585eb6290&#x27;,               |<br>|                        | u&#x27;ipmi_address&#x27;: u&#x27;172.18.21.164&#x27;, u&#x27;deploy_ramdisk&#x27;:                    |<br>|                        | u&#x27;8c23e3e4-e749-4a91-9af7-63674b5f5128&#x27;, u&#x27;ipmi_password&#x27;: u&#x27;******&#x27;&#125;    |<br>| driver_internal_info   | &#123;&#125;                                                                       |<br>| extra                  | &#123;&#125;                                                                       |<br>| inspection_finished_at | None                                                                     |<br>| inspection_started_at  | None                                                                     |<br>| instance_info          | &#123;&#125;                                                                       |<br>| instance_uuid          | None                                                                     |<br>| last_error             | Error enabling the console on node c78f0257-bfbf-489c-b525-a39e0b924b79. |<br>|                        | Reason: Console subprocess failed to start. Command: socat -T600         |<br>|                        | -L/tmp/c78f0257-bfbf-489c-b525-a39e0b924b79.pid                          |<br>|                        | TCP4-LISTEN:8901,bind=172.18.22.161,reuseaddr EXEC:&quot;ipmitool -H          |<br>|                        | 172.18.21.164 -I lanplus -U admin -f /tmp/c78f0257-bfbf-                 |<br>|                        | 489c-b525-a39e0b924b79.pw -v sol activate&quot;,pty,stderr. Exit code: 1.     |<br>|                        | Stderr: &#x27;2017/08/31 09:52:42 socat[39] E bind(11, &#123;AF=2                  |<br>|                        | 172.18.22.161:8901&#125;, 16): Address already in use                         |<br>|                        | &#x27;                                                                        |<br>| maintenance            | False                                                                    |<br>| maintenance_reason     | None                                                                     |<br>| name                   | node1                                                                    |<br>| network_interface      | flat                                                                     |<br>| power_state            | None                                                                     |<br>| properties             | &#123;u&#x27;memory_mb&#x27;: 129024, u&#x27;cpu_arch&#x27;: u&#x27;x86_64&#x27;, u&#x27;local_gb&#x27;: 120,         |<br>|                        | u&#x27;cpus&#x27;: 36&#125;                                                             |<br>| provision_state        | enroll                                                                   |<br>| provision_updated_at   | None                                                                     |<br>| raid_config            | &#123;&#125;                                                                       |<br>| reservation            | None                                                                     |<br>| resource_class         |                                                                          |<br>| target_power_state     | None                                                                     |<br>| target_provision_state | None                                                                     |<br>| target_raid_config     | &#123;&#125;                                                                       |<br>| updated_at             | 2017-08-31T01:53:01+00:00                                                |<br>| uuid                   | c78f0257-bfbf-489c-b525-a39e0b924b79                                     |<br>+------------------------+--------------------------------------------------------------------------+<br></code></pre></td></tr></table></figure><h5 id="修改状态将enroll改为available"><a href="#修改状态将enroll改为available" class="headerlink" title="修改状态将enroll改为available"></a>修改状态将enroll改为available</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@control01 ~]# ironic --ironic-api-version 1.20 node-set-provision-state $NODE_UUID manage<br>[root@control01 ~]# ironic node-list<br>+--------------------------------------+-------+---------------+-------------+--------------------+-------------+<br>| UUID                                 | Name  | Instance UUID | Power State | Provisioning State | Maintenance |<br>+--------------------------------------+-------+---------------+-------------+--------------------+-------------+<br>| c78f0257-bfbf-489c-b525-a39e0b924b79 | node1 | None          | power off   | manageable         | False       |<br>+--------------------------------------+-------+---------------+-------------+--------------------+-------------+<br>[root@control01 ~]# ironic --ironic-api-version 1.20 node-set-provision-state $NODE_UUID provide<br>[root@control01 ~]# ironic node-list<br>+--------------------------------------+-------+---------------+-------------+--------------------+-------------+<br>| UUID                                 | Name  | Instance UUID | Power State | Provisioning State | Maintenance |<br>+--------------------------------------+-------+---------------+-------------+--------------------+-------------+<br>| c78f0257-bfbf-489c-b525-a39e0b924b79 | node1 | None          | power off   | available          | False       |<br>+--------------------------------------+-------+---------------+-------------+--------------------+-------------+<br></code></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@control01 ~]# ironic node-show c78f0257-bfbf-489c-b525-a39e0b924b79<br>+------------------------+-----------------------------------------------------------------------+<br>| Property               | Value                                                                 |<br>+------------------------+-----------------------------------------------------------------------+<br>| chassis_uuid           | None                                                                  |<br>| clean_step             | &#123;&#125;                                                                    |<br>| console_enabled        | False                                                                 |<br>| created_at             | 2017-08-31T01:50:09+00:00                                             |<br>| driver                 | pxe_ipmitool_socat                                                    |<br>| driver_info            | &#123;u&#x27;ipmi_terminal_port&#x27;: 8901, u&#x27;ipmi_username&#x27;: u&#x27;admin&#x27;,             |<br>|                        | u&#x27;deploy_kernel&#x27;: u&#x27;99a5c143-e231-4018-a6bf-121585eb6290&#x27;,            |<br>|                        | u&#x27;ipmi_address&#x27;: u&#x27;172.18.21.164&#x27;, u&#x27;deploy_ramdisk&#x27;:                 |<br>|                        | u&#x27;8c23e3e4-e749-4a91-9af7-63674b5f5128&#x27;, u&#x27;ipmi_password&#x27;: u&#x27;******&#x27;&#125; |<br>| driver_internal_info   | &#123;&#125;                                                                    |<br>| extra                  | &#123;&#125;                                                                    |<br>| inspection_finished_at | None                                                                  |<br>| inspection_started_at  | None                                                                  |<br>| instance_info          | &#123;&#125;                                                                    |<br>| instance_uuid          | None                                                                  |<br>| last_error             | None                                                                  |<br>| maintenance            | False                                                                 |<br>| maintenance_reason     | None                                                                  |<br>| name                   | node1                                                                 |<br>| network_interface      | flat                                                                  |<br>| power_state            | power off                                                             |<br>| properties             | &#123;u&#x27;memory_mb&#x27;: 129024, u&#x27;cpu_arch&#x27;: u&#x27;x86_64&#x27;, u&#x27;local_gb&#x27;: 120,      |<br>|                        | u&#x27;cpus&#x27;: 36&#125;                                                          |<br>| provision_state        | available                                                             |<br>| provision_updated_at   | 2017-08-31T01:56:55+00:00                                             |<br>| raid_config            | &#123;&#125;                                                                    |<br>| reservation            | None                                                                  |<br>| resource_class         |                                                                       |<br>| target_power_state     | None                                                                  |<br>| target_provision_state | None                                                                  |<br>| target_raid_config     | &#123;&#125;                                                                    |<br>| updated_at             | 2017-08-31T01:56:55+00:00                                             |<br>| uuid                   | c78f0257-bfbf-489c-b525-a39e0b924b79                                  |<br>+------------------------+-----------------------------------------------------------------------+<br></code></pre></td></tr></table></figure><h5 id="检查nova-hypervisor"><a href="#检查nova-hypervisor" class="headerlink" title="检查nova hypervisor"></a>检查nova hypervisor</h5><p>ironic node 添加完成后会新增一个nova hypervisor，通过nova hypervisor-show 可以看到裸机的vcpu,memory,disk 等配置信息。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@control01 ~]# nova hypervisor-list<br>+----+--------------------------------------+-------+---------+<br>| ID | Hypervisor hostname                  | State | Status  |<br>+----+--------------------------------------+-------+---------+<br>| 4  | control02                            | up    | enabled |<br>| 7  | control01                            | up    | enabled |<br>| 10 | control03                            | up    | enabled |<br>| 58 | c78f0257-bfbf-489c-b525-a39e0b924b79 | up    | enabled |<br>+----+--------------------------------------+-------+---------+<br>[root@control01 ~]# nova hypervisor-show 58<br>+-------------------------+--------------------------------------+<br>| Property                | Value                                |<br>+-------------------------+--------------------------------------+<br>| cpu_info                | &#123;&#125;                                   |<br>| current_workload        | 0                                    |<br>| disk_available_least    | 120                                  |<br>| free_disk_gb            | 120                                  |<br>| free_ram_mb             | 129024                               |<br>| host_ip                 | 172.18.22.162                        |<br>| hypervisor_hostname     | c78f0257-bfbf-489c-b525-a39e0b924b79 |<br>| hypervisor_type         | ironic                               |<br>| hypervisor_version      | 1                                    |<br>| id                      | 58                                   |<br>| local_gb                | 120                                  |<br>| local_gb_used           | 0                                    |<br>| memory_mb               | 129024                               |<br>| memory_mb_used          | 0                                    |<br>| running_vms             | 0                                    |<br>| service_disabled_reason | None                                 |<br>| service_host            | control01-ironic                     |<br>| service_id              | 103                                  |<br>| state                   | up                                   |<br>| status                  | enabled                              |<br>| vcpus                   | 36                                   |<br>| vcpus_used              | 0                                    |<br>+-------------------------+--------------------------------------+<br></code></pre></td></tr></table></figure><h3 id="创建裸机"><a href="#创建裸机" class="headerlink" title="创建裸机"></a>创建裸机</h3><p>由于目前horizon ironic 部分功能尚未完成，这里先以命令行形式创建虚拟机。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@control01 ~]# openstack server create --image 442f0935-33cc-450b-b641-9f438fdcb7ea --flavor 2327b74a-88b4-49f6-a0cd-9a06c13c801a --key-name mykey \<br><span class="hljs-meta">&gt;</span><span class="bash">     --nic net-id=f157770d-e81c-41a3-acf1-8fc3f4ea7f31 \</span><br><span class="bash">&gt;     --availability-zone nova:control01-ironic <span class="hljs-built_in">test</span></span><br>+-------------------------------------+------------------------------------------------+<br>| Field                               | Value                                          |<br>+-------------------------------------+------------------------------------------------+<br>| OS-DCF:diskConfig                   | MANUAL                                         |<br>| OS-EXT-AZ:availability_zone         | nova                                           |<br>| OS-EXT-SRV-ATTR:host                | None                                           |<br>| OS-EXT-SRV-ATTR:hypervisor_hostname | None                                           |<br>| OS-EXT-SRV-ATTR:instance_name       |                                                |<br>| OS-EXT-STS:power_state              | NOSTATE                                        |<br>| OS-EXT-STS:task_state               | scheduling                                     |<br>| OS-EXT-STS:vm_state                 | building                                       |<br>| OS-SRV-USG:launched_at              | None                                           |<br>| OS-SRV-USG:terminated_at            | None                                           |<br>| accessIPv4                          |                                                |<br>| accessIPv6                          |                                                |<br>| addresses                           |                                                |<br>| adminPass                           | YQS2x3r3AyYJ                                   |<br>| config_drive                        |                                                |<br>| created                             | 2017-08-31T02:01:31Z                           |<br>| flavor                              | ironic (2327b74a-88b4-49f6-a0cd-9a06c13c801a)  |<br>| hostId                              |                                                |<br>| id                                  | 54142f88-8170-4081-a2c0-b8da831a7dd1           |<br>| image                               | centos7 (442f0935-33cc-450b-b641-9f438fdcb7ea) |<br>| key_name                            | mykey                                          |<br>| name                                | test                                           |<br>| progress                            | 0                                              |<br>| project_id                          | 1345d1d9ddf34ee887fd1c17a4bb02b6               |<br>| properties                          |                                                |<br>| security_groups                     | name=&#x27;default&#x27;                                 |<br>| status                              | BUILD                                          |<br>| updated                             | 2017-08-31T02:01:31Z                           |<br>| user_id                             | 4b19a4a2173543888ba8460d54d5eebd               |<br>| volumes_attached                    |                                                |<br>+-------------------------------------+------------------------------------------------+<br></code></pre></td></tr></table></figure><h4 id="检查"><a href="#检查" class="headerlink" title="检查"></a>检查</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@control01 ~]# nova list<br>+--------------------------------------+----------------------------------------------+--------+------------+-------------+------------------------------+<br>| ID                                   | Name                                         | Status | Task State | Power State | Networks                     |<br>+--------------------------------------+----------------------------------------------+--------+------------+-------------+------------------------------+<br>| 7648bfcf-a2c5-4028-b080-5ab62b218c9a | hadoop-cluster-new-vanilla-default-master1-0 | ACTIVE | -          | Running     | net1=10.10.0.7, 172.18.23.17 |<br>| 970c02d3-0d7f-4e83-9866-be0ed25223a2 | hadoop-cluster-new-vanilla-default-worker1-0 | ACTIVE | -          | Running     | net1=10.10.0.10, 172.18.23.8 |<br>| e057db81-eeed-40a4-b36c-ecdad7ade378 | hadoop-cluster-new-vanilla-default-worker1-1 | ACTIVE | -          | Running     | net1=10.10.0.6, 172.18.23.28 |<br>| b36c5ee6-3b97-4e03-a0ef-f08e1866c0c8 | hadoop-cluster-new-vanilla-default-worker1-2 | ACTIVE | -          | Running     | net1=10.10.0.5               |<br>| 54142f88-8170-4081-a2c0-b8da831a7dd1 | test                                         | ACTIVE | -          | Running     | network=172.18.23.11         |<br>+--------------------------------------+----------------------------------------------+--------+------------+-------------+------------------------------+<br>[root@control01 ~]# ping 172.18.23.11<br>PING 172.18.23.11 (172.18.23.11) 56(84) bytes of data.<br>64 bytes from 172.18.23.11: icmp_seq=22 ttl=63 time=3.23 ms<br>64 bytes from 172.18.23.11: icmp_seq=23 ttl=63 time=0.217 ms<br>64 bytes from 172.18.23.11: icmp_seq=24 ttl=63 time=0.223 ms<br>64 bytes from 172.18.23.11: icmp_seq=25 ttl=63 time=0.124 ms<br>64 bytes from 172.18.23.11: icmp_seq=26 ttl=63 time=0.189 ms<br>64 bytes from 172.18.23.11: icmp_seq=27 ttl=63 time=0.195 ms<br>64 bytes from 172.18.23.11: icmp_seq=28 ttl=63 time=0.186 ms<br>64 bytes from 172.18.23.11: icmp_seq=29 ttl=63 time=0.229 ms<br>^C<br>--- 172.18.23.11 ping statistics ---<br>29 packets transmitted, 8 received, 72% packet loss, time 27999ms<br>rtt min/avg/max/mdev = 0.124/0.575/3.239/1.007 ms<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>openstack</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ironic</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Python中类的定义与使用</title>
    <link href="/posts/Python%E4%B8%AD%E7%B1%BB%E7%9A%84%E5%AE%9A%E4%B9%89%E4%B8%8E%E4%BD%BF%E7%94%A8/"/>
    <url>/posts/Python%E4%B8%AD%E7%B1%BB%E7%9A%84%E5%AE%9A%E4%B9%89%E4%B8%8E%E4%BD%BF%E7%94%A8/</url>
    
    <content type="html"><![CDATA[<h3 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h3><p>　　1.类的定义</p><p>　　2.父类，子类定义，以及子类调用父类</p><p>　　3.类的组合使用</p><p>　　4.内置功能</p><a id="more"></a><hr><h3 id="1-类的定义"><a href="#1-类的定义" class="headerlink" title="1.类的定义"></a>1.类的定义</h3><p>代码如下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#!/usr/bin/env python</span><br><span class="hljs-comment">#coding:utf8</span><br><br><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Hotel</span>(<span class="hljs-params"><span class="hljs-built_in">object</span></span>):</span><br>    <span class="hljs-string">&quot;&quot;&quot;docstring for Hotel&quot;&quot;&quot;</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, room, cf=<span class="hljs-number">1.0</span>, br=<span class="hljs-number">15</span></span>):</span><br>        self.room = room<br>        self.cf = cf<br>        self.br = br<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">cacl_all</span>(<span class="hljs-params">self, days=<span class="hljs-number">1</span></span>):</span><br>        <span class="hljs-keyword">return</span> (self.room * self.cf + self.br) * days<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    stdroom = Hotel(<span class="hljs-number">200</span>)<br>    big_room = Hotel(<span class="hljs-number">230</span>, <span class="hljs-number">0.9</span>)<br>    <span class="hljs-built_in">print</span> stdroom.cacl_all()<br>    <span class="hljs-built_in">print</span> stdroom.cacl_all(<span class="hljs-number">2</span>)<br>    <span class="hljs-built_in">print</span> big_room.cacl_all()<br>    <span class="hljs-built_in">print</span> big_room.cacl_all(<span class="hljs-number">3</span>)<br></code></pre></td></tr></table></figure><hr><h3 id="2-父类、子类以及调用父类"><a href="#2-父类、子类以及调用父类" class="headerlink" title="2.父类、子类以及调用父类"></a>2.父类、子类以及调用父类</h3><p>代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#!/usr/bin/env python</span><br><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><br><span class="hljs-comment"># 父类</span><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">AddBook</span>(<span class="hljs-params"><span class="hljs-built_in">object</span></span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, name, phone</span>):</span><br>        self.name = name<br>        self.phone = phone<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_phone</span>(<span class="hljs-params">self</span>):</span><br>        <span class="hljs-keyword">return</span> self.phone<br><br><span class="hljs-comment"># 子类，继承</span><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">EmplEmail</span>(<span class="hljs-params">AddBook</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, nm, ph, email</span>):</span><br>        <span class="hljs-comment"># AddBook.__init__(self, nm, ph) # 调用父类方法一</span><br>        <span class="hljs-built_in">super</span>(EmplEmail, self).__init__(nm, ph) <span class="hljs-comment"># 调用父类方法二</span><br>        self.email = email<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_email</span>(<span class="hljs-params">self</span>):</span><br>        <span class="hljs-keyword">return</span> self.email<br><br><span class="hljs-comment"># 调用</span><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    Detian = AddBook(<span class="hljs-string">&#x27;handetian&#x27;</span>, <span class="hljs-string">&#x27;18210413001&#x27;</span>)<br>    Meng = AddBook(<span class="hljs-string">&#x27;shaomeng&#x27;</span>, <span class="hljs-string">&#x27;18210413002&#x27;</span>)<br><br>    <span class="hljs-built_in">print</span> Detian.get_phone()<br>    <span class="hljs-built_in">print</span> AddBook.get_phone(Meng)<br><br>    alice = EmplEmail(<span class="hljs-string">&#x27;alice&#x27;</span>, <span class="hljs-string">&#x27;18210418888&#x27;</span>, <span class="hljs-string">&#x27;alice@xkops.com&#x27;</span>)<br>    <span class="hljs-built_in">print</span> alice.get_email(), alice.get_phone()<br></code></pre></td></tr></table></figure><hr><h3 id="3-类的组合使用"><a href="#3-类的组合使用" class="headerlink" title="3.类的组合使用"></a>3.类的组合使用</h3><p>代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#!/usr/bin/env python</span><br><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">1.class类的组合使用</span><br><span class="hljs-string">2.手机、邮箱、QQ等是可以变化的（定义在一起），姓名不可变（单独定义）。</span><br><span class="hljs-string">3.在另一个类中引用</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Info</span>(<span class="hljs-params"><span class="hljs-built_in">object</span></span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, phone, email, qq</span>):</span><br>        self.phone = phone<br>        self.email = email<br>        self.qq = qq<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_phone</span>(<span class="hljs-params">self</span>):</span><br>        <span class="hljs-keyword">return</span> self.phone<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">update_phone</span>(<span class="hljs-params">self, newphone</span>):</span><br>        self.phone = newphone<br>        <span class="hljs-built_in">print</span> <span class="hljs-string">&quot;手机号更改已更改&quot;</span><br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_email</span>(<span class="hljs-params">self</span>):</span><br>        <span class="hljs-keyword">return</span> self.email<br><br><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">AddrBook</span>(<span class="hljs-params"><span class="hljs-built_in">object</span></span>):</span><br>    <span class="hljs-string">&#x27;&#x27;&#x27;docstring for AddBook&#x27;&#x27;&#x27;</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, name, phone, email, qq</span>):</span><br>        self.name = name<br>        self.info = Info(phone, email, qq)<br><br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    Detian = AddrBook(<span class="hljs-string">&#x27;handetian&#x27;</span>, <span class="hljs-string">&#x27;18210413001&#x27;</span>, <span class="hljs-string">&#x27;detian@xkops.com&#x27;</span>, <span class="hljs-string">&#x27;123456&#x27;</span>)<br>    <span class="hljs-built_in">print</span> Detian.info.get_phone()<br>    Detian.info.update_phone(<span class="hljs-number">18210413002</span>)<br>    <span class="hljs-built_in">print</span> Detian.info.get_phone()<br>    <span class="hljs-built_in">print</span> Detian.info.get_email()<br></code></pre></td></tr></table></figure><hr><h3 id="4-内置功能-函数（）加与不加的区别）"><a href="#4-内置功能-函数（）加与不加的区别）" class="headerlink" title="4.内置功能(函数（）加与不加的区别）"></a>4.内置功能(函数（）加与不加的区别）</h3><p>代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#!/usr/bin/env python</span><br><span class="hljs-comment">#coding:utf8</span><br><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Books</span>(<span class="hljs-params"><span class="hljs-built_in">object</span></span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, title, author</span>):</span><br>        self.title = title<br>        self.author = author<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__str__</span>(<span class="hljs-params">self</span>):</span><br>        <span class="hljs-keyword">return</span> self.title<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__repr__</span>(<span class="hljs-params">self</span>):</span><br>        <span class="hljs-keyword">return</span> self.title<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__call__</span>(<span class="hljs-params">self</span>):</span><br>        <span class="hljs-built_in">print</span> <span class="hljs-string">&quot;%s is written by %s&quot;</span> %(self.title, self.author)<br><br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    pybook = Books(<span class="hljs-string">&#x27;Core Python&#x27;</span>, <span class="hljs-string">&#x27;Wesley&#x27;</span>)<br>    <span class="hljs-built_in">print</span> pybook<br>    pybook()<br></code></pre></td></tr></table></figure><hr><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#!/usr/bin/env python</span><br><span class="hljs-comment">#coding:utf8</span><br><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Books</span>(<span class="hljs-params"><span class="hljs-built_in">object</span></span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, title, author</span>):</span><br>        self.title = title<br>        self.author = author<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__str__</span>(<span class="hljs-params">self</span>):</span><br>        <span class="hljs-keyword">return</span> self.title<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__repr__</span>(<span class="hljs-params">self</span>):</span><br>        <span class="hljs-keyword">return</span> self.title<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__call__</span>(<span class="hljs-params">self</span>):</span><br>        <span class="hljs-built_in">print</span> <span class="hljs-string">&quot;%s is written by %s&quot;</span> %(self.title, self.author)<br><br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    pybook = Books(<span class="hljs-string">&#x27;Core Python&#x27;</span>, <span class="hljs-string">&#x27;Wesley&#x27;</span>)<br>    <span class="hljs-built_in">print</span> pybook<br>    pybook()<br>复制代码<br>复制代码<br>----------------------------------------------------------------------------------------------------------------------------------------------------------------<br><br>复制代码<br>复制代码<br><span class="hljs-comment">#!/usr/bin/env python</span><br><span class="hljs-comment">#coding:utf8</span><br><br><span class="hljs-class"><span class="hljs-keyword">class</span>  <span class="hljs-title">Number</span>(<span class="hljs-params"><span class="hljs-built_in">object</span></span>):</span><br>    <span class="hljs-string">&quot;&quot;&quot;Custum object</span><br><span class="hljs-string">    add/radd -&gt; +; </span><br><span class="hljs-string">    sub/rsub -&gt; -;</span><br><span class="hljs-string">    mul/rmul -&gt; *;</span><br><span class="hljs-string">    div/rdiv -&gt; /;</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, number</span>):</span><br>        self.number = number<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__add__</span>(<span class="hljs-params">self, other</span>):</span><br>        <span class="hljs-keyword">return</span> self.number + other        <br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__radd__</span>(<span class="hljs-params">self, other</span>):</span><br>        <span class="hljs-keyword">return</span> self.number  + other<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__sub__</span>(<span class="hljs-params">self, other</span>):</span><br>        <span class="hljs-keyword">return</span> self.number - other<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__rsub__</span>(<span class="hljs-params">self, other</span>):</span><br>        <span class="hljs-keyword">return</span> other - self.number<br><br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__gt__</span>(<span class="hljs-params">self, other</span>):</span><br>        <span class="hljs-keyword">if</span> self.number &gt; other:<br>            <span class="hljs-keyword">return</span> <span class="hljs-literal">True</span><br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">False</span><br><br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    num = Number(<span class="hljs-number">10</span>)<br>    <span class="hljs-built_in">print</span> num + <span class="hljs-number">20</span><br>    <span class="hljs-built_in">print</span> <span class="hljs-number">30</span> + num<br>    <span class="hljs-built_in">print</span> num - <span class="hljs-number">5</span><br>    <span class="hljs-built_in">print</span> <span class="hljs-number">11</span> - num<br>    <span class="hljs-built_in">print</span> num &gt; <span class="hljs-number">20</span><br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>基于CentOS 7.3 安装Ceph Jewel 10.2.9</title>
    <link href="/posts/%E5%9F%BA%E4%BA%8ECentOS-7-3-%E5%AE%89%E8%A3%85Ceph-Jewel-10-2-9/"/>
    <url>/posts/%E5%9F%BA%E4%BA%8ECentOS-7-3-%E5%AE%89%E8%A3%85Ceph-Jewel-10-2-9/</url>
    
    <content type="html"><![CDATA[<h3 id="基于CentOS-7-3-安装Ceph-Jewel-10-2-9"><a href="#基于CentOS-7-3-安装Ceph-Jewel-10-2-9" class="headerlink" title="基于CentOS 7.3 安装Ceph Jewel 10.2.9"></a>基于<code>CentOS 7.3</code> 安装<code>Ceph Jewel 10.2.9</code></h3><a id="more"></a><h3 id="网络架构图"><a href="#网络架构图" class="headerlink" title="网络架构图"></a>网络架构图</h3><p><img src="https://ljw.howieli.cn/blog/2017-7-18/ceph-top.png"></p><h3 id="节点信息配置"><a href="#节点信息配置" class="headerlink" title="节点信息配置"></a>节点信息配置</h3><p><img src="https://ljw.howieli.cn/blog/2017-7-18/ceph-lab-configure.png"></p><h3 id="配置说明："><a href="#配置说明：" class="headerlink" title="配置说明："></a>配置说明：</h3><blockquote><p>采用了4台centos7.3系统的虚拟机，1台Ceph-Master作为安装节点，NTP Server；3台Ceph节点，既作为OSD节点，也作为Monitor节点。每个OSD节点有6个盘：300G的系统盘，3个2TB作为SATA池的OSD，800GB作为SSD池的OSD，240GB SSD盘作为日志盘。</p></blockquote><h3 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h3><p>这里安装centos7.3的操作系统我就不多说了，下面我说一下环境准备工作。</p><p>1.检查操作系统的版本</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> cat /etc/redhat-release</span><br>CentOS Linux release 7.3.1611 (Core)<br></code></pre></td></tr></table></figure><p>2.查看系统内核版本</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> uname -r</span><br>3.10.0-514.26.2.el7.x86_64<br></code></pre></td></tr></table></figure><p>3.关闭防火墙和<code>selinux</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell">sed -i &#x27;s/SELINUX=.*/SELINUX=disabled/&#x27; /etc/selinux/config<br>setenforce 0<br>systemctl stop firewalld<br>systemctl disable firewalld<br></code></pre></td></tr></table></figure><p>4.查看设备信息（所有的<code>OSD</code>节点）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> lsblk</span><br>NAME        MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT<br>sda           8:0    0  300G  0 disk <br>├─sda1        8:1    0    1G  0 part /boot<br>└─sda2        8:2    0  299G  0 part <br>  ├─cl-root 253:0    0   50G  0 lvm  /<br>  ├─cl-swap 253:1    0    2G  0 lvm  [SWAP]<br>  └─cl-home 253:2    0  247G  0 lvm  /home<br>sdb           8:16   0    2T  0 disk <br>sdc           8:32   0    2T  0 disk <br>sdd           8:48   0    2T  0 disk <br>sde           8:64   0  800G  0 disk <br>sdf           8:80   0  240G  0 disk <br>sr0          11:0    1 1024M  0 rom<br></code></pre></td></tr></table></figure><p>5.查看网卡配置（所有<code>OSD</code>节点）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> ip a</span><br>1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1<br>    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00<br>    inet 127.0.0.1/8 scope host lo<br>       valid_lft forever preferred_lft forever<br>2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000<br>    link/ether 00:0c:29:06:c8:a4 brd ff:ff:ff:ff:ff:ff<br>    inet 172.16.0.11/24 brd 172.16.0.255 scope global eth0<br>       valid_lft forever preferred_lft forever<br>3: eth1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000<br>    link/ether 00:0c:29:06:c8:ae brd ff:ff:ff:ff:ff:ff<br>    inet 192.168.0.11/24 brd 192.168.0.255 scope global eth1<br>       valid_lft forever preferred_lft forever<br></code></pre></td></tr></table></figure><p>6.在所有节点配置<code>hosts</code>解析</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell">172.16.0.10 ceph-master<br>172.16.0.11 ceph-node-1<br>172.16.0.12 ceph-node-2<br>172.16.0.13 ceph-node-3<br></code></pre></td></tr></table></figure><p>7.安装基础的软件包(所有节点)</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">yum install tree nmap sysstat lrzsz dos2unix wegt git net-tools -y<br></code></pre></td></tr></table></figure><p>8.建立<code>SSH</code>通信（在<code>ceph-master</code>节点上执行）<br>（1）生成秘钥文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">ssh-keygen -t rsa<br></code></pre></td></tr></table></figure><p>（2）拷贝秘钥文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell">ssh-copy-id root@ceph-master<br>ssh-copy-id root@ceph-node-1<br>ssh-copy-id root@ceph-node-2<br>ssh-copy-id root@ceph-node-3<br></code></pre></td></tr></table></figure><p>环境准备基本完成</p><h3 id="配置NTP服务"><a href="#配置NTP服务" class="headerlink" title="配置NTP服务"></a>配置<code>NTP</code>服务</h3><p>首先我们要在所有的节点上安装<code>NTP</code>服务</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> yum install -y ntp</span><br></code></pre></td></tr></table></figure><h4 id="在ceph-master节点上配置"><a href="#在ceph-master节点上配置" class="headerlink" title="在ceph-master节点上配置"></a>在<code>ceph-master</code>节点上配置</h4><p>1.修改<code>NTP</code>配置文件<code>/etc/ntp.conf</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> vim /etc/ntp.conf</span><br><span class="hljs-meta">#</span><span class="bash">server 0.centos.pool.ntp.org iburst</span><br><span class="hljs-meta">#</span><span class="bash">server 1.centos.pool.ntp.org iburst</span><br><span class="hljs-meta">#</span><span class="bash">server 2.centos.pool.ntp.org iburst</span><br><span class="hljs-meta">#</span><span class="bash">server 3.centos.pool.ntp.org iburst</span><br>restrict 172.16.0.0 mask 255.255.255.0 nomodify notrap<br>server 127.127.1.0 minpoll 4<br>fudge 127.127.1.0 stratum 0<br></code></pre></td></tr></table></figure><p>2.修改配置文件<code>/etc/ntp/step-tickers</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> vim /etc/ntp/step-tickers</span><br><span class="hljs-meta">#</span><span class="bash">0.centos.pool.ntp.org</span><br>127.127.1.0<br></code></pre></td></tr></table></figure><p>3.启动<code>NTP</code>服务，并设置开机启动</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">systemctl enable ntpd ; systemctl start ntpd<br></code></pre></td></tr></table></figure><h4 id="在所有的OSD节点配置"><a href="#在所有的OSD节点配置" class="headerlink" title="在所有的OSD节点配置"></a>在所有的<code>OSD</code>节点配置</h4><p>1.修改NTP配置文件<code>/etc/ntp.conf</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> vim /etc/ntp.conf</span><br><span class="hljs-meta">#</span><span class="bash">server 0.centos.pool.ntp.org iburst</span><br><span class="hljs-meta">#</span><span class="bash">server 1.centos.pool.ntp.org iburst</span><br><span class="hljs-meta">#</span><span class="bash">server 2.centos.pool.ntp.org iburst</span><br><span class="hljs-meta">#</span><span class="bash">server 3.centos.pool.ntp.org iburst</span><br>server 172.16.0.10<br></code></pre></td></tr></table></figure><p>2.启动<code>NTP</code>服务，并设置开机启动</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">systemctl enable ntpd ; systemctl start ntpd<br></code></pre></td></tr></table></figure><h4 id="验证NTP"><a href="#验证NTP" class="headerlink" title="验证NTP"></a>验证<code>NTP</code></h4><p>在所有节点执行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> ntpq -p</span><br><br>     remote           refid      st t when poll reach   delay   offset  jitter<br>==============================================================================<br>*ceph-master     .LOCL.           1 u   16   64  377    0.269    0.032   0.269<br></code></pre></td></tr></table></figure><p>前面有 * 表示已经同步。</p><h3 id="安装ceph"><a href="#安装ceph" class="headerlink" title="安装ceph"></a>安装<code>ceph</code></h3><p>####跟新系统源(所有节点执行)</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs shell">rm -rf /etc/yum.repos.d/*.repo<br>wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo<br>wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo<br>sed -i &#x27;/aliyuncs/d&#x27; /etc/yum.repos.d/CentOS-Base.repo<br>sed -i &#x27;s/$releasever/7/g&#x27; /etc/yum.repos.d/CentOS-Base.repo<br>sed -i &#x27;/aliyuncs/d&#x27; /etc/yum.repos.d/epel.repo<br>yum clean all<br>yum makecache fast<br></code></pre></td></tr></table></figure><h4 id="安装ceph-deploy"><a href="#安装ceph-deploy" class="headerlink" title="安装ceph-deploy"></a>安装<code>ceph-deploy</code></h4><p>以下在<code>ceph-master</code>节点上执行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> yum install http://mirrors.163.com/ceph/rpm-jewel/el7/noarch/ceph-deploy-1.5.38-0.noarch.rpm</span><br></code></pre></td></tr></table></figure><h4 id="查看ceph-deploy版本"><a href="#查看ceph-deploy版本" class="headerlink" title="查看ceph-deploy版本"></a>查看<code>ceph-deploy</code>版本</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> ceph-deploy --version</span><br>1.5.38<br></code></pre></td></tr></table></figure><h4 id="创建ceph集群"><a href="#创建ceph集群" class="headerlink" title="创建ceph集群"></a>创建<code>ceph</code>集群</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">ceph-deploy new ceph-node-1 ceph-node-2 ceph-node-3<br></code></pre></td></tr></table></figure><h4 id="编辑ceph配置文件"><a href="#编辑ceph配置文件" class="headerlink" title="编辑ceph配置文件"></a>编辑<code>ceph</code>配置文件</h4><p>在<code>global</code>下添加以下配置：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> vim ceph.conf</span> <br>[global]<br>mon_clock_drift_allowed = 5<br>osd_journal_size = 20480<br>public_network=172.16.0.0/24<br></code></pre></td></tr></table></figure><h4 id="安装Ceph"><a href="#安装Ceph" class="headerlink" title="安装Ceph"></a>安装<code>Ceph</code></h4><p>直接指定源地址，不用担心<code>ceph.com</code>官方源无法下载了。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">ceph-deploy install --release jewel --repo-url http://mirrors.163.com/ceph/rpm-jewel/el7 --gpg-url http://mirrors.163.com/ceph/keys/release.asc ceph-master ceph-node-1 ceph-node-2 ceph-node-3<br></code></pre></td></tr></table></figure><h4 id="检查ceph版本"><a href="#检查ceph版本" class="headerlink" title="检查ceph版本"></a>检查<code>ceph</code>版本</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">ceph -v<br>ceph version 10.2.9 (2ee413f77150c0f375ff6f10edd6c8f9c7d060d0)<br></code></pre></td></tr></table></figure><h4 id="初始化mom节点"><a href="#初始化mom节点" class="headerlink" title="初始化mom节点"></a>初始化<code>mom</code>节点</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">ceph-deploy mon create-initial<br></code></pre></td></tr></table></figure><h4 id="查看集群状态（在OSD节点上查看）"><a href="#查看集群状态（在OSD节点上查看）" class="headerlink" title="查看集群状态（在OSD节点上查看）"></a>查看集群状态（在<code>OSD</code>节点上查看）</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> ceph -s</span><br>    cluster 60597e53-ad29-44bd-8dcd-db6aeae6f580<br>     health HEALTH_ERR<br>            no osds<br>     monmap e2: 3 mons at &#123;ceph-node-1=172.16.0.11:6789/0,ceph-node-2=172.16.0.12:6789/0,ceph-node-3=172.16.0.13:6789/0&#125;<br>            election epoch 6, quorum 0,1,2 ceph-node-1,ceph-node-2,ceph-node-3<br>     osdmap e1: 0 osds: 0 up, 0 in<br>            flags sortbitwise,require_jewel_osds<br>      pgmap v2: 64 pgs, 1 pools, 0 bytes data, 0 objects<br>            0 kB used, 0 kB / 0 kB avail<br>                  64 creating<br></code></pre></td></tr></table></figure><h3 id="配置管理节点ceph-master"><a href="#配置管理节点ceph-master" class="headerlink" title="配置管理节点ceph-master"></a>配置管理节点<code>ceph-master</code></h3><p>为什么每次查看<code>ceph –s</code>的只能在<code>OSD</code>节点上执行，而不能在<code>Master</code>节点上执行？<br>用<code>ceph-deploy</code>把配置文件和<code>admin</code>密钥拷贝到<code>Master</code>节点</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> ceph-deploy admin ceph-master</span><br></code></pre></td></tr></table></figure><p>确保你对<code>ceph.client.admin.keyring</code>有正确的操作权限</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> chmod +r /etc/ceph/ceph.client.admin.keyring</span><br></code></pre></td></tr></table></figure><p>检查集群的健康状况</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> ceph -s</span><br>    cluster 60597e53-ad29-44bd-8dcd-db6aeae6f580<br>     health HEALTH_ERR<br>            no osds<br>     monmap e2: 3 mons at &#123;ceph-node-1=172.16.0.11:6789/0,ceph-node-2=172.16.0.12:6789/0,ceph-node-3=172.16.0.13:6789/0&#125;<br>            election epoch 6, quorum 0,1,2 ceph-node-1,ceph-node-2,ceph-node-3<br>     osdmap e1: 0 osds: 0 up, 0 in<br>            flags sortbitwise,require_jewel_osds<br>      pgmap v2: 64 pgs, 1 pools, 0 bytes data, 0 objects<br>            0 kB used, 0 kB / 0 kB avail<br>                  64 creating<br></code></pre></td></tr></table></figure><h3 id="对OSD节点配置"><a href="#对OSD节点配置" class="headerlink" title="对OSD节点配置"></a>对<code>OSD</code>节点配置</h3><h4 id="磁盘分区"><a href="#磁盘分区" class="headerlink" title="磁盘分区"></a>磁盘分区</h4><p>将<code>240G</code>的<code>SSD</code>盘分出<code>4</code>个<code>20G</code>的分区用作 <code>journal</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> fdisk /dev/sdf</span><br>Welcome to fdisk (util-linux 2.23.2).<br>Changes will remain in memory only, until you decide to write them.<br>Be careful before using the write command.<br>Device does not contain a recognized partition table<br>Building a new DOS disklabel with disk identifier 0x9ec0a047.<br>Command (m for help): g<br>Building a new GPT disklabel (GUID: 31D328DD-E9A0-4306-9C99-8D42F7BA8008)<br>Command (m for help): n<br>Partition number (1-128, default 1): <br>First sector (2048-503316446, default 2048): <br>Last sector, +sectors or +size&#123;K,M,G,T,P&#125; (2048-503316446, default 503316446): +20G<br>Created partition 1<br>Command (m for help): n<br>Partition number (2-128, default 2): <br>First sector (41945088-503316446, default 41945088): <br>Last sector, +sectors or +size&#123;K,M,G,T,P&#125; (41945088-503316446, default 503316446): +20G<br>Created partition 2<br>Command (m for help): n<br>Partition number (3-128, default 3): <br>First sector (83888128-503316446, default 83888128): <br>Last sector, +sectors or +size&#123;K,M,G,T,P&#125; (83888128-503316446, default 503316446): +20G<br>Created partition 3<br>Command (m for help): n<br>Partition number (4-128, default 4): <br>First sector (125831168-503316446, default 125831168): <br>Last sector, +sectors or +size&#123;K,M,G,T,P&#125; (125831168-503316446, default 503316446): +20G<br>Created partition 4<br>Command (m for help): p<br>Disk /dev/sdf: 257.7 GB, 257698037760 bytes, 503316480 sectors<br>Units = sectors of 1 * 512 = 512 bytes<br>Sector size (logical/physical): 512 bytes / 512 bytes<br>I/O size (minimum/optimal): 512 bytes / 512 bytes<br>Disk label type: gpt<br><span class="hljs-meta">#</span><span class="bash">         Start          End    Size  Type            Name</span><br> 1         2048     41945087     20G  Linux filesyste <br> 2     41945088     83888127     20G  Linux filesyste <br> 3     83888128    125831167     20G  Linux filesyste <br> 4    125831168    167774207     20G  Linux filesyste <br>Command (m for help): w<br>The partition table has been altered!<br>Calling ioctl() to re-read partition table.<br>Syncing disks.<br></code></pre></td></tr></table></figure><h4 id="查看分区"><a href="#查看分区" class="headerlink" title="查看分区"></a>查看分区</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> lsblk</span><br>NAME        MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT<br>sda           8:0    0  300G  0 disk <br>├─sda1        8:1    0    1G  0 part /boot<br>└─sda2        8:2    0  299G  0 part <br>  ├─cl-root 253:0    0   50G  0 lvm  /<br>  ├─cl-swap 253:1    0    2G  0 lvm  [SWAP]<br>  └─cl-home 253:2    0  247G  0 lvm  /home<br>sdb           8:16   0    2T  0 disk <br>sdc           8:32   0    2T  0 disk <br>sdd           8:48   0    2T  0 disk <br>sde           8:64   0  800G  0 disk <br>sdf           8:80   0  240G  0 disk <br>├─sdf1        8:81   0   20G  0 part <br>├─sdf2        8:82   0   20G  0 part <br>├─sdf3        8:83   0   20G  0 part <br>└─sdf4        8:84   0   20G  0 part <br>sr0          11:0    1 1024M  0 rom<br></code></pre></td></tr></table></figure><h4 id="修改-Journal-分区权限"><a href="#修改-Journal-分区权限" class="headerlink" title="修改 Journal 分区权限"></a>修改 <code>Journal</code> 分区权限</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">chown ceph:ceph /dev/sdf[1-4]<br></code></pre></td></tr></table></figure><h3 id="添加OSD"><a href="#添加OSD" class="headerlink" title="添加OSD"></a>添加<code>OSD</code></h3><p>在<code>ceph-master</code>执行<br>先部署<code>SATA</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">ceph-deploy osd prepare ceph-node-1:/dev/sdb:/dev/sdf1 ceph-node-1:/dev/sdc:/dev/sdf2 ceph-node-1:/dev/sdd:/dev/sdf3 ceph-node-2:/dev/sdb:/dev/sdf1 ceph-node-2:/dev/sdc:/dev/sdf2 ceph-node-2:/dev/sdd:/dev/sdf3 ceph-node-3:/dev/sdb:/dev/sdf1 ceph-node-3:/dev/sdc:/dev/sdf2 ceph-node-3:/dev/sdd:/dev/sdf3<br></code></pre></td></tr></table></figure><p>再部署<code>SSD</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">ceph-deploy osd prepare ceph-node-1:/dev/sde:/dev/sdf4 ceph-node-2:/dev/sde:/dev/sdf4 ceph-node-3:/dev/sde:/dev/sdf4<br></code></pre></td></tr></table></figure><p>先部署<code>SATA</code>，再部署<code>SSD</code>，这样序号就会连续了…<br>查看集群状态</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> ceph -s</span><br>    cluster f9dba93b-c40f-4b31-8e6d-6ea830cad944<br>     health HEALTH_OK<br>     monmap e1: 3 mons at &#123;ceph-node-1=172.16.0.11:6789/0,ceph-node-2=172.16.0.12:6789/0,ceph-node-3=172.16.0.13:6789/0&#125;<br>            election epoch 6, quorum 0,1,2 ceph-node-1,ceph-node-2,ceph-node-3<br>     osdmap e56: 12 osds: 12 up, 12 in<br>            flags sortbitwise,require_jewel_osds<br>      pgmap v142: 128 pgs, 1 pools, 0 bytes data, 0 objects<br>            413 MB used, 20821 GB / 20821 GB avail<br>                 128 active+clean<br></code></pre></td></tr></table></figure><p>查看<code>OSD</code>的分布</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> ceph osd tree</span><br>ID WEIGHT   TYPE NAME            UP/DOWN REWEIGHT PRIMARY-AFFINITY <br>-1 20.33363 root default                                           <br>-2  6.77788     host ceph-node-1                                   <br> 0  1.99899         osd.0             up  1.00000          1.00000 <br> 1  1.99899         osd.1             up  1.00000          1.00000 <br> 4  1.99899         osd.4             up  1.00000          1.00000 <br> 9  0.78090         osd.9             up  1.00000          1.00000 <br>-3  6.77788     host ceph-node-2                                   <br> 2  1.99899         osd.2             up  1.00000          1.00000 <br> 3  1.99899         osd.3             up  1.00000          1.00000 <br> 5  1.99899         osd.5             up  1.00000          1.00000 <br>10  0.78090         osd.10            up  1.00000          1.00000 <br>-4  6.77788     host ceph-node-3                                   <br> 6  1.99899         osd.6             up  1.00000          1.00000 <br> 7  1.99899         osd.7             up  1.00000          1.00000 <br> 8  1.99899         osd.8             up  1.00000          1.00000 <br>11  0.78090         osd.11            up  1.00000          1.00000<br></code></pre></td></tr></table></figure><h3 id="安装完毕"><a href="#安装完毕" class="headerlink" title="安装完毕"></a>安装完毕</h3><p><code>ceph</code>安装完毕</p>]]></content>
    
    
    <categories>
      
      <category>ceph</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ceph</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>openstack新建云主机流程介绍</title>
    <link href="/posts/openstack%E6%96%B0%E5%BB%BA%E4%BA%91%E4%B8%BB%E6%9C%BA%E6%B5%81%E7%A8%8B%E4%BB%8B%E7%BB%8D/"/>
    <url>/posts/openstack%E6%96%B0%E5%BB%BA%E4%BA%91%E4%B8%BB%E6%9C%BA%E6%B5%81%E7%A8%8B%E4%BB%8B%E7%BB%8D/</url>
    
    <content type="html"><![CDATA[<h3 id="openstack新建云主机流程介绍"><a href="#openstack新建云主机流程介绍" class="headerlink" title="openstack新建云主机流程介绍"></a><code>openstack</code>新建云主机流程介绍</h3><a id="more"></a><p><code>openstack</code> 可以登录到你的<code>dashboard</code>上之后点击“项目-计算-实例-创建实例”按钮，然后选择一下相应的配置，只短短十几秒内就可以创建好一台云主机供我们使用了，然而这么牛逼的事情是怎么做到的呢？<br>新建一个云主机流程图<br><img src="https://ljw.howieli.cn/blog/2017-7-3/%E5%88%9B%E5%BB%BA%E8%99%9A%E6%8B%9F%E6%9C%BA%E6%B5%81%E7%A8%8B%E5%9B%BE.png"></p><p>上图我们可以看出要想新建一台完整的云主机至少需要29步。</p><h3 id="流程图分析"><a href="#流程图分析" class="headerlink" title="流程图分析"></a>流程图分析</h3><h4 id="流程-1"><a href="#流程-1" class="headerlink" title="流程-1"></a>流程-1</h4><p>首先我们访问<code>dashboard</code>之后，浏览器会显示一个登陆界面（如下图所示），<code>horizon</code>会让我们登陆用户名和密码，然后<code>horizon</code>会把用户名和密码交给<code>keystone</code>来确认我们的身份，<code>keystone</code>确认成功后，才会允许我们登陆进去。（在<code>openstack</code>里<code>keystone</code>就相当于<code>openstack</code>这些组件共同的老大哥）<br><img src="https://ljw.howieli.cn/blog/2017-7-3/%E7%99%BB%E9%99%86%E7%95%8C%E9%9D%A2.png"><br>在原来我一直以为这里登陆的时候只不过是一个简单的<code>django</code>框架使用<code>pymysql</code>直接查询数据库，而实际上这里的表单信息是提交到了<code>keystone</code>，然后通过<code>keystone</code>查询数据库进行验证的</p><h4 id="流程-2"><a href="#流程-2" class="headerlink" title="流程-2"></a>流程-2</h4><p><code>keystone</code>接受到<code>horizon</code>前端表单传过来的用户、密码信息以后，会去查询数据库来确认身份，确认了身份后会将一个<code>token</code>（就相当于办理了一个身份证~~~~）返回到该用户，让这个用户在进行操作的时候不在需要提供用户名和密码了，而是直接拿出<code>token</code>来。</p><h4 id="流程-3"><a href="#流程-3" class="headerlink" title="流程-3"></a>流程-3</h4><p><code>horizon</code>拿到<code>token</code>之后，实际上这里在<code>web</code>页面上的显示就是登陆成功了，接下来就是找到“项目-计算-实例-创建实例”按钮，点击创建实例，然后根据我们的需求填写云主机相关的配置信息。<br>填写完配置信息后，点击启动实例之后，<code>horizon</code>就会带着三个东西去找<code>nova-api</code>了。</p><ul><li>创建云主机的请求</li><li>云主机相关配置信息</li><li>刚刚<code>keystone</code>返回给的<code>token</code></li></ul><h4 id="流程-4"><a href="#流程-4" class="headerlink" title="流程-4"></a>流程-4</h4><p><code>nova-api</code>会拿着<code>horizon</code>给的<code>token</code>去找老大哥<code>keystone</code>验证去。</p><h4 id="流程-5"><a href="#流程-5" class="headerlink" title="流程-5"></a>流程-5</h4><p><code>keystone</code>看到<code>nova-api</code>拿来的<code>token</code>，查询一下数据库确认，然后告诉给<code>nova-api</code>，这兄弟信得过，可以照他的安排去做。</p><h4 id="流程-6"><a href="#流程-6" class="headerlink" title="流程-6"></a>流程-6</h4><p><code>nova-api</code>从大哥那回来，接收了<code>horizon</code>提供的两样东西，一是云主机配置信息，二是创建请求，这<code>nova-api</code>手底下也有一帮小兄弟，这帮人之间沟通可不太方便都得通过一块小黑板（<code>mq</code>消息队列），把自己的需求写在小黑板上，能做的了这事的人自然就去做了。但配置信息现在还不能写在小黑板上，得找到确定去干活的人之后才行啊，所以<code>nova-api</code>就把配置信息放到数据库里。</p><h4 id="流程-7"><a href="#流程-7" class="headerlink" title="流程-7"></a>流程-7</h4><p>数据库把配置信息收好之后，对<code>nova-api</code>说了声，我放好了。</p><h4 id="流程-8"><a href="#流程-8" class="headerlink" title="流程-8"></a>流程-8</h4><p>放好配置信息后<code>nova-api</code>就在小黑板上写“现在要创建一台云主机，配置信息我已经放到数据库了，然后让<code>nova-schedular</code>给安排。</p><h4 id="流程-9"><a href="#流程-9" class="headerlink" title="流程-9"></a>流程-9</h4><p><code>nova-schedular</code>他就像是<code>nova-api</code>的秘书，<code>nova-api</code>的有事都是通过它交代给其他人的，这一步就是他从小黑板上看到了<code>nova-api</code>的信息。</p><h4 id="流程-10"><a href="#流程-10" class="headerlink" title="流程-10"></a>流程-10</h4><p><code>nova-schedular</code>现在知道了要创建云主机，但它要看一看云主机都要什么配置，才好决定该把这事交给谁去做（这里是指多个<code>nova-compute</code>的情况，各个计算节点的资源使用情况都在<code>nova-schedular</code>这里），所以他让数据库把云主机配置信息发给他看看。</p><h4 id="流程-11"><a href="#流程-11" class="headerlink" title="流程-11"></a>流程-11</h4><p>数据库收到请求之后，把云主机配置信息发给<code>nova-schedular</code>。</p><h4 id="流程-12"><a href="#流程-12" class="headerlink" title="流程-12"></a>流程-12</h4><p><code>nova-schedular</code>拿到配置信息后，使用调度算法决定了要让<code>nova-compute</code>去干这个事，就在小黑板上写“<code>nova-compute</code>你给创建个云主机，配置都在数据库里了”。</p><h4 id="流程-13"><a href="#流程-13" class="headerlink" title="流程-13"></a>流程-13</h4><p><code>nova-compute</code>看到小黑板上的东西之后，本应该直接去数据库拿取配置信息，但因为<code>nova-compute</code>的特殊身份，<code>nova-compute</code>所在计算节点上全是云主机，万一有一台云主机被黑客入侵从而控制计算节点，直接拖库是很危险的。所以不能让<code>nova-compute</code>知道数据库在什么地方。</p><h4 id="流程-14"><a href="#流程-14" class="headerlink" title="流程-14"></a>流程-14</h4><p><code>nova-compute</code>没办法去数据库取东西难道就不工作了吗？那可不行啊，他不知道去哪取，但他哥们知道啊，于是他在小黑板上写“<code>nova-conductor</code>，你帮我去数据库取一下配置信息”。</p><h4 id="流程-15"><a href="#流程-15" class="headerlink" title="流程-15"></a>流程-15</h4><p><code>nova-conductor</code>从小黑板上看到了<code>nova-compute</code>的请求。</p><h4 id="流程-16"><a href="#流程-16" class="headerlink" title="流程-16"></a>流程-16</h4><p><code>nova-conductor</code>告诉数据库我要查看某某云主机的配置信息。</p><h4 id="流程-17"><a href="#流程-17" class="headerlink" title="流程-17"></a>流程-17</h4><p>数据库把云主机配置信息发送给<code>nova-conductor</code>。</p><h4 id="流程-18"><a href="#流程-18" class="headerlink" title="流程-18"></a>流程-18</h4><p><code>nova-conductor</code>把配置信息写在小黑板上。</p><h4 id="流程-19"><a href="#流程-19" class="headerlink" title="流程-19"></a>流程-19</h4><p><code>nova-compute</code>从小黑板上读取云主机的配置信息。</p><h4 id="流程-20"><a href="#流程-20" class="headerlink" title="流程-20"></a>流程-20</h4><p><code>nova-compute</code>拿到了云主机配置信息一看，人家可是专业的，立马就知道该怎么做了，先去找<code>glance-api</code>拿镜像吧，刚才讲了那么多，可都是在<code>nova</code>组件内部的，这次去找别的组件可不是写在小黑板上了，它得带着自己的身份证去，告诉<code>glance-api</code>，我要<code>xxx</code>镜像。</p><h4 id="流程-21"><a href="#流程-21" class="headerlink" title="流程-21"></a>流程-21</h4><p><code>glance-api</code>看<code>nova-compute</code>过来，他可不认识<code>nova-compute</code>，让<code>nova-compute</code>拿出身份证，拿着人家身份证找到自己的老大哥<code>keystone</code>看看这人靠不靠谱，<code>keystone</code>一看，没问题，按他说的做吧（在<code>nova</code>验证<code>horizon</code>被当做两步，这里化做一步，是为了简化重复的流程）。</p><h4 id="流程-22"><a href="#流程-22" class="headerlink" title="流程-22"></a>流程-22</h4><p><code>glance-api</code>把镜像资源信息返回给<code>nova-compute</code>（这里主要说创建云主机的过程，除<code>nova</code>外其他组件内部先不提）。</p><h4 id="流程-23"><a href="#流程-23" class="headerlink" title="流程-23"></a>流程-23</h4><p>接着<code>nova-compute</code>找到<code>neutron-server</code>（图里画错了，因为是偷别人的图，自己的图画了半天画不好。。。），告诉他我要<code>xxx</code>网络资源。</p><h4 id="流程-24"><a href="#流程-24" class="headerlink" title="流程-24"></a>流程-24</h4><p><code>neutron-server</code>也不认识他，拿着他的身份证找<code>keystone</code>确认了一下身份。</p><h4 id="流程-25"><a href="#流程-25" class="headerlink" title="流程-25"></a>流程-25</h4><p><code>nuetron-server</code>把网络资源信息返回给<code>nova-compute</code>。</p><h4 id="流程-26"><a href="#流程-26" class="headerlink" title="流程-26"></a>流程-26</h4><p><code>nova-compute</code>找到<code>cinder-api</code>要存储资源，云主机得有硬盘啊，得存东西啊（同样，这里图中也有错误）。</p><h4 id="流程-27"><a href="#流程-27" class="headerlink" title="流程-27"></a>流程-27</h4><p><code>cinder-api</code>也不认识他，拿着他的身份证找<code>keystone</code>确认了一下身份。</p><h4 id="流程-28"><a href="#流程-28" class="headerlink" title="流程-28"></a>流程-28</h4><p><code>cinder-api</code>把存储资源信息返回给<code>nova-compute</code>。</p><h4 id="流程-29"><a href="#流程-29" class="headerlink" title="流程-29"></a>流程-29</h4><p><code>nova-compute</code>拿到了所有资源之后，他其实也只是个收集信息的，他把工作全都交给了真正创建虚拟机的<code>Hypervisor</code>（<code>kvm</code>，<code>xen</code>等虚拟化技术）。</p><p>到此为止，你已经拥有了一台云主机了，流程看似复杂实际上在几十秒就完成了。</p>]]></content>
    
    
    <categories>
      
      <category>openstack</category>
      
    </categories>
    
    
    <tags>
      
      <tag>openstack</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>openstack镜像格式转换与上传</title>
    <link href="/posts/openstack%E9%95%9C%E5%83%8F%E6%A0%BC%E5%BC%8F%E8%BD%AC%E6%8D%A2%E4%B8%8E%E4%B8%8A%E4%BC%A0/"/>
    <url>/posts/openstack%E9%95%9C%E5%83%8F%E6%A0%BC%E5%BC%8F%E8%BD%AC%E6%8D%A2%E4%B8%8E%E4%B8%8A%E4%BC%A0/</url>
    
    <content type="html"><![CDATA[<h3 id="openstack镜像格式转换与上传"><a href="#openstack镜像格式转换与上传" class="headerlink" title="openstack镜像格式转换与上传"></a><code>openstack</code>镜像格式转换与上传</h3><a id="more"></a><h3 id="openstack常用镜像格式"><a href="#openstack常用镜像格式" class="headerlink" title="openstack常用镜像格式"></a><code>openstack</code>常用镜像格式</h3><p>今天我给大家介绍一下<code>openstack</code>常用的三种镜像格式，三种镜像格式分别是<code>qcow2</code>、<code>raw</code>、<code>vmdk</code>(所谓的<code>vmdk</code>就是<code>vmware</code>虚拟化的镜像格式，只有<code>openstack</code>纳管<code>vmware</code>才能用的到)。</p><h3 id="惊险格式的转换"><a href="#惊险格式的转换" class="headerlink" title="惊险格式的转换"></a>惊险格式的转换</h3><ul><li>将<code>raw</code>格式的镜像转化成<code>qcow2</code>格式（<code>image</code>代表镜像）<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">$</span><span class="bash"> qemu-img convert -f raw -O qcow2 image.img image.qcow2</span><br></code></pre></td></tr></table></figure></li><li>将<code>vmdk</code>格式的镜像转化成<code>raw</code>格式（<code>image</code>代表镜像）<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">$</span><span class="bash"> qemu-img convert -f vmdk -O raw image.vmdk image.img</span><br></code></pre></td></tr></table></figure></li><li>将<code>vmdk</code>格式的镜像转化成<code>qcow2</code>格式（<code>image</code>代表镜像）<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">$</span><span class="bash"> qemu-img convert -f vmdk -O qcow2 image.vmdk image.qcow2</span><br></code></pre></td></tr></table></figure><h3 id="获取镜像查看镜像信息"><a href="#获取镜像查看镜像信息" class="headerlink" title="获取镜像查看镜像信息"></a>获取镜像查看镜像信息</h3>查看镜像的详细信息<code>qemu-img info image.qcow2</code><br>例如<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">$</span><span class="bash"> qemu-img info CentOS-7-x86_64-GenericCloud.qcow2c</span> <br>image: CentOS-7-x86_64-GenericCloud.qcow2c<br>file format: qcow2                                #格式为qcow2<br>virtual size: 8.0G (8589934592 bytes)             #空间大小<br>disk size: 410M<br>cluster_size: 65536<br></code></pre></td></tr></table></figure><h3 id="上传镜像"><a href="#上传镜像" class="headerlink" title="上传镜像"></a>上传镜像</h3>这里我只介绍在<code>linux</code>命令行上传三种格式的镜像</li><li>上传<code>raw</code>格式的镜像<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">openstack image create --disk-format raw --container-format bare \<br>--public image_raw --file ./image.raw<br></code></pre></td></tr></table></figure></li><li>上传<code>qcow2</code>格式的镜像<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">glance image-create --name image_qcow2  --disk-format qcow2 --container-format bare &lt; image.qcow2<br></code></pre></td></tr></table></figure></li><li>上传<code>vmdk</code>格式的镜像<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">glance image-create --name image_vmdk --disk-format vmdk --container-format bare --property vmware_adaptertype=ide \<br>--property hw_disk_bus=ide --property hypervisor_type=vmware &lt; image.vmdk<br></code></pre></td></tr></table></figure></li></ul>]]></content>
    
    
    <categories>
      
      <category>openstack</category>
      
    </categories>
    
    
    <tags>
      
      <tag>openstack</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>利用kolla快速搭建openstack-ocata单节点</title>
    <link href="/posts/%E5%88%A9%E7%94%A8kolla%E5%BF%AB%E9%80%9F%E6%90%AD%E5%BB%BAopenstack-ocata%E5%8D%95%E8%8A%82%E7%82%B9/"/>
    <url>/posts/%E5%88%A9%E7%94%A8kolla%E5%BF%AB%E9%80%9F%E6%90%AD%E5%BB%BAopenstack-ocata%E5%8D%95%E8%8A%82%E7%82%B9/</url>
    
    <content type="html"><![CDATA[<h3 id="利用kolla快速安装openstack-ocata单节点"><a href="#利用kolla快速安装openstack-ocata单节点" class="headerlink" title="利用kolla快速安装openstack-ocata单节点"></a>利用<code>kolla</code>快速安装<code>openstack-ocata</code>单节点</h3><a id="more"></a><h3 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h3><ul><li>我还是使用大家最常用的<code>vmware workstation 12.0</code>, <code>CentOS 7.3 </code>虚拟机来完成整个的验证过程<br>在虚拟机安装<code>centos7.3</code>我就不再这里多说了,选择最小化安装即可。<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> cat /etc/redhat-release</span> <br>CentOS Linux release 7.3.1611 (Core) <br></code></pre></td></tr></table></figure></li><li>kolla的安装，要求目标机器是两块网卡，所以我虚拟机也是分配两块网卡<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> ip a|grep eno</span><br>2: eno16777736: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000<br>    inet 192.168.18.131/24 brd 192.168.18.255 scope global dynamic eno16777736<br>3: eno33554960: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast master ovs-system state UP qlen 1000<br>    inet 192.168.18.30/24 brd 192.168.18.255 scope global eno33554960<br></code></pre></td></tr></table></figure><code>eno16777736</code>:设置的IP是：<code>192.168.18.131</code>，日后<code>Horizon</code>访问就是通过这个<code>IP</code>地址<br><code>eno33554960</code>:设置的IP是：<code>192.168.18.30</code>，这个是让<code>neutron</code>的<code>br-ex</code>绑定使用，虚拟机是通过这块网卡访问外网。</li><li>修改主机名<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> hostname</span><br>localhost.localdomain<br><span class="hljs-meta">#</span><span class="bash"> hostnamectl set-hostname kolla</span><br></code></pre></td></tr></table></figure></li><li>关闭<code>firewalld</code>和<code>iptables</code><br><code>centos7</code>以上系统默认防火墙是<code>firewalld</code><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> systemctl <span class="hljs-built_in">disable</span> firewalld</span><br><span class="hljs-meta">#</span><span class="bash"> systemctl stop firewalld</span><br><span class="hljs-meta">#</span><span class="bash"> systemctl status firewalld</span><br></code></pre></td></tr></table></figure></li><li>关闭<code>selinux</code><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> sed -i <span class="hljs-string">&quot;s/SELINUX=enforcing/SELINUX=disabled/&quot;</span> /etc/selinux/config</span><br></code></pre></td></tr></table></figure></li><li>查看虚拟机是开启了虚拟<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> egrep <span class="hljs-string">&quot;vmx|svm&quot;</span> /proc/cpuinfo</span><br></code></pre></td></tr></table></figure>如果没有开启虚拟化请开启虚拟化说</li><li>重启系统<code>reboot</code>生效<h3 id="安装基础包"><a href="#安装基础包" class="headerlink" title="安装基础包"></a>安装基础包</h3></li><li>一定要先启用<code>EPEL</code>的<code>repo</code>源<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> yum install epel-release -y</span><br></code></pre></td></tr></table></figure></li><li>安装基础软件包<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> yum install vim net-tools tmux python-devel libffi-devel gcc openssl-devel git python-pip -y</span><br></code></pre></td></tr></table></figure><h3 id="安装docker"><a href="#安装docker" class="headerlink" title="安装docker"></a>安装<code>docker</code></h3></li><li>设置<code>repo</code><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> tee /etc/yum.repos.d/docker.repo &lt;&lt; <span class="hljs-string">&#x27;EOF&#x27;</span></span><br>[dockerrepo]<br>name=Docker Repository<br>baseurl=https://yum.dockerproject.org/repo/main/centos/$releasever/<br>enabled=1<br>gpgcheck=1<br>gpgkey=https://yum.dockerproject.org/gpg<br>EOF<br></code></pre></td></tr></table></figure></li><li>安装<code>Docker 1.12.5</code><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> yum install docker-engine-1.12.5 docker-engine-selinux-1.12.5 -y</span><br></code></pre></td></tr></table></figure></li><li>设置<code>docker</code><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> mkdir /etc/systemd/system/docker.service.d</span><br><span class="hljs-meta">#</span><span class="bash"> tee /etc/systemd/system/docker.service.d/kolla.conf &lt;&lt; <span class="hljs-string">&#x27;EOF&#x27;</span></span><br>[Service]<br>MountFlags=shared<br>EOF<br></code></pre></td></tr></table></figure></li><li>重启先关服务<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> systemctl daemon-reload</span><br><span class="hljs-meta">#</span><span class="bash"> systemctl <span class="hljs-built_in">enable</span> docker</span><br><span class="hljs-meta">#</span><span class="bash"> systemctl restart docker</span><br><span class="hljs-meta">#</span><span class="bash"> systemctl status docker</span><br></code></pre></td></tr></table></figure></li><li>访问私有的<code>Docker</code>仓库<br>查看网卡的<code>ip</code><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> ip a|grep eno16777736 |grep inet|awk <span class="hljs-string">&#x27;&#123;print $2&#125;&#x27;</span>|cut -d/ -f1</span><br></code></pre></td></tr></table></figure>编辑<code>/usr/lib/systemd/system/docker.service</code><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> ExecStart=/usr/bin/dockerd</span><br>ExecStart=/usr/bin/dockerd --insecure-registry 192.168.18.131:4000<br></code></pre></td></tr></table></figure></li><li>重启服务<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> systemctl daemon-reload</span><br><span class="hljs-meta">#</span><span class="bash"> systemctl restart docker</span><br></code></pre></td></tr></table></figure><h3 id="安装Ansible"><a href="#安装Ansible" class="headerlink" title="安装Ansible"></a>安装<code>Ansible</code></h3><code>Kolla</code>项目的<code>Mitaka</code>版本要求<code>ansible</code>版本低于<code>2.0</code>，<code>Newton</code>版本以后的就只支持<code>2.x</code>以上的版本。<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> yum -y install ansible</span><br></code></pre></td></tr></table></figure><h3 id="搭建Registry服务器"><a href="#搭建Registry服务器" class="headerlink" title="搭建Registry服务器"></a>搭建<code>Registry</code>服务器</h3>默认<code>docker</code>的<code>registry</code>是使用<code>5000</code>端口，对于<code>OpenStack</code>来说，有端口冲突，所以我将端口改成了<code>4000</code>。<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> docker run -d -v /opt/registry:/var/lib/registry -p 4000:5000 \</span><br><span class="bash">--restart=always --name registry registry:2</span><br></code></pre></td></tr></table></figure><h3 id="下载kolla官方提供的ocata镜像"><a href="#下载kolla官方提供的ocata镜像" class="headerlink" title="下载kolla官方提供的ocata镜像"></a>下载<code>kolla</code>官方提供的<code>ocata</code>镜像</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> wget http://tarballs.openstack.org/kolla/images/centos-source-registry-ocata.tar.gz</span><br><span class="hljs-meta">#</span><span class="bash"> du -sh centos-source-registry-ocata.tar.gz</span> <br>3.0Gcentos-source-registry-ocata.tar.gz<br><span class="hljs-meta">#</span><span class="bash"> tar zxf centos-source-registry-ocata.tar.gz -C /opt/registry/</span><br></code></pre></td></tr></table></figure><code>centos-source-registry-ocata</code>大约有3G。<h3 id="kolla-ansible"><a href="#kolla-ansible" class="headerlink" title="kolla-ansible"></a><code>kolla-ansible</code></h3></li><li>下载<code>kolla-ansible</code>的代码<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> <span class="hljs-built_in">cd</span></span><br><span class="hljs-meta">#</span><span class="bash"> git <span class="hljs-built_in">clone</span> http://git.trystack.cn/openstack/kolla-ansible -b stable/ocata</span><br></code></pre></td></tr></table></figure></li><li>安装<code>kolla-ansible</code><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> <span class="hljs-built_in">cd</span> kolla-ansible/</span><br><span class="hljs-meta">#</span><span class="bash"> pip install . -i https://pypi.tuna.tsinghua.edu.cn/simple</span><br></code></pre></td></tr></table></figure>如果<code>pip</code>速度很慢，后面可以加上参数<code>-i https://pypi.tuna.tsinghua.edu.cn/simple</code>，指定国内的<code>pip</code>源</li><li>复制相关文件<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> cp -r etc/kolla /etc/kolla/</span><br><span class="hljs-meta">#</span><span class="bash"> <span class="hljs-built_in">cd</span></span><br></code></pre></td></tr></table></figure></li><li>如果是在虚拟机里装<code>kolla</code>，希望可以启动再启动虚拟机，那么你需要把<code>virt_type=qemu</code>，默认是<code>kvm</code><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs shell">mkdir -p /etc/kolla/config/nova<br>cat &lt;&lt; EOF &gt; /etc/kolla/config/nova/nova-compute.conf<br>[libvirt]<br>virt_type=qemu<br>cpu_mode = none<br>EOF<br></code></pre></td></tr></table></figure><h3 id="安装kolla"><a href="#安装kolla" class="headerlink" title="安装kolla"></a>安装<code>kolla</code></h3></li><li>生成密码文件<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> kolla-genpwd</span><br></code></pre></td></tr></table></figure></li><li>编辑 <code>/etc/kolla/passwords.yml</code><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> vim /etc/kolla/passwords.yml</span><br>keystone_admin_password: admin<br></code></pre></td></tr></table></figure>这是登录<code>Dashboard</code>，<code>admin</code>使用的密码，你可以根据自己需要进行修改</li><li>编辑<code>/etc/kolla/globals.yml</code>文件<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> vim /etc/kolla/globals.yml</span><br>kolla_base_distro: &quot;centos&quot;<br>kolla_install_type: &quot;source&quot;<br>openstack_release: &quot;4.0.3&quot;<br>kolla_internal_vip_address: &quot;192.168.18.100&quot;<br>docker_registry: &quot;192.168.18.131:4000&quot;<br>docker_namespace: &quot;lokolla&quot;<br>network_interface: &quot;eno16777736&quot;<br>neutron_external_interface: &quot;eno33554960&quot;<br></code></pre></td></tr></table></figure>如果不知道<code>openstack_release</code>和<code>docker_namespace</code>请查看<br><code># cd /opt/registry/docker/registry/v2/repositories/lokolla/centos-source-keepalived/_manifests/tags/4.0.3/</code><br>就可以知道<code>openstack_release</code>为<code>4.0.3</code>和<code>docker_namespace</code>为<code>lokolla</code>了。<h3 id="验证部署"><a href="#验证部署" class="headerlink" title="验证部署"></a>验证部署</h3></li><li>验证<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> kolla-ansible prechecks -i kolla-ansible/ansible/inventory/all-in-one</span> <br></code></pre></td></tr></table></figure></li><li><code>pull</code>镜像<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> kolla-ansible pull -i kolla-ansible/ansible/inventory/all-in-one</span><br></code></pre></td></tr></table></figure></li><li>部署<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> kolla-ansible deploy -i kolla-ansible/ansible/inventory/all-in-one</span><br></code></pre></td></tr></table></figure>大约半个小时基本上就部署完成了<h3 id="验证部署-1"><a href="#验证部署-1" class="headerlink" title="验证部署"></a>验证部署</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> kolla-ansible post-deploy</span><br></code></pre></td></tr></table></figure>这样就创建 <code>/etc/kolla/admin-openrc.sh </code>文件<h3 id="安装安装OpenStack-client端"><a href="#安装安装OpenStack-client端" class="headerlink" title="安装安装OpenStack client端"></a>安装安装<code>OpenStack client</code>端</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> pip install python-openstackclient</span><br></code></pre></td></tr></table></figure><h3 id="安装成功"><a href="#安装成功" class="headerlink" title="安装成功"></a>安装成功</h3><img src="https://ljw.howieli.cn/blog/2017-6-23/dashboard.jpg"><br>账号：<code>admin</code><br>密码：<code>admin</code><h3 id="创建网络"><a href="#创建网络" class="headerlink" title="创建网络"></a>创建网络</h3></li><li>编辑 <code>/usr/share/kolla-ansible/init-runonce</code><br>网络需要根据实际情况修改,我都是用的<code>nat</code>模式<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">EXT_NET_CIDR=&#x27;192.168.18.0/24&#x27;<br>EXT_NET_RANGE=&#x27;start=192.168.18.10,end=192.168.18.20&#x27;<br>EXT_NET_GATEWAY=&#x27;192.168.18.2&#x27;<br></code></pre></td></tr></table></figure></li><li>运行脚本创建<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">source /etc/kolla/admin-openrc.sh<br>bash /usr/share/kolla-ansible/init-runonce<br></code></pre></td></tr></table></figure><h3 id="最后进入dashboard创建虚拟机，绑定浮动ip"><a href="#最后进入dashboard创建虚拟机，绑定浮动ip" class="headerlink" title="最后进入dashboard创建虚拟机，绑定浮动ip"></a>最后进入<code>dashboard</code>创建虚拟机，绑定浮动<code>ip</code></h3>这里我就不多说了，进入<code>dashboard</code>创建与主机即可，网络选择<code>demo-net</code>网络，创建完成后标定浮动<code>ip</code>即可。<h3 id="检查虚拟网络"><a href="#检查虚拟网络" class="headerlink" title="检查虚拟网络"></a>检查虚拟网络</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> openstack server list</span><br>+--------------------------------------+------+--------+----------------------------------+------------+<br>| ID                                   | Name | Status | Networks                         | Image Name |<br>+--------------------------------------+------+--------+----------------------------------+------------+<br>| 0267025b-8193-42ee-8551-67b7625482ab | tets | ACTIVE | demo-net=10.0.0.6, 192.168.18.12 | cirros     |<br>+--------------------------------------+------+--------+----------------------------------+------------+<br><span class="hljs-meta">#</span><span class="bash"> ping 192.168.18.12 -c 3</span><br>PING 192.168.18.12 (192.168.18.12) 56(84) bytes of data.<br>64 bytes from 192.168.18.12: icmp_seq=1 ttl=63 time=1.85 ms<br>64 bytes from 192.168.18.12: icmp_seq=2 ttl=63 time=1.46 ms<br>64 bytes from 192.168.18.12: icmp_seq=3 ttl=63 time=5.94 ms<br>--- 192.168.18.12 ping statistics ---<br>3 packets transmitted, 3 received, 0% packet loss, time 2005ms<br>rtt min/avg/max/mdev = 1.466/3.087/5.942/2.025 ms<br></code></pre></td></tr></table></figure><h3 id="检查虚拟机是能能通外网"><a href="#检查虚拟机是能能通外网" class="headerlink" title="检查虚拟机是能能通外网"></a>检查虚拟机是能能通外网</h3><code>cirros</code>用户密码默认为<code>cubswin:)</code><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> ssh cirros@192.168.18.12</span><br>cirros@192.168.18.12&#x27;s password: <br><span class="hljs-meta">$</span><span class="bash"> ip a</span><br>1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 16436 qdisc noqueue <br>    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00<br>    inet 127.0.0.1/8 scope host lo<br>    inet6 ::1/128 scope host <br>       valid_lft forever preferred_lft forever<br>2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc pfifo_fast qlen 1000<br>    link/ether fa:16:3e:82:4e:f8 brd ff:ff:ff:ff:ff:ff<br>    inet 10.0.0.6/24 brd 10.0.0.255 scope global eth0<br>    inet6 fe80::f816:3eff:fe82:4ef8/64 scope link <br>       valid_lft forever preferred_lft forever<br><span class="hljs-meta">$</span><span class="bash"> ping 8.8.8.8 -c 2</span><br>PING 8.8.8.8 (8.8.8.8): 56 data bytes<br>64 bytes from 8.8.8.8: seq=0 ttl=127 time=171.481 ms<br>64 bytes from 8.8.8.8: seq=1 ttl=127 time=171.464 ms<br>--- 8.8.8.8 ping statistics ---<br>2 packets transmitted, 2 packets received, 0% packet loss<br>round-trip min/avg/max = 171.464/171.472/171.481 ms<br></code></pre></td></tr></table></figure></li></ul>]]></content>
    
    
    <categories>
      
      <category>openstack</category>
      
    </categories>
    
    
    <tags>
      
      <tag>openstack</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>DevStack</title>
    <link href="/posts/DevStack/"/>
    <url>/posts/DevStack/</url>
    
    <content type="html"><![CDATA[<h3 id="DevStack安装openstack"><a href="#DevStack安装openstack" class="headerlink" title="DevStack安装openstack"></a><code>DevStack</code>安装<code>openstack</code></h3><a id="more"></a><h3 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h3><h4 id="关闭iptables防火墙或firewalld防火墙"><a href="#关闭iptables防火墙或firewalld防火墙" class="headerlink" title="关闭iptables防火墙或firewalld防火墙"></a>关闭<code>iptables</code>防火墙或<code>firewalld</code>防火墙</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> systemctl stop iptables</span><br><span class="hljs-meta">#</span><span class="bash"> systemctl <span class="hljs-built_in">disable</span> iptables</span><br><span class="hljs-meta">#</span><span class="bash"> systemctl stop firewalld</span><br><span class="hljs-meta">#</span><span class="bash"> systemctl <span class="hljs-built_in">disable</span> firewalld</span><br></code></pre></td></tr></table></figure><h4 id="关闭SElinux"><a href="#关闭SElinux" class="headerlink" title="关闭SElinux"></a>关闭<code>SElinux</code></h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> sed -i <span class="hljs-string">&#x27;SELINUX=enforcing/SELINUX=disable/&#x27;</span> /etc/selinux/config</span><br><span class="hljs-meta">#</span><span class="bash"> reboot</span><br></code></pre></td></tr></table></figure><h4 id="安装git"><a href="#安装git" class="headerlink" title="安装git"></a>安装<code>git</code></h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> yum -y install epel-release git</span><br></code></pre></td></tr></table></figure><h3 id="准备openstack环境"><a href="#准备openstack环境" class="headerlink" title="准备openstack环境"></a>准备<code>openstack</code>环境</h3><p>下载代码</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> <span class="hljs-built_in">cd</span> /home</span><br><span class="hljs-meta">#</span><span class="bash"> git <span class="hljs-built_in">clone</span> https://github.com/openstack-dev/devstack -b stable/ocata</span><br>//指定安装openstack ocata版本，如果你想安装其他版本，则指定其他版本名即可。<br></code></pre></td></tr></table></figure><h3 id="创建stack用户"><a href="#创建stack用户" class="headerlink" title="创建stack用户"></a>创建<code>stack</code>用户</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> bash devstack/tools/create-stack-user.sh</span><br></code></pre></td></tr></table></figure><h3 id="修改devstack目录的权限，让stack用户可以访问"><a href="#修改devstack目录的权限，让stack用户可以访问" class="headerlink" title="修改devstack目录的权限，让stack用户可以访问"></a>修改<code>devstack</code>目录的权限，让stack用户可以访问</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> chown -R stack:stack /home/devstack</span><br><span class="hljs-meta">#</span><span class="bash"> chmod 777 /opt/stack -R</span><br></code></pre></td></tr></table></figure><h3 id="切换到stack用户下"><a href="#切换到stack用户下" class="headerlink" title="切换到stack用户下"></a>切换到<code>stack</code>用户下</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> su stack</span><br><span class="hljs-meta">$</span><span class="bash"> <span class="hljs-built_in">cd</span> /home/devstack/</span><br></code></pre></td></tr></table></figure><h3 id="创建一个local-conf文件，添加如下内容。"><a href="#创建一个local-conf文件，添加如下内容。" class="headerlink" title="创建一个local.conf文件，添加如下内容。"></a>创建一个<code>local.conf</code>文件，添加如下内容。</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">$</span><span class="bash"> vim local.conf</span><br>[[local|localrc]]<br>ADMIN_PASSWORD=admin<br>DATABASE_PASSWORD=$ADMIN_PASSWORD<br>RABBIT_PASSWORD=$ADMIN_PASSWORD<br>SERVICE_PASSWORD=$ADMIN_PASSWORD<br></code></pre></td></tr></table></figure><h3 id="运行DevStack，执行安装"><a href="#运行DevStack，执行安装" class="headerlink" title="运行DevStack，执行安装"></a>运行<code>DevStack</code>，执行安装</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">$</span><span class="bash"> ./stack.sh</span><br></code></pre></td></tr></table></figure><p>大约30分钟安装完成</p><h3 id="默认DevStack会创建admin和demo两个用户，通过设置环境变量可以进行相关的操作。进入到-home-devstack目录下。"><a href="#默认DevStack会创建admin和demo两个用户，通过设置环境变量可以进行相关的操作。进入到-home-devstack目录下。" class="headerlink" title="默认DevStack会创建admin和demo两个用户，通过设置环境变量可以进行相关的操作。进入到/home/devstack目录下。"></a>默认<code>DevStack</code>会创建<code>admin</code>和<code>demo</code>两个用户，通过设置环境变量可以进行相关的操作。进入到<code>/home/devstack</code>目录下。</h3><h4 id="admin用户："><a href="#admin用户：" class="headerlink" title="admin用户："></a><code>admin</code>用户：</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">$</span><span class="bash"> <span class="hljs-built_in">source</span> openrc admin admin</span><br></code></pre></td></tr></table></figure><h4 id="demo用户"><a href="#demo用户" class="headerlink" title="demo用户"></a><code>demo</code>用户</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">$</span><span class="bash"> <span class="hljs-built_in">source</span> openrc demo demo</span><br></code></pre></td></tr></table></figure><h3 id="安装失败时，可以再次执行安装命令。"><a href="#安装失败时，可以再次执行安装命令。" class="headerlink" title="安装失败时，可以再次执行安装命令。"></a>安装失败时，可以再次执行安装命令。</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">$</span><span class="bash"> ./unstack.sh &amp;&amp; ./stack.sh</span><br></code></pre></td></tr></table></figure><h3 id="配置网络"><a href="#配置网络" class="headerlink" title="配置网络"></a>配置网络</h3><ul><li>由于在<code>DevStack</code>安装过程中，将<code>br-ex</code>的地址设置成了其他<code>IP</code>地址，因此需要将<code>br-ex</code>地址清楚掉，重新配置。<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">$</span><span class="bash"> sudo ip addr flush br-ex</span><br></code></pre></td></tr></table></figure></li><li>然后将物理网卡<code>eth0</code>作为<code>br-ex</code>的端口，这样创建的虚拟机就可以通过<code>eth0</code>访问外部网络，外部网络也可以通过<code>Floating IP</code>访问虚拟机。下面给出配置内容。<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> cat ifcfg-eth0</span><br>TYPE=OVSPort<br>DEVICE=eth0<br>DEVICETYPE=ovs<br>OVS_BRIDGE=br-ex<br>ONBOOT=yes<br><span class="hljs-meta">#</span><span class="bash"><span class="hljs-comment">###############################################################</span></span><br><span class="hljs-meta">#</span><span class="bash"> cat ifcfg-br-ex</span><br>TYPE=OVSBridge<br>DEVICE=br-ex<br>DEVICETYPE=ovs<br>BOOTPROTO=static<br>IPADDR=192.168.23.100<br>NETMASK=255.255.255.0<br>GATEWAY=192.168.23.1<br></code></pre></td></tr></table></figure></li><li>接袭来，设置<code>br-ex</code>外部网桥和重启网络。<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> ovs-vsctl add-port br-ex eth0</span><br><span class="hljs-meta">#</span><span class="bash"> systemctl restart network</span><br></code></pre></td></tr></table></figure><h3 id="登录Dashboard"><a href="#登录Dashboard" class="headerlink" title="登录Dashboard"></a>登录<code>Dashboard</code></h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">http://192.168.23.100<br>账户名： admin<br>密  码： admin<br></code></pre></td></tr></table></figure><img src="https://ljw.howieli.cn/blog/2017-6-20/dashboard.png"></li></ul>]]></content>
    
    
    <categories>
      
      <category>openstack</category>
      
    </categories>
    
    
    <tags>
      
      <tag>openstack</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>虚拟机手动迁移</title>
    <link href="/posts/%E8%99%9A%E6%8B%9F%E6%9C%BA%E6%89%8B%E5%8A%A8%E8%BF%81%E7%A7%BB/"/>
    <url>/posts/%E8%99%9A%E6%8B%9F%E6%9C%BA%E6%89%8B%E5%8A%A8%E8%BF%81%E7%A7%BB/</url>
    
    <content type="html"><![CDATA[<h3 id="openstack虚拟机手动迁移"><a href="#openstack虚拟机手动迁移" class="headerlink" title="openstack虚拟机手动迁移"></a>openstack虚拟机手动迁移</h3><a id="more"></a><h3 id="openstack迁移计算节点的实例"><a href="#openstack迁移计算节点的实例" class="headerlink" title="openstack迁移计算节点的实例"></a>openstack迁移计算节点的实例</h3><p>您的云平台必须使用的是共享存储</p><h3 id="查看可以迁移到的计算节点"><a href="#查看可以迁移到的计算节点" class="headerlink" title="查看可以迁移到的计算节点"></a>查看可以迁移到的计算节点</h3><p><code># openstack compute service list</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@control161 ~]# openstack compute service list<br>+-----+------------------+------------+----------+---------+-------+----------------------------+<br>|  ID | Binary           | Host       | Zone     | Status  | State | Updated At                 |<br>+-----+------------------+------------+----------+---------+-------+----------------------------+<br>|   4 | nova-scheduler   | control161 | internal | enabled | up    | 2017-06-19T08:31:10.000000 |<br>|  28 | nova-scheduler   | control162 | internal | enabled | up    | 2017-06-19T08:31:07.000000 |<br>|  37 | nova-scheduler   | control163 | internal | enabled | up    | 2017-06-19T08:31:12.000000 |<br>|  76 | nova-conductor   | control161 | internal | enabled | up    | 2017-06-19T08:31:04.000000 |<br>|  97 | nova-conductor   | control162 | internal | enabled | up    | 2017-06-19T08:31:11.000000 |<br>| 121 | nova-conductor   | control163 | internal | enabled | up    | 2017-06-19T08:31:04.000000 |<br>| 136 | nova-consoleauth | control161 | internal | enabled | up    | 2017-06-19T08:31:12.000000 |<br>| 139 | nova-consoleauth | control163 | internal | enabled | up    | 2017-06-19T08:31:06.000000 |<br>| 142 | nova-consoleauth | control162 | internal | enabled | up    | 2017-06-19T08:31:08.000000 |<br>| 145 | nova-compute     | compute164 | nova     | enabled | up    | 2017-06-19T08:31:08.000000 |<br>| 148 | nova-compute     | control162 | nova     | enabled | up    | 2017-06-19T08:31:10.000000 |<br>| 151 | nova-compute     | control161 | nova     | enabled | up    | 2017-06-19T08:31:06.000000 |<br>| 154 | nova-compute     | control163 | nova     | enabled | up    | 2017-06-19T08:31:11.000000 |<br>+-----+------------------+------------+----------+---------+-------+----------------------------+<br></code></pre></td></tr></table></figure><p>可以看到我的云平台上一共有4个计算节点</p><h3 id="获取需要迁移实例的列表"><a href="#获取需要迁移实例的列表" class="headerlink" title="获取需要迁移实例的列表"></a>获取需要迁移实例的列表</h3><p>我需要把control61节点上的所有虚拟机都迁移到control163上去<br><code>openstack server list --host control161 --all-projects</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@control161 ~]# openstack server list --host control161 --all-projects<br>+--------------------------------------+--------+--------+---------------------------------+------------+<br>| ID                                   | Name   | Status | Networks                        | Image Name |<br>+--------------------------------------+--------+--------+---------------------------------+------------+<br>| a834baba-68a6-4120-9f62-6ea26c6b3231 | test-5 | ACTIVE | network1=10.0.1.13              | cirros.raw |<br>| 4e835eaf-2dfe-4ac9-a61d-4345f4722e8e | test-1 | ACTIVE | network1=10.0.1.7, 192.168.24.7 | cirros.raw |<br>+--------------------------------------+--------+--------+---------------------------------+------------+<br></code></pre></td></tr></table></figure><h3 id="迁移所有的实例"><a href="#迁移所有的实例" class="headerlink" title="迁移所有的实例"></a>迁移所有的实例</h3><p>逐个迁移<br><code>openstack server migrate &lt;serverID&gt; --live control161</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@control161 ~]# openstack server migrate a834baba-68a6-4120-9f62-6ea26c6b3231 --live control163<br>[root@control161 ~]# openstack server migrate 4e835eaf-2dfe-4ac9-a61d-4345f4722e8e --live control163<br></code></pre></td></tr></table></figure><h3 id="验证是否迁移成功"><a href="#验证是否迁移成功" class="headerlink" title="验证是否迁移成功"></a>验证是否迁移成功</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@control161 ~]# openstack server list --host control161 --all-projects<br><br>[root@control161 ~]# openstack server list --host control163 --all-projects<br>+--------------------------------------+--------+--------+----------------------------------+------------+<br>| ID                                   | Name   | Status | Networks                         | Image Name |<br>+--------------------------------------+--------+--------+----------------------------------+------------+<br>| a834baba-68a6-4120-9f62-6ea26c6b3231 | test-5 | ACTIVE | network1=10.0.1.13               | cirros.raw |<br>| 84afc9cd-f0cc-4cfb-a455-507a2aa9dc17 | test-4 | ACTIVE | network1=10.0.1.8                | cirros.raw |<br>| 4e835eaf-2dfe-4ac9-a61d-4345f4722e8e | test-1 | ACTIVE | network1=10.0.1.7, 192.168.24.7  | cirros.raw |<br>| 2bfd7854-55fc-4868-bc66-db9dda2cb761 | test   | ACTIVE | network1=10.0.1.10, 192.168.24.5 | cirros     |<br>+--------------------------------------+--------+--------+----------------------------------+------------+<br></code></pre></td></tr></table></figure><p>可以看到controll61上的两台虚拟机test-1，test-5已经都到了contrll63节点上去了。</p>]]></content>
    
    
    <categories>
      
      <category>openstack</category>
      
    </categories>
    
    
    <tags>
      
      <tag>openstack</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>在linux系统下使用awk工具详解</title>
    <link href="/posts/%E5%9C%A8linux%E7%B3%BB%E7%BB%9F%E4%B8%8B%E4%BD%BF%E7%94%A8awk%E5%B7%A5%E5%85%B7%E8%AF%A6%E8%A7%A3/"/>
    <url>/posts/%E5%9C%A8linux%E7%B3%BB%E7%BB%9F%E4%B8%8B%E4%BD%BF%E7%94%A8awk%E5%B7%A5%E5%85%B7%E8%AF%A6%E8%A7%A3/</url>
    
    <content type="html"><![CDATA[<h3 id="在linux系统下使用awk工具详解"><a href="#在linux系统下使用awk工具详解" class="headerlink" title="在linux系统下使用awk工具详解"></a>在linux系统下使用awk工具详解</h3><a id="more"></a><h3 id="导读"><a href="#导读" class="headerlink" title="导读"></a>导读</h3><p>awk 是一个强大的文本分析工具。它不仅是 Linux 中，也是任何环境中现有的功能最强大的数据处理引擎之一。相对于 grep 的查找，sed的编辑，awk在其对数据分析并生成报告时，显得尤为强大。<br>当你第一次拿起双手在电脑上使用 awk 命令处理一个或者多个文件的时候，它会依次读取文件的每一行内容, 然后对其进行处理，awk 命令默认从 stdio 标准输入获取文件内容, awk 使用一对单引号来表示 一些可执行的脚本代码，在可执行脚本代码里面，使用一对花括号来表示一段可执行代码块，可以同时存在多个代码块。 awk 的每个花括号内同时又可以有多个指令，每一个指令用分号分隔，awk 其实就是一个脚本编程语言。说了这么多，你肯定还是一脸的懵逼。</p><h3 id="awk-命令的基本格式"><a href="#awk-命令的基本格式" class="headerlink" title="awk 命令的基本格式"></a>awk 命令的基本格式</h3><p><code>awk [options] &#39;program&#39; file</code><br>options 这个表示一些可选的参数选项，反正就是你爱用不用，不用可以拉到。。。<br>program 这个表示 awk 的可执行脚本代码，这个是必须要有的。 file 这个表示 awk 需要处理的文件，注意是纯文本文件，不是你的 mp3，也不是 mp4 啥的。。</p>]]></content>
    
    
    <categories>
      
      <category>Linux</category>
      
    </categories>
    
    
    <tags>
      
      <tag>awk</tag>
      
      <tag>Linux</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>网卡Bond介绍</title>
    <link href="/posts/%E7%BD%91%E5%8D%A1Bond%E4%BB%8B%E7%BB%8D/"/>
    <url>/posts/%E7%BD%91%E5%8D%A1Bond%E4%BB%8B%E7%BB%8D/</url>
    
    <content type="html"><![CDATA[<h3 id="网卡Bond介绍"><a href="#网卡Bond介绍" class="headerlink" title="网卡Bond介绍"></a>网卡Bond介绍</h3><a id="more"></a><h3 id="总括"><a href="#总括" class="headerlink" title="总括"></a>总括</h3><p>  Bond技术即bonding，它是一个Linux内核的一个模块，能将多块物理网卡绑定到一张虚拟网卡上的技术，并通过修改Linux的网口驱动让多块网卡看起来是一个单独的以太网接口设备并具有相同的 IP 地址。Bond技术一般用于解决网卡的单点故障或用于网卡负载较高的场景。</p><p>  Bond 的网卡运行在混杂模式(Promisc)下。在正常情况下，网卡只接收目的硬件地址(MAC Address)是自身MAC的以太网帧，过滤其他数据帧，以减轻驱动程序的负担。但是网卡也支持另外一种被称为混杂的模式，运行在此模式下的网卡可以接收网络上所有的数据帧。bonding就运行在这种模式下，而且修改了驱动程序中的MAC地址，将两张或多张网卡的MAC地址改成同一MAC地址。</p><p>网卡的bond模式一共有7种，下面详细介绍。</p><h3 id="模式0"><a href="#模式0" class="headerlink" title="模式0"></a>模式0</h3><p>  模式0（mode=0，round-robin）：此模式使用轮询策略，即顺序的在每一个被bond的网卡上发送数据包，这种模式提供负载均衡和容错能力。Bond0可以保证bond虚拟网卡和被bond的两张或多张物理网卡拥有相同的MAC地址，其中bond虚拟网卡的MAC地址是其中一张物理网卡的MAC地址，而bond虚拟网卡的MAC地址是根据bond自己实现的一个算法来选择的。<br>  在bond0模式下，如果一个连接或者会话的数据包从不同的网口发出，途中再经过不同的链路，则在客户端很有可能会出现数据包无序到达的现象，而无序到达的数据包一般需要重新发送，这样网络的吞吐量就会下降。同时，如果做bond0的两张或多张网卡接到了同一交换机上，还需对其配置聚合模式。</p><h3 id="模式1"><a href="#模式1" class="headerlink" title="模式1"></a>模式1</h3><p>  模式1（mode=1，active-backup）：此模式使用主被策略（热备）。在所有做bond1的物理网卡中，同一时刻只有一张网卡被激活，当且仅当活动网卡失效时才会激活其他的网卡。这种模式下做bond的两张或多张网卡的MAC地址和Bond虚拟网卡的MAC地址相同，而Bond的MAC地址是Bond创建启动后活动网卡（Active Slave）的MAC地址。这种模式要求主被网卡能快速的切换，即当主网卡出现故障后能迅速地切换至备用网卡。切换过程中，上层的应用几乎不受影响，因为Bond的驱动程序会临时接管上层应用的数据包，存放至数据缓冲区，等待备用网卡启动后再发送出去。但是如果切换时间过长，则会引起缓冲区的溢出，导致丢包。</p><h3 id="模式2"><a href="#模式2" class="headerlink" title="模式2"></a>模式2</h3><p>  模式2（mode=2，balance-xor）：xor为异或运算(二进制位相异为1，相同为0)。此模式的默认选择策略是：<br>选择网卡的序号=(源MAC地址 XOR 目标MAC地址) % Slave网卡（从网卡）的数量。<br>  其他的传输策略可以通过xmit_hash_policy配置项指定。</p><h3 id="模式3"><a href="#模式3" class="headerlink" title="模式3"></a>模式3</h3><p>  模式3（mode=3，broadcast）：使用广播策略，数据包会被广播至所有Slave网卡进行传送。</p><h3 id="模式4"><a href="#模式4" class="headerlink" title="模式4"></a>模式4</h3><p>  模式4（mode=4，802.3ad）：使用动态链接聚合策略，启动时会创建一个聚合组，所有Slave网卡共享同样的速率和双工设定。<br>必要条件：<br>  1．支持使用ethtool工具获取每个slave网卡的速率和双工设定；<br>  2．需要交换机支持IEEE 802.3ad 动态链路聚合（Dynamic link aggregation）模式</p><h3 id="模式5"><a href="#模式5" class="headerlink" title="模式5"></a>模式5</h3><p>  模式5（mode=5，balance-tlbtransmitload balancing）：基于每个slave网卡的速率选择传输网卡。<br>  必要条件：支持使用ethtool工具获取每个slave网卡的速率。</p><h3 id="模式6"><a href="#模式6" class="headerlink" title="模式6"></a>模式6</h3><p>  模式6（mode=6，balance-alb，Adaptive load balancing）：该模式包含了bond5模式，同时还支持对IPV4流量接收时的负载均衡策略(receive load balance, rlb)，而且不需要任何交换机的支持。<br>必要条件：</p><ol><li>ethtool支持获取每个slave的速率；</li><li>底层驱动支持设置某个网卡设备的硬件地址。</li></ol><h3 id="优劣比较"><a href="#优劣比较" class="headerlink" title="优劣比较"></a>优劣比较</h3><p>7中bond模式对比如下<br><img src="https://ljw.howieli.cn/blog/2017-5-27/bond.png"><br>  在7种模式中，最为常用的是bond0和bond1。在网络流量较大的场景下推荐使用bond0；在可靠性要求较高的场景下推荐使用bond1。</p><h3 id="实践操作"><a href="#实践操作" class="headerlink" title="实践操作"></a>实践操作</h3><p>下面以bond0为例，介绍一下bond的基本配置。具体步骤如下：</p><ol><li><p>配置前查看是否开启bond模块：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">modinfo bonding<br></code></pre></td></tr></table></figure></li><li><p>创建bond0网卡配置文件如下：  </p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs shell">vim /etc/sysconfig/network-scripts/ifcfg-bond0:<br>    DEVICE=bond0<br>    ONBOOT=yes #自动启动<br>    BOOTPROTO=dhcp #可以选择dhcp，static，none<br>    USERCTL=no #该设备只能由root控制<br>    NM_CONTROLLED=no #不需要重启网卡，实时生效<br>    TYPE=Ethernet    #如选DHCP则需要配置IP地址等信息<br>    #IPADDR=10.0.2.10<br>    #NETMASK=255.255.255.0<br>    #GATEWAY=10.0.2.1<br>    #BONDING_OPTS=&quot;mode=0 miimon=100fail_over_mac=1&quot;<br>    #如果使用了BONDING_OPTS选项，则不需要再使用/etc/modprobe.conf 配置文件对绑定设备进行配置。<br></code></pre></td></tr></table></figure></li><li><p>配置被bond的网卡。Bonding接口创建以后，被绑定的网卡必须在他们的设置文件里面添加MASTER和SLAVE两个参数。如eth0的配置如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs shell">vim /etc/sysconfig/network-scripts/ifcfg-eth0:<br>    DEVICE=eth0<br>    ONBOOT=yes<br>    BOOTPROTO=none<br>    USERCTL=no<br>    NM_CONTROLLED=no<br>    MASTER=bond0 #属于哪个bond<br>    SLAVE=yes #是否为从网卡，即是否被做bond<br></code></pre></td></tr></table></figure></li><li><p>创建并编辑<code>/etc/modprobe.d/bond.conf</code>文件，使系统在启动时加载bonding模块。添加：<br><code>alias bond0 bonding</code> //使系统在启动时加载bonding模块，对外虚拟网络接口设备为 bond0<br><code>options bond0 miimon=100 mode=0 fail_over_mac=1</code><br>其中miimon是用来进行链路监测的，其原理是检测网上的链路状态，一般将miimon值设为100，表示系统每100ms监测一次链路连接状态，如果有一条线路不通就转入另一条线路。bonding定义了网卡的4个链路状态：正常状态(BOND_LINK_UP)、网卡出现故障(BOND_LINK_FAIL)、失效状态(BOND_LINK_DOWN)和恢复状态(BOND_LINK_BACK)。<br>fail_over_mac默认等于0，当发生错误时只修改slave网卡的MAC地址而不修改bond的MAC地址；fail_over_mac=1时，当发生错误时只修改bond网卡的MAC地址而不修改slave网卡的MAC地址。这个选项只在虚拟机上进行测试时开启，如果使用物理机则不需配置。</p></li><li><p>重启机器，使用<code>cat /proc/net/bonding/bondX</code>命令查看bond配置是否生效。<br> 注：必须关闭NetworkManger服务，否则会和bond冲突</p></li></ol>]]></content>
    
    
    <categories>
      
      <category>Linux</category>
      
    </categories>
    
    
    <tags>
      
      <tag>linux</tag>
      
      <tag>bond</tag>
      
      <tag>centos</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Tmux Linux 分屏工具</title>
    <link href="/posts/Tmux-Linux-%E5%88%86%E5%B1%8F%E5%B7%A5%E5%85%B7/"/>
    <url>/posts/Tmux-Linux-%E5%88%86%E5%B1%8F%E5%B7%A5%E5%85%B7/</url>
    
    <content type="html"><![CDATA[<h3 id="tmux介绍"><a href="#tmux介绍" class="headerlink" title="tmux介绍"></a>tmux介绍</h3><a id="more"></a><p>tmux是指通过一个终端登录远程主机并运行后，在其中可以开启多个控制台的终端复用软件。<br>有一本关于Tmux的书tmux：<a href="https://www.amazon.com/tmux-Productive-Development-Brian-Hogan/dp/1934356964">Productive Mouse-Free Development</a><br>自己在centos7.2系统上简单的使用了一下</p><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> yum install tmux -y</span><br></code></pre></td></tr></table></figure><h3 id="基本操作"><a href="#基本操作" class="headerlink" title="基本操作"></a>基本操作</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs shell">Ctrl+b ? 显示快捷键帮助<br>Ctrl+b C-o 调换窗口位置，类似与vim 里的C-w<br>Ctrl+b 空格键 采用下一个内置布局<br>Ctrl+b ! 把当前窗口变为新窗口<br>Ctrl+b “ 横向分隔窗口<br>Ctrl+b % 纵向分隔窗口<br>Ctrl+b q 显示分隔窗口的编号<br>Ctrl+b o 跳到下一个分隔窗口<br>Ctrl+b 上下键 上一个及下一个分隔窗口<br>Ctrl+b C-方向键 调整分隔窗口大小<br>Ctrl+b c 创建新窗口<br>Ctrl+b 0~9 选择几号窗口<br>Ctrl+b c 创建新窗口<br>Ctrl+b n 选择下一个窗口<br>Ctrl+b l 切换到最后使用的窗口<br>Ctrl+b p 选择前一个窗口<br>Ctrl+b w 以菜单方式显示及选择窗口<br>Ctrl+b t 显示时钟<br>Ctrl+b ; 切换到最后一个使用的面板<br>Ctrl+b x 关闭面板<br>Ctrl+b &amp; 关闭窗口<br>Ctrl+b s 以菜单方式显示和选择会话<br>Ctrl+b d 退出tumx，并保存当前会话，这时，tmux仍在后台运行，可以通过tmux attach进入 到指定的会话<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>Linux</category>
      
    </categories>
    
    
    <tags>
      
      <tag>tmux</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>openstack实例启动流程</title>
    <link href="/posts/openstack%E5%AE%9E%E4%BE%8B%E5%90%AF%E5%8A%A8%E6%B5%81%E7%A8%8B/"/>
    <url>/posts/openstack%E5%AE%9E%E4%BE%8B%E5%90%AF%E5%8A%A8%E6%B5%81%E7%A8%8B/</url>
    
    <content type="html"><![CDATA[<h3 id="openstack实例启动流程"><a href="#openstack实例启动流程" class="headerlink" title="openstack实例启动流程"></a>openstack实例启动流程</h3><a id="more"></a><p><img src="https://ljw.howieli.cn/blog/2017-5-26/openstack.png"><br>1.客户端使用自己的用户名密码请求认证。<br>2.keystone通过查询在keystone的数据库user表中保存了user的相关信息，包括password加密后的hash值，并返回一个token_id（令牌），和serviceCatalog(一些服务的endpoint地址，cinder、glance-api后面下载镜像和创建块存储时会用到)。<br>3.客户端带上keystone返回的token_id和创建虚机的相关参数，Post请求nova-api创建虚拟机<br>4.nova-api接收到请求后，首先使用请求携带的token_id来访问该api，以验证请求是否有效。<br>5.keystone验证通过后返回更新后的认证信息。<br>6.nova api检查创建虚拟机参数是否有效与合法。<br>检查虚拟机name是否符合命名规范，flavor_id是否在数据库中存在，image_uuid是否是正确的uuid格式<br>检查instance、vcpu、ram的数量是否超过配额。<br>7.当且仅当所有传参都有效合法时，更新nova数据库，新建一条instance记录，vm_states设为BUILDING，task_state设为SCHEDULING.<br>8.nova api 远程调用传递请求、参数给nova scheduler，把消息“请给我创建一台虚拟机”丢到消息队列，然后定期查询虚机的状态。<br>9.nova scheduler从queue中获取到这条消息<br>10.nova scheduler访问nova 数据库，通过调度算法，过滤出一些合适的计算节点，然后进行排序。<br>11.更新虚机节点信息，返回一个最优节点id给nova scheduler。<br>12.nova scheduler选定host之后，通过rpc调用nova-compute服务，把“创建虚机请求”消息丢个mq。<br>13.nova compute收到创建虚拟机请求的消息<br>nova-compute有个定时任务，定期从数据库中查找到运行在该节点上的所有虚拟机信息，统计得到空闲内存大小和空闲磁盘大小。然后更新数据库compute_node信息，以保证调度的准确性。<br>14.nova compute通过rpc查询nova数据库中虚机的信息例如主机模板和id<br>15.nova conductor从消息队列中拿到请求查询数据库<br>16.nova conductor查询nova数据库<br>17.数据库返回虚机信息<br>18.nova compute从消息队列中获取信息。<br>19.nova compute 请求glance 的rest api，下载所需要的镜像，一般是qcow2的。<br>20.glance api 也会去验证请求的token的有效性。<br>21.glance api 返回镜像信息给nova-compute。<br>22.同理，nova compute请求neutron api配置网络，例如获取虚机ip地址<br>23.验证token的有效性<br>24.neutron返回网络信息<br>25-27 同glance、neutron验证token返回块设备信息<br>28.据上面配置的虚拟机信息，生成xml，写入libvirt,xml文件，然后调用libvirt driver去使用libvirt.xml文件启动虚拟机。</p>]]></content>
    
    
    <categories>
      
      <category>openstack</category>
      
    </categories>
    
    
    <tags>
      
      <tag>openstack</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>python环境准备</title>
    <link href="/posts/python%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87/"/>
    <url>/posts/python%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87/</url>
    
    <content type="html"><![CDATA[<h3 id="pyenv配置"><a href="#pyenv配置" class="headerlink" title="pyenv配置"></a>pyenv配置</h3><a id="more"></a><ol><li>安装git <code>yum -y install git</code></li><li>安装pyenv <code>curl -L https://raw.githubusercontent.com/yyuu/pyenv-installer/master/bin/pyenv-installer | bash</code></li><li>配置环境变量，在<code>~/.bash_profile</code>里增加。 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">export</span> PATH=<span class="hljs-string">&quot;~/.pyenv/bin:<span class="hljs-variable">$PATH</span>&quot;</span><br><span class="hljs-built_in">eval</span> <span class="hljs-string">&quot;<span class="hljs-subst">$(pyenv init -)</span>&quot;</span><br><span class="hljs-built_in">eval</span> <span class="hljs-string">&quot;<span class="hljs-subst">$(pyenv virtualenv-init -)</span>&quot;</span><br></code></pre></td></tr></table></figure></li><li>安装编译工具<code>yum -y install gcc make patch</code></li></ol>]]></content>
    
    
    <categories>
      
      <category>python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>python</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
