<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>李家旺</title>
  
  <subtitle>学习日记</subtitle>
  <link href="https://www.lijiawang.org/atom.xml" rel="self"/>
  
  <link href="https://www.lijiawang.org/"/>
  <updated>2019-11-08T15:47:10.980Z</updated>
  <id>https://www.lijiawang.org/</id>
  
  <author>
    <name>李家旺</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>openstack instance vm_power_state 状态置为 NOSTATE</title>
    <link href="https://www.lijiawang.org/openstack%20instance%20vm_power_state%20%E7%8A%B6%E6%80%81%E7%BD%AE%E4%B8%BA%20NOSTATE"/>
    <id>https://www.lijiawang.org/openstack%20instance%20vm_power_state%20%E7%8A%B6%E6%80%81%E7%BD%AE%E4%B8%BA%20NOSTATE</id>
    <published>2019-11-08T15:37:21.000Z</published>
    <updated>2019-11-08T15:47:10.980Z</updated>
    
    <content type="html"><![CDATA[<h3 id="openstack-上的instance运行正常，但是power-state状态为NOSTATE，导致instance热迁移失败"><a href="#openstack-上的instance运行正常，但是power-state状态为NOSTATE，导致instance热迁移失败" class="headerlink" title="openstack 上的instance运行正常，但是power_state状态为NOSTATE，导致instance热迁移失败"></a>openstack 上的instance运行正常，但是power_state状态为NOSTATE，导致instance热迁移失败</h3><h3 id="现象"><a href="#现象" class="headerlink" title="现象"></a>现象</h3><p>1、再一次热迁移时候发现迁移时候报以下错误</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># openstack server migrate --live node-53 86487ef4-cc12-4be6-995e-46f5ac093901</span><br><span class="line">Cannot &#39;os-migrateLive&#39; instance 86487ef4-cc12-4be6-995e-46f5ac093901 while it is in power_state 0 (HTTP 409) (Request-ID: req-6c14e0ee-c3df-42de-873d-9ecc8ad215cc)</span><br></pre></td></tr></table></figure><p>2、查看实例，发现虚拟机运行正常，但是 power_state 为 NOSTATE</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"># openstack server show 86487ef4-cc12-4be6-995e-46f5ac093901</span><br><span class="line">+-------------------------------------+----------------------------------------------------------------+</span><br><span class="line">| Field                               | Value                                                          |</span><br><span class="line">+-------------------------------------+----------------------------------------------------------------+</span><br><span class="line">| OS-DCF:diskConfig                   | MANUAL                                                         |</span><br><span class="line">| OS-EXT-AZ:availability_zone         | m_cpu+san                                                      |</span><br><span class="line">| OS-EXT-SRV-ATTR:host                | node-27                                                        |</span><br><span class="line">| OS-EXT-SRV-ATTR:hypervisor_hostname | node-27                                                        |</span><br><span class="line">| OS-EXT-SRV-ATTR:instance_name       | instance-000071aa                                              |</span><br><span class="line">| OS-EXT-STS:power_state              | NOSTATE                                                        |</span><br><span class="line">| OS-EXT-STS:task_state               | None                                                           |</span><br><span class="line">| OS-EXT-STS:vm_state                 | active                                                         |</span><br><span class="line">| OS-SRV-USG:launched_at              | 2018-09-21T05:40:36.000000                                     |</span><br><span class="line">| OS-SRV-USG:terminated_at            | None                                                           |</span><br><span class="line">| accessIPv4                          |                                                                |</span><br><span class="line">| accessIPv6                          |                                                                |</span><br><span class="line">| addresses                           | vlan_10.122.44.0&#x2F;23&#x3D;10.122.45.53                               |</span><br><span class="line">| config_drive                        | True                                                           |</span><br><span class="line">| created                             | 2018-09-21T05:40:31Z                                           |</span><br><span class="line">| flavor                              | 16-64-100 (1cbe4ea1-8a67-4027-afd4-8f31a8b94851)               |</span><br><span class="line">| hostId                              | 2730fa3d62e347ddb67e155f6eed973787a868c82316f7e6ba641b10       |</span><br><span class="line">| id                                  | 86487ef4-cc12-4be6-995e-46f5ac093901                           |</span><br><span class="line">| image                               | lenovo-centos-7-release (b9f8f864-4217-4ac6-a116-62ecfa0fc074) |</span><br><span class="line">| key_name                            | None                                                           |</span><br><span class="line">| name                                | SLP3YM7HRCX                                                    |</span><br><span class="line">| progress                            | 100                                                            |</span><br><span class="line">| project_id                          | e992715df18a417997c068e5f9834b0f                               |</span><br><span class="line">| properties                          |                                                                |</span><br><span class="line">| security_groups                     | name&#x3D;&#39;default&#39;                                                 |</span><br><span class="line">| status                              | ACTIVE                                                         |</span><br><span class="line">| updated                             | 2019-10-09T07:58:58Z                                           |</span><br><span class="line">| user_id                             | af518aeb935c4d258f8cb7d302c83797                               |</span><br><span class="line">| volumes_attached                    | id&#x3D;&#39;a821856b-409f-4e6e-becd-eb4b2344c7d8&#39;                      |</span><br><span class="line">+-------------------------------------+----------------------------------------------------------------+</span><br></pre></td></tr></table></figure><h3 id="查找原因"><a href="#查找原因" class="headerlink" title="查找原因"></a>查找原因</h3><p>列出该虚拟机的历史操作</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"># nova instance-action-list 86487ef4-cc12-4be6-995e-46f5ac093901</span><br><span class="line">+----------------+------------------------------------------+---------+----------------------------+</span><br><span class="line">| Action         | Request_ID                               | Message | Start_Time                 |</span><br><span class="line">+----------------+------------------------------------------+---------+----------------------------+</span><br><span class="line">| create         | req-4d01a466-ce7a-48a9-b575-e129b61bcc30 | -       | 2018-09-21T05:40:30.000000 |</span><br><span class="line">| live-migration | req-2d93bc73-e43f-4ca6-88fc-a6ad1a4021a6 | -       | 2018-09-21T05:44:47.000000 |</span><br><span class="line">| live-migration | req-7fcbfb15-7e3c-471d-8ea5-020d438d23c3 | -       | 2019-03-24T13:00:30.000000 |</span><br><span class="line">| live-migration | req-2b009c4f-caef-4d90-81fd-d480c1b9efad | Error   | 2019-10-09T07:00:13.000000 |</span><br><span class="line">| live-migration | req-b364724b-6b28-4d5f-8f14-05d1c4abeceb | Error   | 2019-10-09T07:07:37.000000 |</span><br><span class="line">| live-migration | req-8669d3b1-e57e-4b35-8d40-a110b64f47c8 | Error   | 2019-10-09T07:29:13.000000 |</span><br><span class="line">| live-migration | req-ecec98ca-3777-4ca3-afcd-83e36381e038 | Error   | 2019-10-09T07:55:12.000000 |</span><br><span class="line">| live-migration | req-6378f839-89c0-43c0-8a65-40e5b775283c | Error   | 2019-10-09T07:58:14.000000 |</span><br><span class="line">| live-migration | req-e966caa7-06e5-4a29-a371-517d98c37121 | Error   | 2019-10-09T07:58:58.000000 |</span><br><span class="line">| live-migration | req-6c14e0ee-c3df-42de-873d-9ecc8ad215cc | Error   | 2019-10-09T08:31:47.000000 |</span><br><span class="line">+----------------+------------------------------------------+---------+----------------------------+</span><br></pre></td></tr></table></figure><p>列出该实例的迁移记录</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br></pre></td><td class="code"><pre><span class="line"># nova  migration-list|grep 86487ef4-cc12-4be6-995e-46f5ac093901</span><br><span class="line">| 84892 | -           | -         | node-27        | node-29            | -            | error         | 86487ef4-cc12-4be6-995e-46f5ac093901 | 189        | 189        | 2019-10-09T07:07:37.000000 | 2019-10-09T07:07:37.000000 | live-migration |</span><br><span class="line">| 54307 | -           | -         | node-27        | node-53            | -            | error         | 86487ef4-cc12-4be6-995e-46f5ac093901 | 189        | 189        | 2019-03-24T13:00:30.000000 | 2019-03-24T17:10:10.000000 | live-migration |</span><br><span class="line">| 84904 | -           | -         | node-27        | node-53            | -            | error         | 86487ef4-cc12-4be6-995e-46f5ac093901 | 189        | 189        | 2019-10-09T07:29:13.000000 | 2019-10-09T07:29:13.000000 | live-migration |</span><br><span class="line">| 84913 | -           | -         | node-27        | node-53            | -            | error         | 86487ef4-cc12-4be6-995e-46f5ac093901 | 189        | 189        | 2019-10-09T07:55:12.000000 | 2019-10-09T07:55:12.000000 | live-migration |</span><br><span class="line">| 84919 | -           | -         | node-27        | node-53            | -            | error         | 86487ef4-cc12-4be6-995e-46f5ac093901 | 189        | 189        | 2019-10-09T07:58:14.000000 | 2019-10-09T07:58:14.000000 | live-migration |</span><br><span class="line">| 84922 | -           | -         | node-27        | node-53            | -            | error         | 86487ef4-cc12-4be6-995e-46f5ac093901 | 189        | 189        | 2019-10-09T07:58:58.000000 | 2019-10-09T07:58:58.000000 | live-migration |</span><br><span class="line">| 84931 | -           | -         | node-27        | node-53            | -            | error         | 86487ef4-cc12-4be6-995e-46f5ac093901 | 189        | 189        | 2019-10-09T08:31:47.000000 | 2019-10-09T08:31:47.000000 | live-migration |</span><br><span class="line">| 84889 | -           | -         | node-27        | node-89            | -            | error         | 86487ef4-cc12-4be6-995e-46f5ac093901 | 189        | 189        | 2019-10-09T07:00:13.000000 | 2019-10-09T07:00:13.000000 | live-migration |</span><br><span class="line">| 42241 | -           | -         | node-33        | node-27            | -            | completed     | 86487ef4-cc12-4be6-995e-46f5ac093901 | 189        | 189        | 2018-09-21T05:44:48.000000 | 2018-09-21T05:45:13.000000 | live-migration |</span><br><span class="line">可以看到该实例从node-27节点迁移到node-85、node-53、node-29节点均为迁移成功</span><br><span class="line">查看instance所在的节点</span><br><span class="line"># ssh node-27</span><br><span class="line"></span><br><span class="line">root@node-27:~# virsh list --all|grep instance-000071aa </span><br><span class="line">root@node-27:~# exit</span><br><span class="line">logout</span><br><span class="line">Connection to node-27 closed.</span><br><span class="line"># ssh node-53</span><br><span class="line"></span><br><span class="line">root@node-53:~#  virsh list --all|grep instance-000071aa</span><br><span class="line"> 91    instance-000071aa              running</span><br><span class="line">root@node-53:~# exit</span><br><span class="line">logout</span><br><span class="line">Connection to node-53 closed.</span><br><span class="line"># openstack server show 86487ef4-cc12-4be6-995e-46f5ac093901</span><br><span class="line">+-------------------------------------+----------------------------------------------------------------+</span><br><span class="line">| Field                               | Value                                                          |</span><br><span class="line">+-------------------------------------+----------------------------------------------------------------+</span><br><span class="line">| OS-DCF:diskConfig                   | MANUAL                                                         |</span><br><span class="line">| OS-EXT-AZ:availability_zone         | m_cpu+san                                                      |</span><br><span class="line">| OS-EXT-SRV-ATTR:host                | node-27                                                        |</span><br><span class="line">| OS-EXT-SRV-ATTR:hypervisor_hostname | node-27                                                        |</span><br><span class="line">| OS-EXT-SRV-ATTR:instance_name       | instance-000071aa                                              |</span><br><span class="line">| OS-EXT-STS:power_state              | NOSTATE                                                        |</span><br><span class="line">| OS-EXT-STS:task_state               | None                                                           |</span><br><span class="line">| OS-EXT-STS:vm_state                 | active                                                         |</span><br><span class="line">| OS-SRV-USG:launched_at              | 2018-09-21T05:40:36.000000                                     |</span><br><span class="line">| OS-SRV-USG:terminated_at            | None                                                           |</span><br><span class="line">| accessIPv4                          |                                                                |</span><br><span class="line">| accessIPv6                          |                                                                |</span><br><span class="line">| addresses                           | vlan_10.122.44.0&#x2F;23&#x3D;10.122.45.53                               |</span><br><span class="line">| config_drive                        | True                                                           |</span><br><span class="line">| created                             | 2018-09-21T05:40:31Z                                           |</span><br><span class="line">| flavor                              | 16-64-100 (1cbe4ea1-8a67-4027-afd4-8f31a8b94851)               |</span><br><span class="line">| hostId                              | 2730fa3d62e347ddb67e155f6eed973787a868c82316f7e6ba641b10       |</span><br><span class="line">| id                                  | 86487ef4-cc12-4be6-995e-46f5ac093901                           |</span><br><span class="line">| image                               | lenovo-centos-7-release (b9f8f864-4217-4ac6-a116-62ecfa0fc074) |</span><br><span class="line">| key_name                            | None                                                           |</span><br><span class="line">| name                                | SLP3YM7HRCX                                                    |</span><br><span class="line">| progress                            | 100                                                            |</span><br><span class="line">| project_id                          | e992715df18a417997c068e5f9834b0f                               |</span><br><span class="line">| properties                          |                                                                |</span><br><span class="line">| security_groups                     | name&#x3D;&#39;default&#39;                                                 |</span><br><span class="line">| status                              | ACTIVE                                                         |</span><br><span class="line">| updated                             | 2019-10-09T08:31:47Z                                           |</span><br><span class="line">| user_id                             | af518aeb935c4d258f8cb7d302c83797                               |</span><br><span class="line">| volumes_attached                    | id&#x3D;&#39;a821856b-409f-4e6e-becd-eb4b2344c7d8&#39;                      |</span><br><span class="line">+-------------------------------------+----------------------------------------------------------------+</span><br><span class="line">root@node-1:~# mysql -unova -pnova</span><br><span class="line">MariaDB [(none)]&gt; select * from nova.instances where uuid&#x3D;&#39;86487ef4-cc12-4be6-995e-46f5ac093901&#39;\G;</span><br><span class="line">*************************** 1. row ***************************</span><br><span class="line">              created_at: 2018-09-21 05:40:31</span><br><span class="line">              updated_at: 2019-10-09 08:31:47</span><br><span class="line">              deleted_at: NULL</span><br><span class="line">                      id: 29098</span><br><span class="line">             internal_id: NULL</span><br><span class="line">                 user_id: af518aeb935c4d258f8cb7d302c83797</span><br><span class="line">              project_id: e992715df18a417997c068e5f9834b0f</span><br><span class="line">               image_ref: b9f8f864-4217-4ac6-a116-62ecfa0fc074</span><br><span class="line">               kernel_id: </span><br><span class="line">              ramdisk_id: </span><br><span class="line">            launch_index: 0</span><br><span class="line">                key_name: NULL</span><br><span class="line">                key_data: NULL</span><br><span class="line">             power_state: 0</span><br><span class="line">                vm_state: active</span><br><span class="line">               memory_mb: 65536</span><br><span class="line">                   vcpus: 16</span><br><span class="line">                hostname: slp3ym7hrcx</span><br><span class="line">                    host: node-27</span><br><span class="line">               user_data: NULL</span><br><span class="line">          reservation_id: r-o8dr8ibn</span><br><span class="line">             launched_at: 2018-09-21 05:40:36</span><br><span class="line">           terminated_at: NULL</span><br><span class="line">            display_name: SLP3YM7HRCX</span><br><span class="line">     display_description: SLP3YM7HRCX</span><br><span class="line">       availability_zone: no_san</span><br><span class="line">                  locked: 0</span><br><span class="line">                 os_type: NULL</span><br><span class="line">             launched_on: node-33</span><br><span class="line">        instance_type_id: 189</span><br><span class="line">                 vm_mode: NULL</span><br><span class="line">                    uuid: 86487ef4-cc12-4be6-995e-46f5ac093901</span><br><span class="line">            architecture: NULL</span><br><span class="line">        root_device_name: &#x2F;dev&#x2F;vda</span><br><span class="line">            access_ip_v4: NULL</span><br><span class="line">            access_ip_v6: NULL</span><br><span class="line">            config_drive: True</span><br><span class="line">              task_state: NULL</span><br><span class="line">default_ephemeral_device: NULL</span><br><span class="line">     default_swap_device: NULL</span><br><span class="line">                progress: 100</span><br><span class="line">        auto_disk_config: 0</span><br><span class="line">      shutdown_terminate: 0</span><br><span class="line">       disable_terminate: 0</span><br><span class="line">                 root_gb: 100</span><br><span class="line">            ephemeral_gb: 0</span><br><span class="line">               cell_name: NULL</span><br><span class="line">                    node: node-27</span><br><span class="line">                 deleted: 0</span><br><span class="line">               locked_by: NULL</span><br><span class="line">                 cleaned: 1</span><br><span class="line">      ephemeral_key_uuid: NULL</span><br><span class="line">1 row in set (0.00 sec)</span><br><span class="line"></span><br><span class="line">ERROR: No query specified</span><br></pre></td></tr></table></figure><p>发现实际instance启动在node-53节点上，但是数据库记录在node-27节点上</p><p>manager.py 捕获到 InstanceNotFound 设置 vm_power_state=NOSTATE</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">/usr/lib/python2<span class="number">.7</span>/dist-packages/nova/compute/manager.py</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_query_driver_power_state_and_sync</span>(<span class="params">self, context, db_instance</span>):</span></span><br><span class="line">        <span class="keyword">if</span> db_instance.task_state <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            LOG.info(_LI(<span class="string">&quot;During sync_power_state the instance has a &quot;</span></span><br><span class="line">                         <span class="string">&quot;pending task (%(task)s). Skip.&quot;</span>),</span><br><span class="line">                     &#123;<span class="string">&#x27;task&#x27;</span>: db_instance.task_state&#125;, instance=db_instance)</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        <span class="comment"># No pending tasks. Now try to figure out the real vm_power_state.</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            vm_instance = self.driver.get_info(db_instance)</span><br><span class="line">            vm_power_state = vm_instance.state</span><br><span class="line">        <span class="keyword">except</span> exception.InstanceNotFound:         <span class="comment"># 可以看到如果InstanceNotFound</span></span><br><span class="line">            vm_power_state = power_state.NOSTATE   <span class="comment"># power_state设置为NOSTATE</span></span><br><span class="line">        <span class="comment"># Note(maoy): the above get_info call might take a long time,</span></span><br><span class="line">        <span class="comment"># for example, because of a broken libvirt driver.</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            self._sync_instance_power_state(context,</span><br><span class="line">                                            db_instance,</span><br><span class="line">                                            vm_power_state,</span><br><span class="line">                                            use_slave=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">except</span> exception.InstanceNotFound:</span><br><span class="line">            <span class="comment"># NOTE(hanlind): If the instance gets deleted during sync,</span></span><br><span class="line">            <span class="comment"># silently ignore.</span></span><br><span class="line">            <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><h3 id="得到原因"><a href="#得到原因" class="headerlink" title="得到原因"></a>得到原因</h3><p>由于之前 migrate ERROR 导致 instance 实际启动的节点和数据库记录的节点不一致，comoute 通过 manager.py 捕获到 InstanceNotFound， 所以把 vm_power_state 状态置为 NOSTATE</p><h3 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h3><p>更新数据库，将数据库中的instance对应的host、node更新成实际instance启动的节点信息。硬重启虚拟机</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br></pre></td><td class="code"><pre><span class="line">root@node-1:~# mysql -unova -pnova</span><br><span class="line">MariaDB [(none)]&gt; update nova.instances set host&#x3D;&#39;node-53&#39; where uuid&#x3D;&#39;86487ef4-cc12-4be6-995e-46f5ac093901&#39;\G;</span><br><span class="line">Query OK, 1 row affected (0.00 sec)</span><br><span class="line">Rows matched: 1  Changed: 1  Warnings: 0</span><br><span class="line"></span><br><span class="line">ERROR: No query specified</span><br><span class="line"></span><br><span class="line">MariaDB [(none)]&gt; update nova.instances set node&#x3D;&#39;node-53&#39; where uuid&#x3D;&#39;86487ef4-cc12-4be6-995e-46f5ac093901&#39;\G;</span><br><span class="line">Query OK, 1 row affected (0.00 sec)</span><br><span class="line">Rows matched: 1  Changed: 1  Warnings: 0</span><br><span class="line"></span><br><span class="line">ERROR: No query specified</span><br><span class="line"></span><br><span class="line">MariaDB [(none)]&gt; select * from nova.instances where uuid&#x3D;&#39;86487ef4-cc12-4be6-995e-46f5ac093901&#39;\G;</span><br><span class="line">*************************** 1. row ***************************</span><br><span class="line">              created_at: 2018-09-21 05:40:31</span><br><span class="line">              updated_at: 2019-10-09 08:31:47</span><br><span class="line">              deleted_at: NULL</span><br><span class="line">                      id: 29098</span><br><span class="line">             internal_id: NULL</span><br><span class="line">                 user_id: af518aeb935c4d258f8cb7d302c83797</span><br><span class="line">              project_id: e992715df18a417997c068e5f9834b0f</span><br><span class="line">               image_ref: b9f8f864-4217-4ac6-a116-62ecfa0fc074</span><br><span class="line">               kernel_id: </span><br><span class="line">              ramdisk_id: </span><br><span class="line">            launch_index: 0</span><br><span class="line">                key_name: NULL</span><br><span class="line">                key_data: NULL</span><br><span class="line">             power_state: 0</span><br><span class="line">                vm_state: active</span><br><span class="line">               memory_mb: 65536</span><br><span class="line">                   vcpus: 16</span><br><span class="line">                hostname: slp3ym7hrcx</span><br><span class="line">                    host: node-53</span><br><span class="line">               user_data: NULL</span><br><span class="line">          reservation_id: r-o8dr8ibn</span><br><span class="line">             launched_at: 2018-09-21 05:40:36</span><br><span class="line">           terminated_at: NULL</span><br><span class="line">            display_name: SLP3YM7HRCX</span><br><span class="line">     display_description: SLP3YM7HRCX</span><br><span class="line">       availability_zone: no_san</span><br><span class="line">                  locked: 0</span><br><span class="line">                 os_type: NULL</span><br><span class="line">             launched_on: node-33</span><br><span class="line">        instance_type_id: 189</span><br><span class="line">                 vm_mode: NULL</span><br><span class="line">                    uuid: 86487ef4-cc12-4be6-995e-46f5ac093901</span><br><span class="line">            architecture: NULL</span><br><span class="line">        root_device_name: &#x2F;dev&#x2F;vda</span><br><span class="line">            access_ip_v4: NULL</span><br><span class="line">            access_ip_v6: NULL</span><br><span class="line">            config_drive: True</span><br><span class="line">              task_state: NULL</span><br><span class="line">default_ephemeral_device: NULL</span><br><span class="line">     default_swap_device: NULL</span><br><span class="line">                progress: 100</span><br><span class="line">        auto_disk_config: 0</span><br><span class="line">      shutdown_terminate: 0</span><br><span class="line">       disable_terminate: 0</span><br><span class="line">                 root_gb: 100</span><br><span class="line">            ephemeral_gb: 0</span><br><span class="line">               cell_name: NULL</span><br><span class="line">                    node: node-53</span><br><span class="line">                 deleted: 0</span><br><span class="line">               locked_by: NULL</span><br><span class="line">                 cleaned: 1</span><br><span class="line">      ephemeral_key_uuid: NULL</span><br><span class="line">1 row in set (0.00 sec)</span><br><span class="line"></span><br><span class="line">ERROR: No query specified</span><br><span class="line"></span><br><span class="line">MariaDB [(none)]&gt; exit</span><br><span class="line">Bye</span><br><span class="line"># openstack server show 86487ef4-cc12-4be6-995e-46f5ac093901</span><br><span class="line">+-------------------------------------+----------------------------------------------------------------+</span><br><span class="line">| Field                               | Value                                                          |</span><br><span class="line">+-------------------------------------+----------------------------------------------------------------+</span><br><span class="line">| OS-DCF:diskConfig                   | MANUAL                                                         |</span><br><span class="line">| OS-EXT-AZ:availability_zone         | no_san                                                         |</span><br><span class="line">| OS-EXT-SRV-ATTR:host                | node-53                                                        |</span><br><span class="line">| OS-EXT-SRV-ATTR:hypervisor_hostname | node-53                                                        |</span><br><span class="line">| OS-EXT-SRV-ATTR:instance_name       | instance-000071aa                                              |</span><br><span class="line">| OS-EXT-STS:power_state              | NOSTATE                                                        |</span><br><span class="line">| OS-EXT-STS:task_state               | None                                                           |</span><br><span class="line">| OS-EXT-STS:vm_state                 | active                                                         |</span><br><span class="line">| OS-SRV-USG:launched_at              | 2018-09-21T05:40:36.000000                                     |</span><br><span class="line">| OS-SRV-USG:terminated_at            | None                                                           |</span><br><span class="line">| accessIPv4                          |                                                                |</span><br><span class="line">| accessIPv6                          |                                                                |</span><br><span class="line">| addresses                           | vlan_10.122.44.0&#x2F;23&#x3D;10.122.45.53                               |</span><br><span class="line">| config_drive                        | True                                                           |</span><br><span class="line">| created                             | 2018-09-21T05:40:31Z                                           |</span><br><span class="line">| flavor                              | 16-64-100 (1cbe4ea1-8a67-4027-afd4-8f31a8b94851)               |</span><br><span class="line">| hostId                              | f5274ebe433297bc376a4ce09151591735a04d2b6762280866de2729       |</span><br><span class="line">| id                                  | 86487ef4-cc12-4be6-995e-46f5ac093901                           |</span><br><span class="line">| image                               | lenovo-centos-7-release (b9f8f864-4217-4ac6-a116-62ecfa0fc074) |</span><br><span class="line">| key_name                            | None                                                           |</span><br><span class="line">| name                                | SLP3YM7HRCX                                                    |</span><br><span class="line">| progress                            | 100                                                            |</span><br><span class="line">| project_id                          | e992715df18a417997c068e5f9834b0f                               |</span><br><span class="line">| properties                          |                                                                |</span><br><span class="line">| security_groups                     | name&#x3D;&#39;default&#39;                                                 |</span><br><span class="line">| status                              | ACTIVE                                                         |</span><br><span class="line">| updated                             | 2019-10-09T08:31:47Z                                           |</span><br><span class="line">| user_id                             | af518aeb935c4d258f8cb7d302c83797                               |</span><br><span class="line">| volumes_attached                    | id&#x3D;&#39;a821856b-409f-4e6e-becd-eb4b2344c7d8&#39;                      |</span><br><span class="line">+-------------------------------------+----------------------------------------------------------------+</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># nova reboot --hard 86487ef4-cc12-4be6-995e-46f5ac093901</span><br><span class="line"></span><br><span class="line"># openstack  server show 86487ef4-cc12-4be6-995e-46f5ac093901</span><br><span class="line">+-------------------------------------+----------------------------------------------------------------+</span><br><span class="line">| Field                               | Value                                                          |</span><br><span class="line">+-------------------------------------+----------------------------------------------------------------+</span><br><span class="line">| OS-DCF:diskConfig                   | MANUAL                                                         |</span><br><span class="line">| OS-EXT-AZ:availability_zone         | no_san                                                         |</span><br><span class="line">| OS-EXT-SRV-ATTR:host                | node-53                                                        |</span><br><span class="line">| OS-EXT-SRV-ATTR:hypervisor_hostname | node-53                                                        |</span><br><span class="line">| OS-EXT-SRV-ATTR:instance_name       | instance-000071aa                                              |</span><br><span class="line">| OS-EXT-STS:power_state              | Running                                                        |</span><br><span class="line">| OS-EXT-STS:task_state               | None                                                           |</span><br><span class="line">| OS-EXT-STS:vm_state                 | active                                                         |</span><br><span class="line">| OS-SRV-USG:launched_at              | 2018-09-21T05:40:36.000000                                     |</span><br><span class="line">| OS-SRV-USG:terminated_at            | None                                                           |</span><br><span class="line">| accessIPv4                          |                                                                |</span><br><span class="line">| accessIPv6                          |                                                                |</span><br><span class="line">| addresses                           | vlan_10.122.44.0&#x2F;23&#x3D;10.122.45.53                               |</span><br><span class="line">| config_drive                        | True                                                           |</span><br><span class="line">| created                             | 2018-09-21T05:40:31Z                                           |</span><br><span class="line">| flavor                              | 16-64-100 (1cbe4ea1-8a67-4027-afd4-8f31a8b94851)               |</span><br><span class="line">| hostId                              | 4bfe001f6fa4d443f283121b56139ef8bfbff47be8106387cc19edc6       |</span><br><span class="line">| id                                  | 86487ef4-cc12-4be6-995e-46f5ac093901                           |</span><br><span class="line">| image                               | lenovo-centos-7-release (b9f8f864-4217-4ac6-a116-62ecfa0fc074) |</span><br><span class="line">| key_name                            | None                                                           |</span><br><span class="line">| name                                | SLP3YM7HRCX                                                    |</span><br><span class="line">| progress                            | 0                                                              |</span><br><span class="line">| project_id                          | e992715df18a417997c068e5f9834b0f                               |</span><br><span class="line">| properties                          |                                                                |</span><br><span class="line">| security_groups                     | name&#x3D;&#39;default&#39;                                                 |</span><br><span class="line">| status                              | ACTIVE                                                         |</span><br><span class="line">| updated                             | 2019-10-09T10:04:19Z                                           |</span><br><span class="line">| user_id                             | af518aeb935c4d258f8cb7d302c83797                               |</span><br><span class="line">| volumes_attached                    | id&#x3D;&#39;a821856b-409f-4e6e-becd-eb4b2344c7d8&#39;                      |</span><br><span class="line">|                                     | id&#x3D;&#39;ff7d2dad-1954-4906-ae18-95c76a09b442&#39;                      |</span><br><span class="line">+-------------------------------------+----------------------------------------------------------------+</span><br><span class="line"></span><br><span class="line"># openstack server migrate --live node-89 86487ef4-cc12-4be6-995e-46f5ac093901</span><br><span class="line"></span><br><span class="line"># openstack  server show 86487ef4-cc12-4be6-995e-46f5ac093901</span><br><span class="line">+-------------------------------------+----------------------------------------------------------------+</span><br><span class="line">| Field                               | Value                                                          |</span><br><span class="line">+-------------------------------------+----------------------------------------------------------------+</span><br><span class="line">| OS-DCF:diskConfig                   | MANUAL                                                         |</span><br><span class="line">| OS-EXT-AZ:availability_zone         | Contingency                                                    |</span><br><span class="line">| OS-EXT-SRV-ATTR:host                | node-89                                                        |</span><br><span class="line">| OS-EXT-SRV-ATTR:hypervisor_hostname | node-89                                                        |</span><br><span class="line">| OS-EXT-SRV-ATTR:instance_name       | instance-000071aa                                              |</span><br><span class="line">| OS-EXT-STS:power_state              | Running                                                        |</span><br><span class="line">| OS-EXT-STS:task_state               | None                                                           |</span><br><span class="line">| OS-EXT-STS:vm_state                 | active                                                         |</span><br><span class="line">| OS-SRV-USG:launched_at              | 2018-09-21T05:40:36.000000                                     |</span><br><span class="line">| OS-SRV-USG:terminated_at            | None                                                           |</span><br><span class="line">| accessIPv4                          |                                                                |</span><br><span class="line">| accessIPv6                          |                                                                |</span><br><span class="line">| addresses                           | vlan_10.122.44.0&#x2F;23&#x3D;10.122.45.53                               |</span><br><span class="line">| config_drive                        | True                                                           |</span><br><span class="line">| created                             | 2018-09-21T05:40:31Z                                           |</span><br><span class="line">| flavor                              | 16-64-100 (1cbe4ea1-8a67-4027-afd4-8f31a8b94851)               |</span><br><span class="line">| hostId                              | 4bfe001f6fa4d443f283121b56139ef8bfbff47be8106387cc19edc6       |</span><br><span class="line">| id                                  | 86487ef4-cc12-4be6-995e-46f5ac093901                           |</span><br><span class="line">| image                               | lenovo-centos-7-release (b9f8f864-4217-4ac6-a116-62ecfa0fc074) |</span><br><span class="line">| key_name                            | None                                                           |</span><br><span class="line">| name                                | SLP3YM7HRCX                                                    |</span><br><span class="line">| progress                            | 0                                                              |</span><br><span class="line">| project_id                          | e992715df18a417997c068e5f9834b0f                               |</span><br><span class="line">| properties                          |                                                                |</span><br><span class="line">| security_groups                     | name&#x3D;&#39;default&#39;                                                 |</span><br><span class="line">| status                              | ACTIVE                                                         |</span><br><span class="line">| updated                             | 2019-10-09T10:04:19Z                                           |</span><br><span class="line">| user_id                             | af518aeb935c4d258f8cb7d302c83797                               |</span><br><span class="line">| volumes_attached                    | id&#x3D;&#39;a821856b-409f-4e6e-becd-eb4b2344c7d8&#39;                      |</span><br><span class="line">|                                     | id&#x3D;&#39;ff7d2dad-1954-4906-ae18-95c76a09b442&#39;                      |</span><br><span class="line">+-------------------------------------+----------------------------------------------------------------+</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;openstack-上的instance运行正常，但是power-state状态为NOSTATE，导致instance热迁移失败&quot;&gt;&lt;a href=&quot;#openstack-上的instance运行正常，但是power-state状态为NOSTATE，导致insta</summary>
      
    
    
    
    <category term="openstack" scheme="https://www.lijiawang.org/categories/openstack/"/>
    
    
    <category term="openstack" scheme="https://www.lijiawang.org/tags/openstack/"/>
    
  </entry>
  
  <entry>
    <title>使用kubeadm升级kubernetes集群</title>
    <link href="https://www.lijiawang.org/kubeadm%E5%8D%87%E7%BA%A7k8s%E9%9B%86%E7%BE%A4(v1.15.3%20to%20v1.16.0%E7%89%88)"/>
    <id>https://www.lijiawang.org/kubeadm%E5%8D%87%E7%BA%A7k8s%E9%9B%86%E7%BE%A4(v1.15.3%20to%20v1.16.0%E7%89%88)</id>
    <published>2019-09-23T14:38:14.000Z</published>
    <updated>2019-11-05T14:49:22.038Z</updated>
    
    <content type="html"><![CDATA[<h3 id="kubeadm升级k8s集群-v1-15-3-to-v1-16-0版"><a href="#kubeadm升级k8s集群-v1-15-3-to-v1-16-0版" class="headerlink" title="kubeadm升级k8s集群(v1.15.3 to v1.16.0版)"></a><code>kubeadm</code>升级<code>k8s</code>集群(<code>v1.15.3 to v1.16.0</code>版)</h3><blockquote><p><code>Kubernetes</code>在2019年9月18日发布了年度的第三个版本<code>1.16</code>,本篇文章介绍使用<code>kubeadm</code>升级现在有的集群到<code>v1.16.0</code>。</p></blockquote><h3 id="Kubernetes-1-16版本的发布徽章"><a href="#Kubernetes-1-16版本的发布徽章" class="headerlink" title="Kubernetes 1.16版本的发布徽章"></a><code>Kubernetes 1.16</code>版本的发布徽章</h3><p><img src="https://ljw.howieli.cn/blog/2019-9-23/Kubernetes%201.16%E7%89%88%E6%9C%AC%E7%9A%84%E5%8F%91%E5%B8%83%E5%BE%BD%E7%AB%A0.jpg" alt="Kubernetes 1.16版本的发布徽章"></p><h3 id="查看k8s集群版本"><a href="#查看k8s集群版本" class="headerlink" title="查看k8s集群版本"></a>查看<code>k8s</code>集群版本</h3><p>查看当前的<code>k8s</code>版本</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]# kubectl get nodes</span><br><span class="line">NAME         STATUS   ROLES    AGE   VERSION</span><br><span class="line">k8s-master   Ready    master   20d   v1.15.3</span><br><span class="line">k8s-node1    Ready    &lt;none&gt;   20d   v1.15.3</span><br><span class="line">k8s-node2    Ready    &lt;none&gt;   20d   v1.15.3</span><br><span class="line">k8s-node3    Ready    &lt;none&gt;   20d   v1.15.3</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]# kubeadm version</span><br><span class="line">kubeadm version: &amp;version.Info&#123;Major:&quot;1&quot;, Minor:&quot;15&quot;, GitVersion:&quot;v1.15.3&quot;, GitCommit:&quot;2d3c76f9091b6bec110a5e63777c332469e0cba2&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2019-08-19T11:11:18Z&quot;, GoVersion:&quot;go1.12.9&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux/amd64&quot;&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]# kubelet --version</span><br><span class="line">Kubernetes v1.15.3</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@k8s-master ~]# systemctl status kubelet.service |grep Active</span><br><span class="line">   Active: active (running) since Fri 2019-08-30 07:26:33 EDT; 3 weeks 0 days ago</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master yum.repos.d]# kubectl version</span><br><span class="line">Client Version: version.Info&#123;Major:&quot;1&quot;, Minor:&quot;15&quot;, GitVersion:&quot;v1.15.3&quot;, GitCommit:&quot;2d3c76f9091b6bec110a5e63777c332469e0cba2&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2019-08-19T11:13:54Z&quot;, GoVersion:&quot;go1.12.9&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux&#x2F;amd64&quot;&#125;</span><br><span class="line">Server Version: version.Info&#123;Major:&quot;1&quot;, Minor:&quot;15&quot;, GitVersion:&quot;v1.15.3&quot;, GitCommit:&quot;2d3c76f9091b6bec110a5e63777c332469e0cba2&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2019-08-19T11:05:50Z&quot;, GoVersion:&quot;go1.12.9&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux&#x2F;amd64&quot;&#125;</span><br></pre></td></tr></table></figure><h3 id="查看版本支持"><a href="#查看版本支持" class="headerlink" title="查看版本支持"></a>查看版本支持</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master yum.repos.d]# yum list kubelet kubeadm kubectl</span><br><span class="line">Loaded plugins: fastestmirror, langpacks</span><br><span class="line">Loading mirror speeds from cached hostfile</span><br><span class="line"> * base: mirrors.aliyun.com</span><br><span class="line"> * extras: mirrors.aliyun.com</span><br><span class="line"> * updates: mirrors.aliyun.com</span><br><span class="line">Installed Packages</span><br><span class="line">kubeadm.x86_64                                                        1.15.3-0                                                        @kubernetes</span><br><span class="line">kubectl.x86_64                                                        1.15.3-0                                                        @kubernetes</span><br><span class="line">kubelet.x86_64                                                        1.15.3-0                                                        @kubernetes</span><br><span class="line">Available Packages</span><br><span class="line">kubeadm.x86_64                                                        1.16.0-0                                                        kubernetes </span><br><span class="line">kubectl.x86_64                                                        1.16.0-0                                                        kubernetes </span><br><span class="line">kubelet.x86_64                                                        1.16.0-0                                                        kubernetes </span><br></pre></td></tr></table></figure><h3 id="升级kubelet-kubeadm-kubectl版本到1-16-0-每个节点都执行"><a href="#升级kubelet-kubeadm-kubectl版本到1-16-0-每个节点都执行" class="headerlink" title="升级kubelet kubeadm kubectl版本到1.16.0(每个节点都执行)"></a>升级<code>kubelet</code> <code>kubeadm</code> <code>kubectl</code>版本到<code>1.16.0</code>(每个节点都执行)</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]# yum install -y kubelet kubeadm kubectl</span><br><span class="line"><span class="meta">#</span><span class="bash"> systemctl daemon-reload &amp;&amp; systemctl restart kubelet &amp;&amp; systemctl status kubelet</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@k8s-master ~]# kubectl version</span><br><span class="line">Client Version: version.Info&#123;Major:&quot;1&quot;, Minor:&quot;16&quot;, GitVersion:&quot;v1.16.0&quot;, GitCommit:&quot;2bd9643cee5b3b3a5ecbd3af49d09018f0773c77&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2019-09-18T14:36:53Z&quot;, GoVersion:&quot;go1.12.9&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux/amd64&quot;&#125;</span><br><span class="line">Server Version: version.Info&#123;Major:&quot;1&quot;, Minor:&quot;15&quot;, GitVersion:&quot;v1.15.3&quot;, GitCommit:&quot;2d3c76f9091b6bec110a5e63777c332469e0cba2&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2019-08-19T11:05:50Z&quot;, GoVersion:&quot;go1.12.9&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux/amd64&quot;&#125;</span><br><span class="line">[root@k8s-master ~]# kubelet --version</span><br><span class="line">Kubernetes v1.16.0</span><br><span class="line">[root@k8s-master ~]# kubeadm version</span><br><span class="line">kubeadm version: &amp;version.Info&#123;Major:&quot;1&quot;, Minor:&quot;16&quot;, GitVersion:&quot;v1.16.0&quot;, GitCommit:&quot;2bd9643cee5b3b3a5ecbd3af49d09018f0773c77&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2019-09-18T14:34:01Z&quot;, GoVersion:&quot;go1.12.9&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux/amd64&quot;&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="升级k8s集群"><a href="#升级k8s集群" class="headerlink" title="升级k8s集群"></a>升级<code>k8s</code>集群</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]# kubectl get nodes</span><br><span class="line">NAME         STATUS   ROLES    AGE   VERSION</span><br><span class="line">k8s-master   Ready    master   21d   v1.15.3</span><br><span class="line">k8s-node1    Ready    &lt;none&gt;   20d   v1.15.3</span><br><span class="line">k8s-node2    Ready    &lt;none&gt;   20d   v1.15.3</span><br><span class="line">k8s-node3    Ready    &lt;none&gt;   20d   v1.15.3</span><br></pre></td></tr></table></figure><h3 id="升级集群"><a href="#升级集群" class="headerlink" title="升级集群"></a>升级集群</h3><pre><code>[root@k8s-master ~]# kubeadm config print init-defaults &gt; kubeadm1.16.yaml<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>[root@k8s-master ~]# cat kubeadm1.16.yaml apiVersion: kubeadm.k8s.io/v1beta2bootstrapTokens:- groups:  - system:bootstrappers:kubeadm:default-node-token  token: abcdef.0123456789abcdef  ttl: 24h0m0s  usages:  - signing  - authenticationkind: InitConfigurationlocalAPIEndpoint:  advertiseAddress: 10.122.17.200  bindPort: 6443nodeRegistration:  criSocket: /var/run/dockershim.sock  name: k8s-master  taints:  - effect: NoSchedule    key: node-role.kubernetes.io/master---apiServer:  timeoutForControlPlane: 4m0sapiVersion: kubeadm.k8s.io/v1beta2certificatesDir: /etc/kubernetes/pkiclusterName: kubernetescontrollerManager: &#123;&#125;dns:  type: CoreDNSetcd:  local:    dataDir: /var/lib/etcdimageRepository: registry.aliyuncs.com/google_containerskind: ClusterConfigurationkubernetesVersion: v1.16.0networking:  dnsDomain: cluster.local  podSubnet: 192.168.0.0/16  serviceSubnet: 10.96.0.0/12scheduler: &#123;&#125;---apiVersion: kubeproxy.config.k8s.io/v1alpha1kind: KubeProxyConfigurationmode: ipvs<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>[root@k8s-master ~]# kubeadm upgrade apply --config kubeadm1.16.yaml.....................[addons] Applied essential addon: CoreDNS[addons] Applied essential addon: kube-proxy[upgrade/successful] SUCCESS! Your cluster was upgraded to &quot;v1.16.0&quot;. Enjoy![upgrade/kubelet] Now that your control plane is upgraded, please proceed with upgrading your kubelets if you haven&#39;t already done so.<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>[root@k8s-master ~]# kubectl versionClient Version: version.Info&#123;Major:&quot;1&quot;, Minor:&quot;16&quot;, GitVersion:&quot;v1.16.0&quot;, GitCommit:&quot;2bd9643cee5b3b3a5ecbd3af49d09018f0773c77&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2019-09-18T14:36:53Z&quot;, GoVersion:&quot;go1.12.9&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux/amd64&quot;&#125;Server Version: version.Info&#123;Major:&quot;1&quot;, Minor:&quot;16&quot;, GitVersion:&quot;v1.16.0&quot;, GitCommit:&quot;2bd9643cee5b3b3a5ecbd3af49d09018f0773c77&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2019-09-18T14:27:17Z&quot;, GoVersion:&quot;go1.12.9&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux/amd64&quot;&#125;<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#96;&#96;&#96;shell</span><br><span class="line">[root@k8s-node1 ~]# yum clean all</span><br><span class="line">[root@k8s-node1 ~]# yum install -y kubelet kubeadm kubectl</span><br><span class="line"></span><br><span class="line">systemctl daemon-reload &amp;&amp; systemctl restart kubelet &amp;&amp; systemctl status kubelet</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@k8s-node1 ~]# kubectl version</span><br><span class="line">Client Version: version.Info&#123;Major:&quot;1&quot;, Minor:&quot;16&quot;, GitVersion:&quot;v1.16.0&quot;, GitCommit:&quot;2bd9643cee5b3b3a5ecbd3af49d09018f0773c77&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2019-09-18T14:36:53Z&quot;, GoVersion:&quot;go1.12.9&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux&#x2F;amd64&quot;&#125;</span><br><span class="line">The connection to the server localhost:8080 was refused - did you specify the right host or port?</span><br><span class="line">[root@k8s-node1 ~]# kubelet --version</span><br><span class="line">Kubernetes v1.16.0</span><br><span class="line">[root@k8s-node1 ~]# kubeadm version</span><br><span class="line">kubeadm version: &amp;version.Info&#123;Major:&quot;1&quot;, Minor:&quot;16&quot;, GitVersion:&quot;v1.16.0&quot;, GitCommit:&quot;2bd9643cee5b3b3a5ecbd3af49d09018f0773c77&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2019-09-18T14:34:01Z&quot;, GoVersion:&quot;go1.12.9&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux&#x2F;amd64&quot;&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]# kubectl version</span><br><span class="line">Client Version: version.Info&#123;Major:&quot;1&quot;, Minor:&quot;16&quot;, GitVersion:&quot;v1.16.0&quot;, GitCommit:&quot;2bd9643cee5b3b3a5ecbd3af49d09018f0773c77&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2019-09-18T14:36:53Z&quot;, GoVersion:&quot;go1.12.9&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux/amd64&quot;&#125;</span><br><span class="line">Server Version: version.Info&#123;Major:&quot;1&quot;, Minor:&quot;16&quot;, GitVersion:&quot;v1.16.0&quot;, GitCommit:&quot;2bd9643cee5b3b3a5ecbd3af49d09018f0773c77&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2019-09-18T14:27:17Z&quot;, GoVersion:&quot;go1.12.9&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux/amd64&quot;&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@k8s-master ~]# kubectl get nodes</span><br><span class="line">NAME         STATUS   ROLES    AGE   VERSION</span><br><span class="line">k8s-master   Ready    master   21d   v1.16.0</span><br><span class="line">k8s-node1    Ready    &lt;none&gt;   21d   v1.16.0</span><br><span class="line">k8s-node2    Ready    &lt;none&gt;   21d   v1.16.0</span><br><span class="line">k8s-node3    Ready    &lt;none&gt;   21d   v1.16.0</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@k8s-master ~]# kubectl get pod -A</span><br><span class="line">NAMESPACE       NAME                                        READY   STATUS    RESTARTS   AGE</span><br><span class="line">default         curl-6bf6db5c4f-dqw9x                       1/1     Running   2          20d</span><br><span class="line">ingress-nginx   nginx-ingress-controller-6956498fcf-jkzbs   1/1     Running   1          16d</span><br><span class="line">kube-system     calico-kube-controllers-65b8787765-hx9cr    1/1     Running   1          21d</span><br><span class="line">kube-system     calico-node-2rlsz                           1/1     Running   1          21d</span><br><span class="line">kube-system     calico-node-5vz46                           1/1     Running   1          21d</span><br><span class="line">kube-system     calico-node-6szsd                           1/1     Running   1          21d</span><br><span class="line">kube-system     calico-node-s5nmr                           1/1     Running   1          21d</span><br><span class="line">kube-system     calicoctl                                   1/1     Running   1          20d</span><br><span class="line">kube-system     coredns-58cc8c89f4-cnl45                    1/1     Running   1          26m</span><br><span class="line">kube-system     coredns-58cc8c89f4-nj8hr                    1/1     Running   1          9m4s</span><br><span class="line">kube-system     etcd-k8s-master                             1/1     Running   0          6m35s</span><br><span class="line">kube-system     kube-apiserver-k8s-master                   1/1     Running   0          6m30s</span><br><span class="line">kube-system     kube-controller-manager-k8s-master          1/1     Running   0          6m35s</span><br><span class="line">kube-system     kube-proxy-52mbf                            1/1     Running   1          26m</span><br><span class="line">kube-system     kube-proxy-gf75r                            1/1     Running   1          25m</span><br><span class="line">kube-system     kube-proxy-hbvjf                            1/1     Running   1          26m</span><br><span class="line">kube-system     kube-proxy-zppdf                            1/1     Running   1          26m</span><br><span class="line">kube-system     kube-scheduler-k8s-master                   1/1     Running   0          6m31s</span><br><span class="line">kube-system     kubernetes-dashboard-5dc4c54b55-c7sv2       1/1     Running   1          20d</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@k8s-master ~]# kubectl get pod</span><br><span class="line">NAME                    READY   STATUS    RESTARTS   AGE</span><br><span class="line">curl-6bf6db5c4f-dqw9x   1/1     Running   2          20d</span><br><span class="line"></span><br></pre></td></tr></table></figure>### 验证`cordns`<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]# kubectl get pod</span><br><span class="line">NAME                    READY   STATUS    RESTARTS   AGE</span><br><span class="line">curl-6bf6db5c4f-dqw9x   1&#x2F;1     Running   2          20d</span><br><span class="line">[root@k8s-master ~]# kubectl exec -it curl-6bf6db5c4f-dqw9x -- &#x2F;bin&#x2F;sh</span><br><span class="line">&#x2F;bin&#x2F;sh: shopt: not found</span><br><span class="line">[ root@curl-6bf6db5c4f-dqw9x:&#x2F; ]$ nslookup kubernetes.default</span><br><span class="line">Server:    10.96.0.10</span><br><span class="line">Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local</span><br><span class="line"></span><br><span class="line">Name:      kubernetes.default</span><br><span class="line">Address 1: 10.96.0.1 kubernetes.default.svc.cluster.local</span><br><span class="line">[ root@curl-6bf6db5c4f-dqw9x:&#x2F; ]$ exit</span><br><span class="line">[root@k8s-master ~]# </span><br></pre></td></tr></table></figure>### 未解决的问题`kubectl get cs`显示为`unknown`错误提示信息示例如下:<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> kubectl get cs</span></span><br><span class="line">NAME                 AGE</span><br><span class="line">scheduler            &lt;unknown&gt;</span><br><span class="line">controller-manager   &lt;unknown&gt;</span><br><span class="line">etcd-0               &lt;unknown&gt;</span><br></pre></td></tr></table></figure>这个问题似乎对集群没有太大的影响，暂未解决，后续愿意确认之后会继续更新，如果有大神知道如何解决，请指点。### 参考链接1、 [https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-upgrade/](https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-upgrade/)2、 [http://www.sohu.com/a/342118551_198222](http://www.sohu.com/a/342118551_198222)3、 [https://www.linuxidc.com/Linux/2019-09/160728.htm](https://www.linuxidc.com/Linux/2019-09/160728.htm)</code></pre>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;kubeadm升级k8s集群-v1-15-3-to-v1-16-0版&quot;&gt;&lt;a href=&quot;#kubeadm升级k8s集群-v1-15-3-to-v1-16-0版&quot; class=&quot;headerlink&quot; title=&quot;kubeadm升级k8s集群(v1.15.3 t</summary>
      
    
    
    
    <category term="kubernetes" scheme="https://www.lijiawang.org/categories/kubernetes/"/>
    
    
    <category term="kubeadm" scheme="https://www.lijiawang.org/tags/kubeadm/"/>
    
  </entry>
  
  <entry>
    <title>在k8s上部署ingress-nginx并使用</title>
    <link href="https://www.lijiawang.org/%E5%9C%A8k8s%E4%B8%8A%E9%83%A8%E7%BD%B2ingress-nginx%E5%B9%B6%E4%BD%BF%E7%94%A8"/>
    <id>https://www.lijiawang.org/%E5%9C%A8k8s%E4%B8%8A%E9%83%A8%E7%BD%B2ingress-nginx%E5%B9%B6%E4%BD%BF%E7%94%A8</id>
    <published>2019-09-22T14:30:36.000Z</published>
    <updated>2019-09-22T14:34:24.629Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><p>[TOC]</p><h3 id="部署nginx-ingress"><a href="#部署nginx-ingress" class="headerlink" title="部署nginx-ingress"></a>部署<code>nginx-ingress</code></h3><h3 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h3><p>准备一套<code>k8s</code>环境</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> kubectl get cs</span></span><br><span class="line">NAME                 STATUS    MESSAGE             ERROR</span><br><span class="line">controller-manager   Healthy   ok                  </span><br><span class="line">scheduler            Healthy   ok                  </span><br><span class="line">etcd-0               Healthy   &#123;&quot;health&quot;:&quot;true&quot;&#125;   </span><br><span class="line"><span class="meta">#</span><span class="bash"> kubectl cluster-info</span></span><br><span class="line">Kubernetes master is running at https://10.122.17.200:6443</span><br><span class="line">KubeDNS is running at https://10.122.17.200:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy</span><br><span class="line"></span><br><span class="line">To further debug and diagnose cluster problems, use &#x27;kubectl cluster-info dump&#x27;.</span><br><span class="line"><span class="meta">#</span><span class="bash"> kubectl get nodes</span></span><br><span class="line">NAME         STATUS   ROLES    AGE    VERSION</span><br><span class="line">k8s-master   Ready    master   4d1h   v1.15.3</span><br><span class="line">k8s-node1    Ready    &lt;none&gt;   4d1h   v1.15.3</span><br><span class="line">k8s-node2    Ready    &lt;none&gt;   4d     v1.15.3</span><br><span class="line">k8s-node3    Ready    &lt;none&gt;   4d     v1.15.3</span><br></pre></td></tr></table></figure><h3 id="下载安装nginx-ingress的yaml文件"><a href="#下载安装nginx-ingress的yaml文件" class="headerlink" title="下载安装nginx-ingress的yaml文件"></a>下载安装<code>nginx-ingress</code>的<code>yaml</code>文件</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/mandatory.yaml</span></span><br></pre></td></tr></table></figure><h3 id="设置nginx-ingress-controller调度节点"><a href="#设置nginx-ingress-controller调度节点" class="headerlink" title="设置nginx-ingress-controller调度节点"></a>设置<code>nginx-ingress-controller</code>调度节点</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> kubectl label nodes k8s-node1 hosts=nginx-ingress-controller</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> kubectl get node -l hosts=nginx-ingress-controller --show-labels</span> </span><br><span class="line">NAME        STATUS   ROLES    AGE    VERSION   LABELS</span><br><span class="line">k8s-node1   Ready    &lt;none&gt;   4d6h   v1.15.3   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,hosts=nginx-ingress-controller,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-node1,kubernetes.io/os=linux</span><br></pre></td></tr></table></figure><h3 id="修改mandatory-yaml文件"><a href="#修改mandatory-yaml文件" class="headerlink" title="修改mandatory.yaml文件"></a>修改<code>mandatory.yaml</code>文件</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> vim mandatory.yaml</span></span><br><span class="line">......</span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">......</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app.kubernetes.io/name: ingress-nginx</span><br><span class="line">      app.kubernetes.io/part-of: ingress-nginx</span><br><span class="line">  template:</span><br><span class="line">    ......</span><br><span class="line">    spec:</span><br><span class="line">      hostNetwork: true    # 使用hostNetwork 模式</span><br><span class="line">      nodeSelector:        # 使用nodeSelector</span><br><span class="line">        hosts: nginx-ingress-controller   # 选择标签为hosts=nginx-ingress-controller节点</span><br><span class="line">      serviceAccountName: nginx-ingress-serviceaccount</span><br><span class="line">      containers:</span><br><span class="line">        - name: nginx-ingress-controller</span><br><span class="line">          image: lijiawang/nginx-ingress-controller:0.25.1   # 更换镜像源</span><br><span class="line">  ......</span><br><span class="line">......</span><br></pre></td></tr></table></figure><h3 id="执行"><a href="#执行" class="headerlink" title="执行"></a>执行</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> kubectl apply -f mandatory.yaml</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> kubectl get pod -n ingress-nginx</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> kubectl get pod -n ingress-nginx -o wide</span></span><br><span class="line">NAME                                        READY   STATUS    RESTARTS   AGE   IP              NODE        NOMINATED NODE   READINESS GATES</span><br><span class="line">nginx-ingress-controller-6956498fcf-jkzbs   1/1     Running   0          55s   10.122.17.204   k8s-node1   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure><p>可以看到<code>nginx-ingress-controller-6956498fcf-jkzbs</code> <code>POD</code> 调度到了打好标记的<code>k8s-node1</code>。</p><h3 id="登录nginx-ingress-controller节点验证"><a href="#登录nginx-ingress-controller节点验证" class="headerlink" title="登录nginx-ingress-controller节点验证"></a>登录<code>nginx-ingress-controller</code>节点验证</h3><p>因为<code>k8s-node1</code>为<code>nginx-ingress-controller</code>节点，所有登录<code>k8s-node1</code>节点即可</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> ssh k8s-node1</span></span><br><span class="line">[root@k8s-node1 ~]# netstat -lntp|grep 80</span><br><span class="line">tcp        0      0 0.0.0.0:80              0.0.0.0:*               LISTEN      5901/nginx: master  </span><br><span class="line">tcp6       0      0 :::80                   :::*                    LISTEN      5901/nginx: master  </span><br><span class="line">[root@k8s-node1 ~]# netstat -lntp|grep 443</span><br><span class="line">tcp        0      0 0.0.0.0:443             0.0.0.0:*               LISTEN      5901/nginx: master  </span><br><span class="line">tcp6       0      0 :::443                  :::*                    LISTEN      5901/nginx: master  </span><br><span class="line">[root@k8s-node1 ~]# exit</span><br><span class="line">logout</span><br><span class="line">Connection to k8s-node1 closed.</span><br></pre></td></tr></table></figure><h3 id="部署应用"><a href="#部署应用" class="headerlink" title="部署应用"></a>部署应用</h3><p>部署测试应用</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat deploy-demon.yaml </span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">myapp</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">myapp</span></span><br><span class="line">    <span class="attr">release:</span> <span class="string">canary</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">http</span></span><br><span class="line">    <span class="attr">port:</span> <span class="number">80</span></span><br><span class="line">    <span class="attr">targetPort:</span> <span class="number">80</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span> </span><br><span class="line">  <span class="attr">name:</span> <span class="string">myapp-deploy</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">5</span></span><br><span class="line">  <span class="attr">selector:</span> </span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">myapp</span></span><br><span class="line">      <span class="attr">release:</span> <span class="string">canary</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">myapp</span></span><br><span class="line">        <span class="attr">release:</span> <span class="string">canary</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">myapp</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">lijiawang/myapp:v2</span></span><br><span class="line">        <span class="attr">ports:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">httpd</span></span><br><span class="line">          <span class="attr">containerPort:</span> <span class="number">80</span></span><br><span class="line"><span class="comment"># kubectl  apply -f deploy-demon.yaml </span></span><br><span class="line"><span class="string">service/myapp</span> <span class="string">created</span></span><br><span class="line"><span class="string">deployment.apps/myapp-deploy</span> <span class="string">created</span></span><br><span class="line"><span class="comment"># kubectl get pod </span></span><br><span class="line"><span class="string">NAME</span>                           <span class="string">READY</span>   <span class="string">STATUS</span>    <span class="string">RESTARTS</span>   <span class="string">AGE</span></span><br><span class="line"><span class="string">curl-6bf6db5c4f-dqw9x</span>          <span class="number">1</span><span class="string">/1</span>     <span class="string">Running</span>   <span class="number">1</span>          <span class="string">16d</span></span><br><span class="line"><span class="string">myapp-deploy-c69757d67-9bv4w</span>   <span class="number">1</span><span class="string">/1</span>     <span class="string">Running</span>   <span class="number">0</span>          <span class="string">38s</span></span><br><span class="line"><span class="string">myapp-deploy-c69757d67-j2kxc</span>   <span class="number">1</span><span class="string">/1</span>     <span class="string">Running</span>   <span class="number">0</span>          <span class="string">38s</span></span><br><span class="line"><span class="string">myapp-deploy-c69757d67-jgm5v</span>   <span class="number">1</span><span class="string">/1</span>     <span class="string">Running</span>   <span class="number">0</span>          <span class="string">38s</span></span><br><span class="line"><span class="string">myapp-deploy-c69757d67-n5rxw</span>   <span class="number">1</span><span class="string">/1</span>     <span class="string">Running</span>   <span class="number">0</span>          <span class="string">38s</span></span><br><span class="line"><span class="string">myapp-deploy-c69757d67-t4nxr</span>   <span class="number">1</span><span class="string">/1</span>     <span class="string">Running</span>   <span class="number">0</span>          <span class="string">38s</span></span><br><span class="line"><span class="comment"># kubectl get svc</span></span><br><span class="line"><span class="string">NAME</span>         <span class="string">TYPE</span>        <span class="string">CLUSTER-IP</span>       <span class="string">EXTERNAL-IP</span>   <span class="string">PORT(S)</span>   <span class="string">AGE</span></span><br><span class="line"><span class="string">kubernetes</span>   <span class="string">ClusterIP</span>   <span class="number">10.96</span><span class="number">.0</span><span class="number">.1</span>        <span class="string">&lt;none&gt;</span>        <span class="number">443</span><span class="string">/TCP</span>   <span class="string">17d</span></span><br><span class="line"><span class="string">myapp</span>        <span class="string">ClusterIP</span>   <span class="number">10.104</span><span class="number">.246</span><span class="number">.234</span>   <span class="string">&lt;none&gt;</span>        <span class="number">80</span><span class="string">/TCP</span>    <span class="string">2m58s</span></span><br></pre></td></tr></table></figure><h3 id="以ingress方式暴露应用"><a href="#以ingress方式暴露应用" class="headerlink" title="以ingress方式暴露应用"></a>以<code>ingress</code>方式暴露应用</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> cat ingress-myapp.yaml</span> </span><br><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  name: ingress-myapp</span><br><span class="line">  namespace: default</span><br><span class="line">  annotations: </span><br><span class="line">    kubernetes.io/ingress.class: &quot;nginx&quot;</span><br><span class="line">spec:</span><br><span class="line">  rules:</span><br><span class="line">  - host: myapp.lijw19.com   # 定义域名</span><br><span class="line">    http:</span><br><span class="line">      paths:</span><br><span class="line">      - path: </span><br><span class="line">        backend:</span><br><span class="line">          serviceName: myapp   # 跟你要爆了的服务的svc名字相同</span><br><span class="line">          servicePort: 80     # 要暴露的端口</span><br><span class="line"><span class="meta">#</span><span class="bash"> kubectl apply -f ingress-myapp.yaml</span> </span><br><span class="line">ingress.extensions/ingress-myapp created</span><br><span class="line"><span class="meta">#</span><span class="bash"> kubectl get ingresses.</span></span><br><span class="line">NAME            HOSTS              ADDRESS   PORTS   AGE</span><br><span class="line">ingress-myapp   myapp.lijw19.com             80      10s</span><br></pre></td></tr></table></figure><p>本地<code>host</code>解析<br><code>windows</code>的<code>hosts</code>文件在<code>c:\windows\system32\drivers\etc</code>下<br>修改<code>host</code>文件，增加以下内容</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">10.122.17.204 myapp.lijw19.com </span><br></pre></td></tr></table></figure><h3 id="访问应用"><a href="#访问应用" class="headerlink" title="访问应用"></a>访问应用</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> <span class="keyword">for</span> i <span class="keyword">in</span> `seq 10`; <span class="keyword">do</span> curl http://myapp.lijw19.com; <span class="keyword">done</span></span></span><br><span class="line">Hello MyApp | Version: v2 | &lt;a href=&quot;hostname.html&quot;&gt;Pod Name&lt;/a&gt;</span><br><span class="line">Hello MyApp | Version: v2 | &lt;a href=&quot;hostname.html&quot;&gt;Pod Name&lt;/a&gt;</span><br><span class="line">Hello MyApp | Version: v2 | &lt;a href=&quot;hostname.html&quot;&gt;Pod Name&lt;/a&gt;</span><br><span class="line">Hello MyApp | Version: v2 | &lt;a href=&quot;hostname.html&quot;&gt;Pod Name&lt;/a&gt;</span><br><span class="line">Hello MyApp | Version: v2 | &lt;a href=&quot;hostname.html&quot;&gt;Pod Name&lt;/a&gt;</span><br><span class="line">Hello MyApp | Version: v2 | &lt;a href=&quot;hostname.html&quot;&gt;Pod Name&lt;/a&gt;</span><br><span class="line">Hello MyApp | Version: v2 | &lt;a href=&quot;hostname.html&quot;&gt;Pod Name&lt;/a&gt;</span><br><span class="line">Hello MyApp | Version: v2 | &lt;a href=&quot;hostname.html&quot;&gt;Pod Name&lt;/a&gt;</span><br><span class="line">Hello MyApp | Version: v2 | &lt;a href=&quot;hostname.html&quot;&gt;Pod Name&lt;/a&gt;</span><br><span class="line">Hello MyApp | Version: v2 | &lt;a href=&quot;hostname.html&quot;&gt;Pod Name&lt;/a&gt;</span><br></pre></td></tr></table></figure><p>使用浏览器访问<code>http://myapp.lijw19.com</code><br><img src="https://ljw.howieli.cn/blog/2019-9-18/http0.png"></p><p><img src="https://ljw.howieli.cn/blog/2019-9-18/http1.png"><br>这里我们是使用的http访问的，那如果要使用</p><h3 id="使用https访问"><a href="#使用https访问" class="headerlink" title="使用https访问"></a>使用<code>https</code>访问</h3><p>创建证书</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"># openssl genrsa -out tls.key 2048</span><br><span class="line">Generating RSA private key, 2048 bit long modulus</span><br><span class="line">......................+++</span><br><span class="line">.....................................+++</span><br><span class="line">e is 65537 (0x10001)</span><br><span class="line"># ls -l tls.key </span><br><span class="line">-rw-r--r-- 1 root root 1675 Sep 16 08:43 tls.key</span><br><span class="line"># openssl req -new -x509 -key tls.key -out tls.crt</span><br><span class="line">You are about to be asked to enter information that will be incorporated</span><br><span class="line">into your certificate request.</span><br><span class="line">What you are about to enter is what is called a Distinguished Name or a DN.</span><br><span class="line">There are quite a few fields but you can leave some blank</span><br><span class="line">For some fields there will be a default value,</span><br><span class="line">If you enter &#39;.&#39;, the field will be left blank.</span><br><span class="line">-----</span><br><span class="line">Country Name (2 letter code) [XX]:CN     </span><br><span class="line">State or Province Name (full name) []:lijw19</span><br><span class="line">Locality Name (eg, city) [Default City]:lijw19</span><br><span class="line">Organization Name (eg, company) [Default Company Ltd]:test</span><br><span class="line">Organizational Unit Name (eg, section) []:test</span><br><span class="line">Common Name (eg, your name or your server&#39;s hostname) []:test.lijw19.com</span><br><span class="line">Email Address []:</span><br><span class="line">[root@k8s-master ~]# ls -l tls.*</span><br><span class="line">-rw-r--r-- 1 root root 1318 Sep 16 08:46 tls.crt</span><br><span class="line">-rw-r--r-- 1 root root 1675 Sep 16 08:43 tls.key</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="证书转成secret"><a href="#证书转成secret" class="headerlink" title="证书转成secret"></a>证书转成<code>secret</code></h3><p>将创建好的证书转成<code>secret</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> kubectl create secret tls lijw-ingress-secret --cert=tls.crt --key=tls.key</span></span><br><span class="line">secret/lijw-ingress-secret created</span><br><span class="line"><span class="meta">#</span><span class="bash"> kubectl get secrets</span> </span><br><span class="line">NAME                  TYPE                                  DATA   AGE</span><br><span class="line">default-token-vmfwt   kubernetes.io/service-account-token   3      17d</span><br><span class="line">lijw-ingress-secret   kubernetes.io/tls                     2      10s</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="创建https-ingress"><a href="#创建https-ingress" class="headerlink" title="创建https ingress"></a>创建<code>https</code> <code>ingress</code></h3><p>修改下<code>ingress-myapp-https.yaml</code>加入刚刚添加的<code>secret</code>，修改后的文件如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"># cat ingress-myapp-https.yaml </span><br><span class="line">apiVersion: extensions&#x2F;v1beta1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  name: ingress-myapp-https</span><br><span class="line">  namespace: default</span><br><span class="line">  annotations: </span><br><span class="line">    kubernetes.io&#x2F;ingress.class: &quot;nginx&quot;</span><br><span class="line">spec:</span><br><span class="line">  tls:    # 添加了tls这一段</span><br><span class="line">  - hosts:</span><br><span class="line">    - test.lijw19.com</span><br><span class="line">    secretName: lijw-ingress-secret  # 到这结束</span><br><span class="line">  rules:</span><br><span class="line">  - host: test.lijw19.com</span><br><span class="line">    http:</span><br><span class="line">      paths:</span><br><span class="line">      - path: </span><br><span class="line">        backend:</span><br><span class="line">          serviceName: myapp</span><br><span class="line">          servicePort: 80</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># kubectl apply -f ingress-myapp-https.yaml </span><br><span class="line">ingress.extensions&#x2F;ingress-myapp-https created</span><br><span class="line"># kubectl get ingresses ingress-myapp-https </span><br><span class="line">NAME                  HOSTS             ADDRESS   PORTS     AGE</span><br><span class="line">ingress-myapp-https   test.lijw19.com             80, 443   13s</span><br></pre></td></tr></table></figure><h3 id="本地host解析"><a href="#本地host解析" class="headerlink" title="本地host解析"></a>本地<code>host</code>解析</h3><p><code>windows</code>的<code>hosts</code>文件在<code>c:\windows\system32\drivers\etc</code>下<br>修改<code>host</code>文件，增加以下内容</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">10.122.17.204 myapp.lijw19.com  test.lijw19.com</span><br></pre></td></tr></table></figure><p>在浏览器访问<code>https://test.lijw19.com</code><br><img src="https://ljw.howieli.cn/blog/2019-9-18/https0.png"></p><p><img src="https://ljw.howieli.cn/blog/2019-9-18/https1.png"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;

&lt;p&gt;[TOC]&lt;/p&gt;
&lt;h3 id=&quot;部署nginx-ingress&quot;&gt;&lt;a href=&quot;#部署nginx-ingress&quot; class=&quot;headerlink&quot; title=&quot;部署nginx-ingress&quot;&gt;&lt;/a&gt;部署&lt;code&gt;n</summary>
      
    
    
    
    <category term="kubernets" scheme="https://www.lijiawang.org/categories/kubernets/"/>
    
    
    <category term="kubernets" scheme="https://www.lijiawang.org/tags/kubernets/"/>
    
  </entry>
  
  <entry>
    <title>使用kubeadm搭建kubernetes1.15</title>
    <link href="https://www.lijiawang.org/%E4%BD%BF%E7%94%A8kubeadm%E6%90%AD%E5%BB%BAkubernetes1.15"/>
    <id>https://www.lijiawang.org/%E4%BD%BF%E7%94%A8kubeadm%E6%90%AD%E5%BB%BAkubernetes1.15</id>
    <published>2019-09-01T14:07:30.000Z</published>
    <updated>2019-09-22T14:24:49.559Z</updated>
    
    <content type="html"><![CDATA[<h3 id="使用kubeadm方式安装kubernetes"><a href="#使用kubeadm方式安装kubernetes" class="headerlink" title="使用kubeadm方式安装kubernetes"></a>使用<code>kubeadm</code>方式安装<code>kubernetes</code></h3><a id="more"></a><p>[TOC]</p><h3 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h3><p>在所有节点执行，此环节所有的节点操作系统均匀<code>centos7.6</code></p><h4 id="1、配置hosts解析"><a href="#1、配置hosts解析" class="headerlink" title="1、配置hosts解析"></a>1、配置<code>hosts</code>解析</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> cat /etc/hosts</span></span><br><span class="line">10.122.17.206 k8s-node3</span><br><span class="line">10.122.17.205 k8s-node2</span><br><span class="line">10.122.17.204 k8s-node1</span><br><span class="line">10.122.17.200 k8s-master</span><br><span class="line"><span class="meta">#</span><span class="bash"> masker 主机到node免密</span></span><br></pre></td></tr></table></figure><h4 id="2、禁用防火墙"><a href="#2、禁用防火墙" class="headerlink" title="2、禁用防火墙"></a>2、禁用防火墙</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">systemctl stop firewalld</span><br><span class="line">systemctl disable firewalld</span><br></pre></td></tr></table></figure><h4 id="3、禁用SELINUX"><a href="#3、禁用SELINUX" class="headerlink" title="3、禁用SELINUX"></a>3、禁用<code>SELINUX</code></h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> setenforce 0</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> sed -i <span class="string">&#x27;s/SELINUX=enforcing/SELINUX=disabled/&#x27;</span> /etc/selinux/config</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> cat /etc/selinux/config</span></span><br><span class="line">SELINUX=disabled</span><br></pre></td></tr></table></figure><h4 id="4、创建-etc-sysctl-d-k8s-conf-添加如下内容-并使修改生效"><a href="#4、创建-etc-sysctl-d-k8s-conf-添加如下内容-并使修改生效" class="headerlink" title="4、创建/etc/sysctl.d/k8s.conf,添加如下内容,并使修改生效"></a>4、创建<code>/etc/sysctl.d/k8s.conf</code>,添加如下内容,并使修改生效</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> cat /etc/sysctl.d/k8s.conf</span> </span><br><span class="line">net.bridge.bridge-nf-call-ip6tables = 1</span><br><span class="line">net.bridge.bridge-nf-call-iptables = 1</span><br><span class="line">net.ipv4.ip_forward = 1</span><br><span class="line"><span class="meta">#</span><span class="bash"> modprobe br_netfilter</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> sysctl -p /etc/sysctl.d/k8s.conf</span></span><br></pre></td></tr></table></figure><h4 id="5、安装ipvs"><a href="#5、安装ipvs" class="headerlink" title="5、安装ipvs"></a>5、安装ipvs</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># cat &gt; &#x2F;etc&#x2F;sysconfig&#x2F;modules&#x2F;ipvs.modules &lt;&lt;EOF</span><br><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line">modprobe -- ip_vs</span><br><span class="line">modprobe -- ip_vs_rr</span><br><span class="line">modprobe -- ip_vs_wrr</span><br><span class="line">modprobe -- ip_vs_sh</span><br><span class="line">modprobe -- nf_conntrack_ipv4</span><br><span class="line">EOF</span><br><span class="line"># chmod 755 &#x2F;etc&#x2F;sysconfig&#x2F;modules&#x2F;ipvs.modules &amp;&amp; bash &#x2F;etc&#x2F;sysconfig&#x2F;modules&#x2F;ipvs.modules &amp;&amp; lsmod | grep -e ip_vs -e nf_conntrack_ipv4</span><br></pre></td></tr></table></figure><p>上面脚本创建了的<code>/etc/sysconfig/modules/ipvs.modules</code>文件，保证在节点重启后能自动加载所需模块。使用<code>lsmod | grep -e ip_vs -e nf_conntrack_ipv4</code>命令查看是否已经正确加载所需的内核模块。<br>接下来还需要确保各个节点上已经安装了 ipset 软件包</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> yum install ipset -y</span></span><br></pre></td></tr></table></figure><p>为了便于查看 <code>ipvs</code> 的代理规则，最好安装一下管理工具 <code>ipvsadm</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> yum -y install ipvsadm -y</span></span><br></pre></td></tr></table></figure><h4 id="6、安装时间同步服务"><a href="#6、安装时间同步服务" class="headerlink" title="6、安装时间同步服务"></a>6、安装时间同步服务</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> yum install chrony -y</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> systemctl <span class="built_in">enable</span> chronyd</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> systemctl start chronyd</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> chronyc sources</span></span><br></pre></td></tr></table></figure><h4 id="7、关闭swap分区"><a href="#7、关闭swap分区" class="headerlink" title="7、关闭swap分区"></a>7、关闭swap分区</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> swapoff -a</span></span><br></pre></td></tr></table></figure><p>修改<code>/etc/fstab</code>文件，注释掉<code>SWAP</code>的自动挂载，使用<code>free -m</code>确认<code>swap</code>已经关闭。<code>swappiness</code>参数调整，修改<code>/etc/sysctl.d/k8s.conf</code>添加下面一行：</p><h4 id="8、安装docker-ce"><a href="#8、安装docker-ce" class="headerlink" title="8、安装docker-ce"></a>8、安装<code>docker-ce</code></h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> yum install -y yum-utils device-mapper-persistent-data lvm2</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> yum makecache fast</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> yum list docker-ce --showduplicates | sort -r</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 可以选择安装一个版本，比如我们这里安装最新版本</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> yum install docker-ce-19.03.1-3.el7 -y</span></span><br></pre></td></tr></table></figure><h4 id="9、配置Docker镜像加速器"><a href="#9、配置Docker镜像加速器" class="headerlink" title="9、配置Docker镜像加速器"></a>9、配置<code>Docker</code>镜像加速器</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt;EOF &gt;/etc/docker/daemon.json</span><br><span class="line">&#123;</span><br><span class="line">  &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;],</span><br><span class="line">  &quot;registry-mirrors&quot; : [</span><br><span class="line">    &quot;https://ot2k4d59.mirror.aliyuncs.com/&quot;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h4 id="10、启动Docker"><a href="#10、启动Docker" class="headerlink" title="10、启动Docker"></a>10、启动<code>Docker</code></h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> systemctl daemon-reload</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> systemctl restart docker</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> systemctl <span class="built_in">enable</span> docker</span></span><br></pre></td></tr></table></figure><h4 id="11、安装kubeadm、kubelet、kubectl"><a href="#11、安装kubeadm、kubelet、kubectl" class="headerlink" title="11、安装kubeadm、kubelet、kubectl"></a>11、安装<code>kubeadm</code>、<code>kubelet</code>、<code>kubectl</code></h4><p>配置<code>yum</code>源</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo</span><br><span class="line">[kubernetes]</span><br><span class="line">name=Kubernetes</span><br><span class="line">baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">repo_gpgcheck=1</span><br><span class="line">gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>安装<code>kubeadm</code>、<code>kubelet</code>、<code>kubectl</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes -y</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> kubeadm version</span></span><br><span class="line">kubeadm version: &amp;version.Info&#123;Major:&quot;1&quot;, Minor:&quot;15&quot;, GitVersion:&quot;v1.15.3&quot;, GitCommit:&quot;2d3c76f9091b6bec110a5e63777c332469e0cba2&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2019-08-19T11:11:18Z&quot;, GoVersion:&quot;go1.12.9&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux/amd64&quot;&#125;</span><br></pre></td></tr></table></figure><p>将 <code>kubelet</code> 设置成开机启动</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> systemctl <span class="built_in">enable</span> kubelet.service</span></span><br></pre></td></tr></table></figure><p>到这里为止上面所有的操作都需要在所有节点执行配置</p><h3 id="初始化集群"><a href="#初始化集群" class="headerlink" title="初始化集群"></a>初始化集群</h3><h4 id="1、在master节点配置配置kubeadm初始文件"><a href="#1、在master节点配置配置kubeadm初始文件" class="headerlink" title="1、在master节点配置配置kubeadm初始文件"></a>1、在master节点配置配置kubeadm初始文件</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">  kubeadm config <span class="built_in">print</span> init-defaults &gt; kubeadm.yaml</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> cat kubeadm.yaml</span> </span><br><span class="line">apiVersion: kubeadm.k8s.io/v1beta2</span><br><span class="line">bootstrapTokens:</span><br><span class="line">- groups:</span><br><span class="line">  - system:bootstrappers:kubeadm:default-node-token</span><br><span class="line">  token: abcdef.0123456789abcdef</span><br><span class="line">  ttl: 24h0m0s</span><br><span class="line">  usages:</span><br><span class="line">  - signing</span><br><span class="line">  - authentication</span><br><span class="line">kind: InitConfiguration</span><br><span class="line">localAPIEndpoint:</span><br><span class="line">  advertiseAddress: 10.122.17.200   # apiserver 节点内网IP</span><br><span class="line">  bindPort: 6443</span><br><span class="line">nodeRegistration:</span><br><span class="line">  criSocket: /var/run/dockershim.sock</span><br><span class="line">  name: k8s-master</span><br><span class="line">  taints:</span><br><span class="line">  - effect: NoSchedule</span><br><span class="line">    key: node-role.kubernetes.io/master</span><br><span class="line">---</span><br><span class="line">apiServer:</span><br><span class="line">  timeoutForControlPlane: 4m0s</span><br><span class="line">apiVersion: kubeadm.k8s.io/v1beta2</span><br><span class="line">certificatesDir: /etc/kubernetes/pki</span><br><span class="line">clusterName: kubernetes</span><br><span class="line">controllerManager: &#123;&#125;</span><br><span class="line">dns:</span><br><span class="line">  type: CoreDNS</span><br><span class="line">etcd:</span><br><span class="line">  local:</span><br><span class="line">    dataDir: /var/lib/etcd</span><br><span class="line">imageRepository: registry.aliyuncs.com/google_containers    # 科学上网使用阿里registry源</span><br><span class="line">kind: ClusterConfiguration</span><br><span class="line">kubernetesVersion: v1.15.3    # k8s版本</span><br><span class="line">networking:</span><br><span class="line">  dnsDomain: cluster.local</span><br><span class="line">  podSubnet: 192.168.0.0/16   # 我们这里是准备安装 calico 网络插件的，需要将 networking.podSubnet 设置为192.168.0.0/16</span><br><span class="line">  serviceSubnet: 10.96.0.0/12</span><br><span class="line">scheduler: &#123;&#125;</span><br><span class="line">---</span><br><span class="line">apiVersion: kubeproxy.config.k8s.io/v1alpha1</span><br><span class="line">kind: KubeProxyConfiguration</span><br><span class="line">mode: ipvs       # kube-proxy 模式</span><br></pre></td></tr></table></figure><h4 id="2、初始化master配置文件"><a href="#2、初始化master配置文件" class="headerlink" title="2、初始化master配置文件"></a>2、初始化<code>master</code>配置文件</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> kubeadm init --config kubeadm.yaml</span></span><br><span class="line">Your Kubernetes control-plane has initialized successfully!</span><br><span class="line"></span><br><span class="line">To start using your cluster, you need to run the following as a regular user:</span><br><span class="line"></span><br><span class="line">  mkdir -p $HOME/.kube</span><br><span class="line">  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config</span><br><span class="line">  sudo chown $(id -u):$(id -g) $HOME/.kube/config</span><br><span class="line"></span><br><span class="line">You should now deploy a pod network to the cluster.</span><br><span class="line">Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at:</span><br><span class="line">  https://kubernetes.io/docs/concepts/cluster-administration/addons/</span><br><span class="line"></span><br><span class="line">Then you can join any number of worker nodes by running the following on each as root:</span><br><span class="line"></span><br><span class="line">kubeadm join 10.122.17.200:6443 --token abcdef.0123456789abcdef \</span><br><span class="line">    --discovery-token-ca-cert-hash sha256:44a4ed41c1f69a23139b5d1e2bb6ec158d835257c6aab0a7607a122e77f51c04 </span><br></pre></td></tr></table></figure><h4 id="3、拷贝-kubeconfig-文件"><a href="#3、拷贝-kubeconfig-文件" class="headerlink" title="3、拷贝 kubeconfig 文件"></a>3、拷贝 kubeconfig 文件</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> mkdir -p <span class="variable">$HOME</span>/.kube</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> sudo cp -i /etc/kubernetes/admin.conf <span class="variable">$HOME</span>/.kube/config</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> sudo chown $(id -u):$(id -g) <span class="variable">$HOME</span>/.kube/config</span></span><br></pre></td></tr></table></figure><h4 id="4、kubectl自动补全"><a href="#4、kubectl自动补全" class="headerlink" title="4、kubectl自动补全"></a>4、kubectl自动补全</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> yum install -y bash-completion</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> locate bash_completion</span></span><br><span class="line">/etc/bash_completion.d</span><br><span class="line">/etc/bash_completion.d/git</span><br><span class="line">/etc/bash_completion.d/iprutils</span><br><span class="line">/etc/bash_completion.d/oscap</span><br><span class="line">/etc/bash_completion.d/redefine_filedir</span><br><span class="line">/etc/bash_completion.d/scl.bash</span><br><span class="line">/etc/bash_completion.d/yum-utils.bash</span><br><span class="line">/etc/profile.d/bash_completion.sh</span><br><span class="line">/root/naftis/vendor/github.com/spf13/cobra/bash_completions.go</span><br><span class="line">/usr/share/bash-completion/bash_completion</span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">source</span> /usr/share/bash-completion/bash_completion</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">source</span> &lt;(kubectl completion bash)</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">echo</span> <span class="string">&#x27;source /usr/share/bash-completion/bash_completion&#x27;</span> &gt;&gt; .bashrc</span> </span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">echo</span> <span class="string">&#x27;source &lt;(kubectl completion bash)&#x27;</span> &gt;&gt;.bashrc</span></span><br></pre></td></tr></table></figure><h3 id="添加计算节点"><a href="#添加计算节点" class="headerlink" title="添加计算节点"></a>添加计算节点</h3><h4 id="1、使用kubeadm添加计算节点"><a href="#1、使用kubeadm添加计算节点" class="headerlink" title="1、使用kubeadm添加计算节点"></a>1、使用<code>kubeadm</code>添加计算节点</h4><p><code>node</code>节点记住初始化集群上面的配置和操作要提前做好，必须安装好<code>kubeadm</code>、<code>kubelet</code>，执行以下命名</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> kubeadm join 10.122.17.200:6443 --token abcdef.0123456789abcdef \</span></span><br><span class="line"><span class="bash">    --discovery-token-ca-cert-hash sha256:44a4ed41c1f69a23139b5d1e2bb6ec158d835257c6aab0a7607a122e77f51c04</span> </span><br></pre></td></tr></table></figure><p>如果忘记了上面的<code>join</code>命令可以在<code>master</code>节点使用命令<code>kubeadm token create --print-join-command</code>重新获取,如下。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> kubeadm token create --print-join-command</span></span><br><span class="line">kubeadm join 10.122.17.200:6443 --token dkb75n.dxkqjiv96eu7ky2s \</span><br><span class="line">    --discovery-token-ca-cert-hash sha256:44a4ed41c1f69a23139b5d1e2bb6ec158d835257c6aab0a7607a122e77f51c04 </span><br></pre></td></tr></table></figure><h4 id="2、执行成功后可以在master节点运行kubectl-get-nodes命令"><a href="#2、执行成功后可以在master节点运行kubectl-get-nodes命令" class="headerlink" title="2、执行成功后可以在master节点运行kubectl get nodes命令"></a>2、执行成功后可以在<code>master</code>节点运行<code>kubectl get nodes</code>命令</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> kubectl get nodes</span></span><br><span class="line">NAME         STATUS      ROLES    AGE     VERSION</span><br><span class="line">k8s-master   NotReady    master   3m    v1.15.3</span><br><span class="line">k8s-node1    NotReady    &lt;none&gt;   48m   v1.15.3</span><br><span class="line">k8s-node2    NotReady    &lt;none&gt;   24m   v1.15.3</span><br><span class="line">k8s-node3    NotReady    &lt;none&gt;   13m   v1.15.3</span><br></pre></td></tr></table></figure><p>可以看到是<code>NotReady</code>状态，这是因为还没有安装网络插件，接下来安装网络插件</p><h3 id="安装calio网络组建"><a href="#安装calio网络组建" class="headerlink" title="安装calio网络组建"></a>安装<code>calio</code>网络组建</h3><p>以下命令在控制节点执行</p><h4 id="1、安装calio网络插件"><a href="#1、安装calio网络插件" class="headerlink" title="1、安装calio网络插件"></a>1、安装<code>calio</code>网络插件</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">  wget https://docs.projectcalico.org/v3.8/manifests/calico.yaml</span></span><br><span class="line"><span class="meta">#</span><span class="bash">  如果节点是多网卡，可以在资源清单文件中指定内网网卡</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> vi calico.yaml</span></span><br><span class="line">......</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - env:</span><br><span class="line">    - name: DATASTORE_TYPE</span><br><span class="line">      value: kubernetes</span><br><span class="line">    - name: IP_AUTODETECTION_METHOD  # DaemonSet中添加该环境变量</span><br><span class="line">      value: interface=eth0    # 指定内网网卡</span><br><span class="line">    - name: WAIT_FOR_DATASTORE</span><br><span class="line">      value: &quot;true&quot;</span><br><span class="line">......</span><br><span class="line"><span class="meta">#</span><span class="bash"> kubectl apply -f calico.yaml  <span class="comment"># 安装calico网络插件</span></span></span><br></pre></td></tr></table></figure><p>等待一段时间可以查看下<code>pod</code>的状态</p><h4 id="2、查看pod状态"><a href="#2、查看pod状态" class="headerlink" title="2、查看pod状态"></a>2、查看<code>pod</code>状态</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> kubectl get pods -n kube-system</span></span><br><span class="line">NAME                                       READY   STATUS    RESTARTS   AGE</span><br><span class="line">calico-kube-controllers-65b8787765-hx9cr   1/1     Running   0          3h50m</span><br><span class="line">calico-node-2rlsz                          1/1     Running   0          3h18m</span><br><span class="line">calico-node-5vz46                          1/1     Running   0          3h50m</span><br><span class="line">calico-node-6szsd                          1/1     Running   0          3h50m</span><br><span class="line">calico-node-s5nmr                          1/1     Running   0          3h29m</span><br><span class="line">coredns-bccdc95cf-9jf8w                    1/1     Running   0          4h8m</span><br><span class="line">coredns-bccdc95cf-gk27l                    1/1     Running   0          4h8m</span><br><span class="line">etcd-k8s-master                            1/1     Running   0          4h7m</span><br><span class="line">kube-apiserver-k8s-master                  1/1     Running   0          4h7m</span><br><span class="line">kube-controller-manager-k8s-master         1/1     Running   0          4h7m</span><br><span class="line">kube-proxy-5jbft                           1/1     Running   0          3h29m</span><br><span class="line">kube-proxy-d9p8w                           1/1     Running   0          3h18m</span><br><span class="line">kube-proxy-h6n88                           1/1     Running   0          3h54m</span><br><span class="line">kube-proxy-mqgj7                           1/1     Running   0          4h8m</span><br><span class="line">kube-scheduler-k8s-master                  1/1     Running   0          4h7m</span><br></pre></td></tr></table></figure><p>网络插件运行成功了，node 状态也正常了：</p><h4 id="3、查看node状态"><a href="#3、查看node状态" class="headerlink" title="3、查看node状态"></a>3、查看<code>node</code>状态</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> kubectl get nodes</span></span><br><span class="line">NAME         STATUS   ROLES    AGE     VERSION</span><br><span class="line">k8s-master   Ready    master   4h10m   v1.15.3</span><br><span class="line">k8s-node1    Ready    &lt;none&gt;   3h55m   v1.15.3</span><br><span class="line">k8s-node2    Ready    &lt;none&gt;   3h31m   v1.15.3</span><br><span class="line">k8s-node3    Ready    &lt;none&gt;   3h20m   v1.15.3</span><br></pre></td></tr></table></figure><h4 id="4、安装calicoctl"><a href="#4、安装calicoctl" class="headerlink" title="4、安装calicoctl"></a>4、安装calicoctl</h4><p>将<code>calicoctl</code>安装为<code>Kubernetes pod</code><br>使用<code>Kubernetes API</code>数据存储区</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> kubectl apply -f https://docs.projectcalico.org/v3.8/manifests/calicoctl.yaml</span></span><br></pre></td></tr></table></figure><p>然后，您可以使用kubectl运行命令，如下所示</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> kubectl <span class="built_in">exec</span> -ti -n kube-system calicoctl -- /calicoctl get profiles -o wide</span></span><br><span class="line">NAME                                                 LABELS   </span><br><span class="line">kns.default                                          map[]    </span><br><span class="line">kns.kube-node-lease                                  map[]    </span><br><span class="line">kns.kube-public                                      map[]    </span><br><span class="line">kns.kube-system                                      map[]    </span><br><span class="line">ksa.default.default                                  map[]    </span><br><span class="line">ksa.kube-node-lease.default                          map[]    </span><br><span class="line">ksa.kube-public.default                              map[]    </span><br><span class="line">ksa.kube-system.attachdetach-controller              map[]    </span><br><span class="line">ksa.kube-system.bootstrap-signer                     map[]    </span><br><span class="line">ksa.kube-system.calico-kube-controllers              map[]    </span><br><span class="line">ksa.kube-system.calico-node                          map[]    </span><br><span class="line">ksa.kube-system.calicoctl                            map[]    </span><br><span class="line">ksa.kube-system.certificate-controller               map[]    </span><br><span class="line">ksa.kube-system.clusterrole-aggregation-controller   map[]    </span><br><span class="line">ksa.kube-system.coredns                              map[]    </span><br><span class="line">ksa.kube-system.cronjob-controller                   map[]    </span><br><span class="line">ksa.kube-system.daemon-set-controller                map[]    </span><br><span class="line">ksa.kube-system.default                              map[]    </span><br><span class="line">ksa.kube-system.deployment-controller                map[]    </span><br><span class="line">ksa.kube-system.disruption-controller                map[]    </span><br><span class="line">ksa.kube-system.endpoint-controller                  map[]    </span><br><span class="line">ksa.kube-system.expand-controller                    map[]    </span><br><span class="line">ksa.kube-system.generic-garbage-collector            map[]    </span><br><span class="line">ksa.kube-system.horizontal-pod-autoscaler            map[]    </span><br><span class="line">ksa.kube-system.job-controller                       map[]    </span><br><span class="line">ksa.kube-system.kube-proxy                           map[]    </span><br><span class="line">ksa.kube-system.namespace-controller                 map[]    </span><br><span class="line">ksa.kube-system.node-controller                      map[]    </span><br><span class="line">ksa.kube-system.persistent-volume-binder             map[]    </span><br><span class="line">ksa.kube-system.pod-garbage-collector                map[]    </span><br><span class="line">ksa.kube-system.pv-protection-controller             map[]    </span><br><span class="line">ksa.kube-system.pvc-protection-controller            map[]    </span><br><span class="line">ksa.kube-system.replicaset-controller                map[]    </span><br><span class="line">ksa.kube-system.replication-controller               map[]    </span><br><span class="line">ksa.kube-system.resourcequota-controller             map[]    </span><br><span class="line">ksa.kube-system.service-account-controller           map[]    </span><br><span class="line">ksa.kube-system.service-controller                   map[]    </span><br><span class="line">ksa.kube-system.statefulset-controller               map[]    </span><br><span class="line">ksa.kube-system.token-cleaner                        map[]    </span><br><span class="line">ksa.kube-system.ttl-controller                       map[]    </span><br></pre></td></tr></table></figure><h4 id="5、设置别名"><a href="#5、设置别名" class="headerlink" title="5、设置别名"></a>5、设置别名</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">alias</span> calicoctl=<span class="string">&quot;kubectl exec -i -n kube-system calicoctl /calicoctl -- &quot;</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">echo</span> <span class="string">&#x27;alias calicoctl=&quot;kubectl exec -i -n kube-system calicoctl /calicoctl -- &quot;&#x27;</span> &gt;&gt; .bashrc</span> </span><br><span class="line"><span class="meta">#</span><span class="bash"> tail -n 1 .bashrc</span> </span><br><span class="line">alias calicoctl=&quot;kubectl exec -i -n kube-system calicoctl /calicoctl -- &quot;</span><br></pre></td></tr></table></figure><h4 id="6、使命别名执行命令"><a href="#6、使命别名执行命令" class="headerlink" title="6、使命别名执行命令"></a>6、使命别名执行命令</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> calicoctl get nodes</span></span><br><span class="line">NAME         </span><br><span class="line">k8s-master   </span><br><span class="line">k8s-node1    </span><br><span class="line">k8s-node2    </span><br><span class="line">k8s-node3    </span><br></pre></td></tr></table></figure><h4 id="7、测试coredns"><a href="#7、测试coredns" class="headerlink" title="7、测试coredns"></a>7、测试coredns</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> kubectl run curl --image=radial/busyboxplus:curl -it</span> </span><br><span class="line">kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.</span><br><span class="line">If you don&#x27;t see a command prompt, try pressing enter.</span><br><span class="line">[ root@curl-6bf6db5c4f-dqw9x:/ ]$ nslookup kubernetes.default</span><br><span class="line">Server:    10.96.0.10</span><br><span class="line">Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local</span><br><span class="line"></span><br><span class="line">Name:      kubernetes.default</span><br><span class="line">Address 1: 10.96.0.1 kubernetes.default.svc.cluster.local</span><br><span class="line">[ root@curl-6bf6db5c4f-dqw9x:/ ]$ </span><br></pre></td></tr></table></figure><h3 id="安装Dashboard"><a href="#安装Dashboard" class="headerlink" title="安装Dashboard"></a>安装Dashboard</h3><h4 id="1、下载kubernetes-dashboard-yaml文件"><a href="#1、下载kubernetes-dashboard-yaml文件" class="headerlink" title="1、下载kubernetes-dashboard.yaml文件"></a>1、下载<code>kubernetes-dashboard.yaml</code>文件</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> wget https://raw.githubusercontent.com/kubernetes/dashboard/v1.10.1/src/deploy/recommended/kubernetes-dashboard.yaml</span></span><br></pre></td></tr></table></figure><h4 id="2、科学上网"><a href="#2、科学上网" class="headerlink" title="2、科学上网"></a>2、科学上网</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> vim kubernetes-dashboard.yaml</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 修改镜像名称</span></span><br><span class="line">......</span><br><span class="line">containers:</span><br><span class="line">- args:</span><br><span class="line">  - --auto-generate-certificates</span><br><span class="line">  image: registry.aliyuncs.com/google_containers/kubernetes-dashboard-amd64:v1.10.1   # 修改成阿里源</span><br><span class="line">  imagePullPolicy: IfNotPresent</span><br><span class="line">......</span><br><span class="line"><span class="meta">#</span><span class="bash"> 修改Service为NodePort类型</span></span><br><span class="line">......</span><br><span class="line">selector:</span><br><span class="line">  k8s-app: kubernetes-dashboard</span><br><span class="line">type: NodePort</span><br><span class="line">......</span><br></pre></td></tr></table></figure><h4 id="3、安装Dashboard"><a href="#3、安装Dashboard" class="headerlink" title="3、安装Dashboard"></a>3、安装<code>Dashboard</code></h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> kubectl apply -f kubernetes-dashboard.yaml</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> kubectl get pods -n kube-system -l k8s-app=kubernetes-dashboard</span></span><br><span class="line">NAME                                    READY   STATUS    RESTARTS   AGE</span><br><span class="line">kubernetes-dashboard-5dc4c54b55-c7sv2   1/1     Running   0          62s</span><br><span class="line">[root@k8s-master ~]# kubectl get pods,svc -n kube-system -l k8s-app=kubernetes-dashboard</span><br><span class="line">NAME                                        READY   STATUS    RESTARTS   AGE</span><br><span class="line">pod/kubernetes-dashboard-5dc4c54b55-c7sv2   1/1     Running   0          81s</span><br><span class="line"></span><br><span class="line">NAME                           TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)         AGE</span><br><span class="line">service/kubernetes-dashboard   NodePort   10.107.132.165   &lt;none&gt;        443:30114/TCP   80s</span><br></pre></td></tr></table></figure><h4 id="4、创建一个具有全局所有权限的用户来登录Dashboard"><a href="#4、创建一个具有全局所有权限的用户来登录Dashboard" class="headerlink" title="4、创建一个具有全局所有权限的用户来登录Dashboard"></a>4、创建一个具有全局所有权限的用户来登录<code>Dashboard</code></h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> vim rbac.yml</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">  name: admin-user</span><br><span class="line">  namespace: kube-system</span><br><span class="line">---</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">metadata:</span><br><span class="line">  name: admin-user</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: cluster-admin</span><br><span class="line">subjects:</span><br><span class="line">- kind: ServiceAccount</span><br><span class="line">  name: admin-user</span><br><span class="line">  namespace: kube-system</span><br></pre></td></tr></table></figure><p>直接创建</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> kubectl apply -f rbac.yml</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> kubectl get secret -n kube-system|grep admin</span></span><br><span class="line">admin-user-token-wpqck                           kubernetes.io/service-account-token   3      2m38s</span><br></pre></td></tr></table></figure><h4 id="6、获取登录token"><a href="#6、获取登录token" class="headerlink" title="6、获取登录token"></a>6、获取登录<code>token</code></h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk <span class="string">&#x27;&#123;print $1&#125;&#x27;</span>)|grep token:</span></span><br><span class="line">token:      eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi11c2VyLXRva2VuLXdwcWNrIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImFkbWluLXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiIwZmUzNTRiYS1mODkxLTQ0M2UtOWUxMy0wNjE3NWEzNWI5NzMiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06YWRtaW4tdXNlciJ9.Xqq3V8MmFp5ssCw4Hhzxy9YmdiVxMLsMSXnMaklOkVjHiX4B4kVGED2IDmwTh3wEi2VqYxjkRVm-b_JY2AmCfoaPMUtb4R9OZj414volWgAH1CX4VbFB0o1yr6aP3i8SKmQlQfIweEqYUWDGB5zv3ml1u0WjUGyVJeiXY0GxDAtiROelsKaK4gHGsUlIFZ8izW0xmuGDnT9hvCuztqbGsb78bFmQfAL2dKNB12rLOg01EwF0g88jvtfHiv80yX18ulP46ZK0MFx4YhTrLRjq09rAqCSnMGWPHtBSogKUQhpDkwCze61ByKlG6KM_DhFE7elQkBEj-jwvygDwvn7tMQ</span><br></pre></td></tr></table></figure><h4 id="7、使用Firefox浏览器登录Dashboard"><a href="#7、使用Firefox浏览器登录Dashboard" class="headerlink" title="7、使用Firefox浏览器登录Dashboard"></a>7、使用<code>Firefox</code>浏览器登录<code>Dashboard</code></h4><p><img src="https://ljw.howieli.cn/blog/2019-9-1/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20190830152421.png"></p><p><img src="https://ljw.howieli.cn/blog/2019-9-1/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20190830152716.png"></p>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;使用kubeadm方式安装kubernetes&quot;&gt;&lt;a href=&quot;#使用kubeadm方式安装kubernetes&quot; class=&quot;headerlink&quot; title=&quot;使用kubeadm方式安装kubernetes&quot;&gt;&lt;/a&gt;使用&lt;code&gt;kubeadm&lt;/code&gt;方式安装&lt;code&gt;kubernetes&lt;/code&gt;&lt;/h3&gt;</summary>
    
    
    
    <category term="kubernetes" scheme="https://www.lijiawang.org/categories/kubernetes/"/>
    
    
    <category term="使用kubeadm搭建kubernetes1.15" scheme="https://www.lijiawang.org/tags/%E4%BD%BF%E7%94%A8kubeadm%E6%90%AD%E5%BB%BAkubernetes1-15/"/>
    
  </entry>
  
  <entry>
    <title>harbor搭建</title>
    <link href="https://www.lijiawang.org/harbor%E6%90%AD%E5%BB%BA"/>
    <id>https://www.lijiawang.org/harbor%E6%90%AD%E5%BB%BA</id>
    <published>2019-05-31T14:49:17.000Z</published>
    <updated>2019-05-31T14:56:14.193Z</updated>
    
    <content type="html"><![CDATA[<h3 id="harbor介绍"><a href="#harbor介绍" class="headerlink" title="harbor介绍"></a>harbor介绍</h3><p>Harbor是一个用于存储和分发Docker镜像的企业级Registry服务器，通过添加一些企业必需的功能特性，例如安全、标识和管理等，扩展了开源Docker Distribution。作为一个企业级私有Registry服务器，Harbor提供了更好的性能和安全。提升用户使用Registry构建和运行环境传输镜像的效率。Harbor支持安装在多个Registry节点的镜像资源复制，镜像全部保存在私有Registry中， 确保数据和知识产权在公司内部网络中管控。另外，Harbor也提供了高级的安全特性，诸如用户管理，访问控制和活动审计等。</p><a id="more"></a><h3 id="配置网络yum源"><a href="#配置网络yum源" class="headerlink" title="配置网络yum源"></a>配置网络yum源</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cd &#x2F;etc&#x2F;yum.repos.d</span><br><span class="line">rm -rf *</span><br><span class="line">cd</span><br><span class="line">curl -o &#x2F;etc&#x2F;yum.repos.d&#x2F;CentOS-Base.repo http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;repo&#x2F;Centos-7.repo</span><br><span class="line">wget -O &#x2F;etc&#x2F;yum.repos.d&#x2F;epel.repo http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;repo&#x2F;epel-7.repo</span><br></pre></td></tr></table></figure><h3 id="安装docker"><a href="#安装docker" class="headerlink" title="安装docker"></a>安装docker</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># 安装必要的一些系统工具</span><br><span class="line">yum install -y yum-utils device-mapper-persistent-data lvm2</span><br><span class="line"># 添加软件源信息</span><br><span class="line">yum-config-manager --add-repo http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;docker-ce&#x2F;linux&#x2F;centos&#x2F;docker-ce.repo</span><br><span class="line"># 更新并安装 Docker-CE</span><br><span class="line">yum makecache fast</span><br><span class="line">yum -y install docker-ce</span><br><span class="line"># 开启Docker服务</span><br><span class="line">systemctl enable docker</span><br><span class="line">systemctl restart docker</span><br><span class="line">docker version</span><br></pre></td></tr></table></figure><h3 id="安装docker-compose"><a href="#安装docker-compose" class="headerlink" title="安装docker-compose"></a>安装docker-compose</h3><p>github上地址<a href="https://github.com/docker/compose">https://github.com/docker/compose</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">wget https:&#x2F;&#x2F;github.com&#x2F;docker&#x2F;compose&#x2F;releases&#x2F;download&#x2F;1.24.0&#x2F;docker-compose-Linux-x86_64</span><br><span class="line">mv  docker-compose-Linux-x86_64 &#x2F;usr&#x2F;local&#x2F;bin&#x2F;docker-compose</span><br><span class="line">chmod +x &#x2F;usr&#x2F;local&#x2F;bin&#x2F;docker-compose</span><br><span class="line">&#x2F;usr&#x2F;local&#x2F;bin&#x2F;docker-compose version</span><br></pre></td></tr></table></figure><h3 id="安装harbor"><a href="#安装harbor" class="headerlink" title="安装harbor"></a>安装harbor</h3><p>项目地址<a href="https://github.com/goharbor/harbor">https://github.com/goharbor/harbor</a></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">wget https://storage.googleapis.com/harbor-releases/release-1.7.0/harbor-offline-installer-v1.7.5.tgz</span><br><span class="line">tar  xf harbor-offline-installer-v1.7.5.tgz -C /usr/share/</span><br><span class="line">[root@harbor ~]# ll /usr/share/harbor/</span><br><span class="line">total 572840</span><br><span class="line">drwxr-xr-x. 3 root root        23 May 14 16:08 common</span><br><span class="line">-rw-r--r--. 1 root root       939 Apr  1 12:07 docker-compose.chartmuseum.yml</span><br><span class="line">-rw-r--r--. 1 root root       975 Apr  1 12:07 docker-compose.clair.yml</span><br><span class="line">-rw-r--r--. 1 root root      1434 Apr  1 12:07 docker-compose.notary.yml</span><br><span class="line">-rw-r--r--. 1 root root      5608 Apr  1 12:07 docker-compose.yml</span><br><span class="line">-rw-r--r--. 1 root root      8033 Apr  1 12:07 harbor.cfg</span><br><span class="line">-rw-r--r--. 1 root root 585234819 Apr  1 12:08 harbor.v1.7.5.tar.gz</span><br><span class="line">-rwxr-xr-x. 1 root root      5739 Apr  1 12:07 install.sh</span><br><span class="line">-rw-r--r--. 1 root root     11347 Apr  1 12:07 LICENSE</span><br><span class="line">-rw-r--r--. 1 root root   1263409 Apr  1 12:07 open_source_license</span><br><span class="line">-rwxr-xr-x. 1 root root     36337 Apr  1 12:07 prepare</span><br></pre></td></tr></table></figure><h3 id="修改harbor配置文件harbor-cfg"><a href="#修改harbor配置文件harbor-cfg" class="headerlink" title="修改harbor配置文件harbor.cfg"></a>修改harbor配置文件harbor.cfg</h3><p>在刚才解压完的目录下有harbor配置文件: harbor.cfg, 这里有几处必要配置需要修改:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> hostname设置访问地址，可以使用ip、域名，不可以设置为127.0.0.1或localhost</span></span><br><span class="line">hostname = 192.168.1.109</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 访问协议，默认是http，也可以设置https，如果设置https，则nginx ssl需要设置on</span></span><br><span class="line">ui_url_protocol = http</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> mysql数据库root用户默认密码root123，实际使用时修改下</span></span><br><span class="line">db_password = root123</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 启动Harbor后，管理员UI登录的密码，默认是Harbor12345</span></span><br><span class="line">harbor_admin_password = Harbor12345</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 是否开启自注册</span></span><br><span class="line">self_registration = on</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Token有效时间，默认30分钟</span></span><br><span class="line">token_expiration = 30</span><br></pre></td></tr></table></figure><h3 id="开始安装"><a href="#开始安装" class="headerlink" title="开始安装"></a>开始安装</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd &#x2F;usr&#x2F;share&#x2F;harbor&#x2F;</span><br><span class="line">.&#x2F;install.sh</span><br></pre></td></tr></table></figure><p><img src="https://ljw.howieli.cn/blog/2019-05-14/harborlogin.png"><br><img src="https://ljw.howieli.cn/blog/2019-05-14/harbor1.png"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cat &#x2F;etc&#x2F;docker&#x2F;daemon.json </span><br><span class="line">&#123;</span><br><span class="line">  &quot;insecure-registries&quot;: [&quot;10.122.52.227&quot;]</span><br><span class="line">&#125;</span><br><span class="line"># 重启docker服务</span><br><span class="line">systemctl restart docker</span><br><span class="line">systemctl restart docker</span><br></pre></td></tr></table></figure><h3 id="将镜像push到Harbor"><a href="#将镜像push到Harbor" class="headerlink" title="将镜像push到Harbor"></a>将镜像push到Harbor</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">docker images|grep centos</span><br><span class="line">centos                          latest              9f38484d220f        2 months ago        202MB</span><br><span class="line"></span><br><span class="line">docker tag centos:latest 10.122.52.227&#x2F;library&#x2F;centos:latest</span><br><span class="line"></span><br><span class="line"># docker images|grep centos</span><br><span class="line">10.122.52.227&#x2F;library&#x2F;centos    latest              9f38484d220f        2 months ago        202MB</span><br><span class="line">centos                          latest              9f38484d220f        2 months ago        202MB</span><br><span class="line"></span><br><span class="line">docker push 10.122.52.227&#x2F;library&#x2F;centos:latest</span><br></pre></td></tr></table></figure><p><img src="https://ljw.howieli.cn/blog/2019-05-14/login.png"><br><img src="https://ljw.howieli.cn/blog/2019-05-14/harbor2.png"></p>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;harbor介绍&quot;&gt;&lt;a href=&quot;#harbor介绍&quot; class=&quot;headerlink&quot; title=&quot;harbor介绍&quot;&gt;&lt;/a&gt;harbor介绍&lt;/h3&gt;&lt;p&gt;Harbor是一个用于存储和分发Docker镜像的企业级Registry服务器，通过添加一些企业必需的功能特性，例如安全、标识和管理等，扩展了开源Docker Distribution。作为一个企业级私有Registry服务器，Harbor提供了更好的性能和安全。提升用户使用Registry构建和运行环境传输镜像的效率。Harbor支持安装在多个Registry节点的镜像资源复制，镜像全部保存在私有Registry中， 确保数据和知识产权在公司内部网络中管控。另外，Harbor也提供了高级的安全特性，诸如用户管理，访问控制和活动审计等。&lt;/p&gt;</summary>
    
    
    
    <category term="docker" scheme="https://www.lijiawang.org/categories/docker/"/>
    
    
    <category term="docker" scheme="https://www.lijiawang.org/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>使用Kubeadm安装最新版Kubernetes 1.12</title>
    <link href="https://www.lijiawang.org/%E4%BD%BF%E7%94%A8Kubeadm%E5%AE%89%E8%A3%85%E6%9C%80%E6%96%B0%E7%89%88Kubernetes%201.12"/>
    <id>https://www.lijiawang.org/%E4%BD%BF%E7%94%A8Kubeadm%E5%AE%89%E8%A3%85%E6%9C%80%E6%96%B0%E7%89%88Kubernetes%201.12</id>
    <published>2018-10-25T13:40:57.000Z</published>
    <updated>2018-10-25T13:42:45.347Z</updated>
    
    <content type="html"><![CDATA[<h3 id="使用Kubeadm安装最新版Kubernetes-1-12"><a href="#使用Kubeadm安装最新版Kubernetes-1-12" class="headerlink" title="使用Kubeadm安装最新版Kubernetes 1.12"></a>使用Kubeadm安装最新版Kubernetes 1.12</h3><blockquote><p>kubeadm是Kubernetes官方提供的用于快速安装Kubernetes集群的工具，伴随Kubernetes每个版本的发布都会同步更新，kubeadm会对集群配置方面的一些实践做调整，通过实验kubeadm可以学习到Kubernetes官方在集群配置上一些新的最佳实践。</p></blockquote><a id="more"></a><h3 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h3><h4 id="系统配置"><a href="#系统配置" class="headerlink" title="系统配置"></a>系统配置</h4><p>我这有三台centos7.5主机如下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">172.20.16.243  k8smaster</span><br><span class="line">172.20.14.51   node01</span><br><span class="line">172.20.14.92   node02</span><br></pre></td></tr></table></figure><p>操作系统都为centos7.5</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> cat /etc/redhat-release</span> </span><br><span class="line">CentOS Linux release 7.5.1804 (Core)</span><br><span class="line"><span class="meta">#</span><span class="bash"> uname -a</span></span><br><span class="line">Linux k8smaster 3.10.0-862.14.4.el7.x86_64 #1 SMP Wed Sep 26 15:12:11 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux</span><br></pre></td></tr></table></figure><h4 id="环境说明"><a href="#环境说明" class="headerlink" title="环境说明"></a>环境说明</h4><p>测试使用的Kubernetes集群可由一个master主机及一个以上（建议至少两个）node主机组成，本测试环境将由k8smaster、node01和node02三个独立的主机组成，它们分别拥有8核心的CPU及8G的内存资源。</p><h4 id="初始环境"><a href="#初始环境" class="headerlink" title="初始环境"></a>初始环境</h4><p>1.</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> yum -y update</span></span><br></pre></td></tr></table></figure><p>2.借助于NTP服务设定各节点时间精确同步；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum -y install ntpd</span><br></pre></td></tr></table></figure><p>3.配置hosts解析;</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> cat /etc/hosts</span></span><br><span class="line">127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4</span><br><span class="line">::1         localhost localhost.localdomain localhost6 localhost6.localdomain6</span><br><span class="line">172.20.16.243  k8smaster</span><br><span class="line">172.20.14.51   node01</span><br><span class="line">172.20.14.92   node02</span><br></pre></td></tr></table></figure><p>4.关闭各节点的iptables或firewalld服务，并确保它们被禁止随系统引导过程启动；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">systemctl stop firewalld.service </span><br><span class="line">systemctl stop iptables.service</span><br><span class="line">systemctl disable firewalld.service</span><br><span class="line">systemctl disable iptables.service</span><br></pre></td></tr></table></figure><p>5.各节点禁用SELinux；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sed -i &#x27;s@^\(SELINUX=\).*@\1disabled@&#x27; /etc/sysconfig/selinux</span><br><span class="line">setenforce 0</span><br></pre></td></tr></table></figure><p>6.禁用Swap设备;</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> swapoff -a</span></span><br><span class="line">而后编辑/etc/fstab配置文件，注释用于挂载Swap设备的所有行;</span><br><span class="line"><span class="meta">#</span><span class="bash"> free -m</span> </span><br><span class="line">              total        used        free      shared  buff/cache   available</span><br><span class="line">Mem:           7805         750        6061          18         993        6655</span><br><span class="line">Swap:             0           0           0</span><br></pre></td></tr></table></figure><p>7.若要使用ipvs模型的proxy，各节点还需要载入ipvs相关的各模块；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> cat /etc/sysconfig/modules/ipvs.modules</span></span><br><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line">ipvs_modules_dir=&quot;/usr/lib/modules/$(uname -r)/kernel/net/netfilter/ipvs&quot;</span><br><span class="line">for i in $(ls $ipvs_modules_dir | sed  -r &#x27;s@(.*).ko.xz@\1@&#x27;); do</span><br><span class="line">    /sbin/modinfo -F filename $i  &amp;&gt; /dev/null</span><br><span class="line">    if [ $? -eq 0 ]; then</span><br><span class="line">        /sbin/modprobe $i</span><br><span class="line">    fi</span><br><span class="line">done</span><br><span class="line"><span class="meta">#</span><span class="bash"> chmod +x /etc/sysconfig/modules/ipvs.modules</span> </span><br><span class="line"><span class="meta">#</span><span class="bash"> bash /etc/sysconfig/modules/ipvs.modules</span> </span><br></pre></td></tr></table></figure><h3 id="安装k8s相关的软件包-在各节点执行"><a href="#安装k8s相关的软件包-在各节点执行" class="headerlink" title="安装k8s相关的软件包(在各节点执行)"></a>安装k8s相关的软件包(在各节点执行)</h3><h4 id="获取docker-ce的配置仓库文件"><a href="#获取docker-ce的配置仓库文件" class="headerlink" title="获取docker-ce的配置仓库文件"></a>获取docker-ce的配置仓库文件</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> wget https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo -O /etc/yum.repos.d/docker.repo</span></span><br></pre></td></tr></table></figure><h4 id="配置kubernetes的yum仓库"><a href="#配置kubernetes的yum仓库" class="headerlink" title="配置kubernetes的yum仓库"></a>配置kubernetes的yum仓库</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># cat &#x2F;etc&#x2F;yum.repos.d&#x2F;kubernetes.repo </span><br><span class="line">[kubernetes]</span><br><span class="line">name&#x3D;Kubernetes</span><br><span class="line">baseurl&#x3D;https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;kubernetes&#x2F;yum&#x2F;repos&#x2F;kubernetes-el7-x86_64&#x2F;</span><br><span class="line">gpgcheck&#x3D;1</span><br><span class="line">gpgkey&#x3D;https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;kubernetes&#x2F;yum&#x2F;doc&#x2F;rpm-package-key.gpg</span><br><span class="line">enabled&#x3D;1</span><br></pre></td></tr></table></figure><h4 id="安装相关软件包"><a href="#安装相关软件包" class="headerlink" title="安装相关软件包"></a>安装相关软件包</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> yum install -y --<span class="built_in">setopt</span>=obsoletes=0 docker-ce-17.03.2.ce docker-ce-selinux-17.03.2.ce</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> yum install kubelet kubeadm kubectl</span></span><br></pre></td></tr></table></figure><h3 id="配置并启动docker服务-在各节点执行"><a href="#配置并启动docker服务-在各节点执行" class="headerlink" title="配置并启动docker服务(在各节点执行)"></a>配置并启动docker服务(在各节点执行)</h3><p>1.编辑systemctl的Docker启动文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sed -i &quot;13i ExecStartPost=/usr/sbin/iptables -P FORWARD ACCEPT&quot; /usr/lib/systemd/system/docker.service</span><br><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl start docker.service</span><br></pre></td></tr></table></figure><p>2.配置Docker源</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p /etc/docker</span><br><span class="line">echo -e &#x27;&#123;\n&quot;insecure-registries&quot;:[&quot;k8s.gcr.io&quot;, &quot;gcr.io&quot;, &quot;quay.io&quot;]\n&#125;&#x27; &gt;/etc/docker/daemon.json</span><br><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl restart docker</span><br></pre></td></tr></table></figure><p>3.设置docker和kubelet服务开机自启动</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> systemctl <span class="built_in">enable</span> docker kubelet</span></span><br></pre></td></tr></table></figure><h3 id="初始化master节点"><a href="#初始化master节点" class="headerlink" title="初始化master节点"></a>初始化master节点</h3><p>在运行初始化命令之前先运行如下命令单独获取相关的镜像文件，而后再运行后面的kubeadm init命令，以便于观察到镜像文件的下载过程(可选择)。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> kubeadm config images pull</span></span><br></pre></td></tr></table></figure><p>使用kubeadm初始化集群，选择k8smaster作为Master Node，在k8smaster上执行下面的命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kubeadm init --kubernetes-version=v1.12.1 \</span><br><span class="line">    --pod-network-cidr=10.244.0.0/16 \</span><br><span class="line">--service-cidr=10.96.0.0/12 </span><br></pre></td></tr></table></figure><p>命令中的各选项简单说明如下：<br>    (1) –kubernetes-version选项的版本号用于指定要部署的Kubenretes程序版本，它需要与当前的kubeadm支持的版本保持一致；<br>    (2) –pod-network-cidr选项用于指定分Pod分配使用的网络地址，它通常应该与要部署使用的网络插件（例如flannel、calico等）的默认设定保持一致，10.244.0.0/16是flannel默认使用的网络；<br>    (3) –service-cidr用于指定为Service分配使用的网络地址，它由kubernetes管理，默认即为10.96.0.0/12.<br>命令运行结束后，请记录最后的kubeadm join命令输出的最后提示的操作步骤,这是初始化node节点要用到的。</p><h3 id="初始化kubectl"><a href="#初始化kubectl" class="headerlink" title="初始化kubectl"></a>初始化kubectl</h3><p>kubectl是kube-apiserver的命令行客户端程序，实现了除系统部署之外的几乎全部的管理操作，是kubernetes管理员使用最多的命令之一。kubectl需经由API server认证及授权后方能执行相应的管理操作，kubeadm部署的集群为其生成了一个具有管理员权限的认证配置文件/etc/kubernetes/admin.conf，它可由kubectl通过默认的“$HOME/.kube/config”的路径进行加载。当然，用户也可在kubectl命令上使用–kubeconfig选项指定一个别的位置<br>复制认证为Kubernetes系统管理员的配置文件至目标用户（例如当前用户root）的家目录下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p $HOME/.kube</span><br><span class="line">sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config</span><br><span class="line">sudo chown $(id -u):$(id -g) $HOME/.kube/config</span><br></pre></td></tr></table></figure><h3 id="添加flannel网络插件"><a href="#添加flannel网络插件" class="headerlink" title="添加flannel网络插件"></a>添加flannel网络插件</h3><p>Kubernetes系统上Pod网络的实现依赖于第三方插件进行，这类插件有近数十种之多，较为著名的有flannel、calico、canal和kube-router等，简单易用的实现是为CoreOS提供的flannel项目。下面的命令用于在线部署flannel至Kubernetes系统之上：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml</span></span><br></pre></td></tr></table></figure><p>检查flannel的pod状态</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">  kubectl get pods -n kube-system -l app=flannel</span></span><br><span class="line">NAME                          READY   STATUS    RESTARTS   AGE</span><br><span class="line">kube-flannel-ds-amd64-95wzl   1/1     Running   1          22h</span><br></pre></td></tr></table></figure><p>验证master节点是否就绪</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> kubectl get nodes</span></span><br><span class="line">NAME        STATUS   ROLES    AGE   VERSION</span><br><span class="line">k8smaster   Ready    master   22h   v1.12.1</span><br></pre></td></tr></table></figure><h3 id="添加node节点到集群中-在node01和node02上执行"><a href="#添加node节点到集群中-在node01和node02上执行" class="headerlink" title="添加node节点到集群中(在node01和node02上执行)"></a>添加node节点到集群中(在node01和node02上执行)</h3><p>添加node节点</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> kubeadm join 172.20.16.243:6443 --token 7l22o0.x1rwp6ffh30tppyz --discovery-token-ca-cert-hash sha256:20efefe8d3ef236219dab4c09e26d933fd9b8754099467babaa668076a230f49</span></span><br></pre></td></tr></table></figure><p>检查node节点是否加入集群(在master节点上执行)</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> kubectl get nodes</span></span><br><span class="line">NAME        STATUS   ROLES    AGE   VERSION</span><br><span class="line">k8smaster   Ready    master   22h   v1.12.1</span><br><span class="line">node01      Ready    &lt;none&gt;   20h   v1.12.1</span><br><span class="line">node02      Ready    &lt;none&gt;   20h   v1.12.1</span><br></pre></td></tr></table></figure><h3 id="如何从集群中移除Node"><a href="#如何从集群中移除Node" class="headerlink" title="如何从集群中移除Node"></a>如何从集群中移除Node</h3><p>如果需要从集群中移除node02这个Node执行下面的命令：<br>在master节点上执行：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> kubectl drain node02 --delete-local-data --force --ignore-daemonsets</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> kubectl delete node node02</span></span><br></pre></td></tr></table></figure><p>在node02上执行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">kubeadm reset</span><br><span class="line">ifconfig cni0 down</span><br><span class="line">ip link delete cni0</span><br><span class="line">ifconfig flannel.1 down</span><br><span class="line">ip link delete flannel.1</span><br><span class="line">rm -rf /var/lib/cni/</span><br></pre></td></tr></table></figure><h3 id="查看k8s相关的服务是否正常"><a href="#查看k8s相关的服务是否正常" class="headerlink" title="查看k8s相关的服务是否正常"></a>查看k8s相关的服务是否正常</h3><p>几乎所有的kubernetes组件本身也在pod里运行，执行以下命令。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> kubectl get pod --all-namespaces -o wide</span></span><br><span class="line">NAMESPACE     NAME                                READY   STATUS    RESTARTS   AGE   IP              NODE        NOMINATED NODE</span><br><span class="line">default       myapp-69587444dd-6jdr5              1/1     Running   1          20h   10.244.2.3      node02      &lt;none&gt;</span><br><span class="line">kube-system   coredns-576cbf47c7-5wfbp            1/1     Running   30         23h   10.244.0.10     k8smaster   &lt;none&gt;</span><br><span class="line">kube-system   coredns-576cbf47c7-qfb87            1/1     Running   30         23h   10.244.0.11     k8smaster   &lt;none&gt;</span><br><span class="line">kube-system   etcd-k8smaster                      1/1     Running   1          23h   172.20.16.243   k8smaster   &lt;none&gt;</span><br><span class="line">kube-system   kube-apiserver-k8smaster            1/1     Running   1          23h   172.20.16.243   k8smaster   &lt;none&gt;</span><br><span class="line">kube-system   kube-controller-manager-k8smaster   1/1     Running   1          23h   172.20.16.243   k8smaster   &lt;none&gt;</span><br><span class="line">kube-system   kube-flannel-ds-amd64-95wzl         1/1     Running   1          23h   172.20.16.243   k8smaster   &lt;none&gt;</span><br><span class="line">kube-system   kube-flannel-ds-amd64-lc6x7         1/1     Running   1          20h   172.20.14.92    node02      &lt;none&gt;</span><br><span class="line">kube-system   kube-flannel-ds-amd64-xdvxf         1/1     Running   1          20h   172.20.14.51    node01      &lt;none&gt;</span><br><span class="line">kube-system   kube-proxy-9cq4d                    1/1     Running   1          23h   172.20.16.243   k8smaster   &lt;none&gt;</span><br><span class="line">kube-system   kube-proxy-wczgd                    1/1     Running   1          20h   172.20.14.51    node01      &lt;none&gt;</span><br><span class="line">kube-system   kube-proxy-xnxxm                    1/1     Running   1          20h   172.20.14.92    node02      &lt;none&gt;</span><br><span class="line">kube-system   kube-scheduler-k8smaster            1/1     Running   1          23h   172.20.16.243   k8smaster   &lt;none&gt;</span><br><span class="line">kube-system   tiller-deploy-6f6fd74b68-qndkl      1/1     Running   0          19m   10.244.1.2      node01      &lt;none&gt;</span><br></pre></td></tr></table></figure><p>kubelet是唯一没有以容器形式运行的kubernetes组件，在centos7中可以通过systemd服务运行：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> systemctl status kubelet.service</span></span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;使用Kubeadm安装最新版Kubernetes-1-12&quot;&gt;&lt;a href=&quot;#使用Kubeadm安装最新版Kubernetes-1-12&quot; class=&quot;headerlink&quot; title=&quot;使用Kubeadm安装最新版Kubernetes 1.12&quot;&gt;&lt;/a&gt;使用Kubeadm安装最新版Kubernetes 1.12&lt;/h3&gt;&lt;blockquote&gt;
&lt;p&gt;kubeadm是Kubernetes官方提供的用于快速安装Kubernetes集群的工具，伴随Kubernetes每个版本的发布都会同步更新，kubeadm会对集群配置方面的一些实践做调整，通过实验kubeadm可以学习到Kubernetes官方在集群配置上一些新的最佳实践。&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="kubernetes" scheme="https://www.lijiawang.org/categories/kubernetes/"/>
    
    
    <category term="kubernetes" scheme="https://www.lijiawang.org/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>kolla部署openstack对接华为san存储</title>
    <link href="https://www.lijiawang.org/kolla%E9%83%A8%E7%BD%B2openstack%E5%AF%B9%E6%8E%A5%E5%8D%8E%E4%B8%BAsan%E5%AD%98%E5%82%A8"/>
    <id>https://www.lijiawang.org/kolla%E9%83%A8%E7%BD%B2openstack%E5%AF%B9%E6%8E%A5%E5%8D%8E%E4%B8%BAsan%E5%AD%98%E5%82%A8</id>
    <published>2018-08-02T03:25:25.000Z</published>
    <updated>2018-08-02T03:37:03.897Z</updated>
    
    <content type="html"><![CDATA[<h3 id="kolla部署openstack对接华为san存储"><a href="#kolla部署openstack对接华为san存储" class="headerlink" title="kolla部署openstack对接华为san存储"></a>kolla部署openstack对接华为san存储</h3><p>openstack O版对接华为SAN存储（kolla部署模式）</p><a id="more"></a><h3 id="修改globals-yml文件"><a href="#修改globals-yml文件" class="headerlink" title="修改globals.yml文件"></a>修改globals.yml文件</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> grep enable_multipathd /etc/kolla/globals.yml</span> </span><br><span class="line">enable_multipathd: &quot;yes&quot;</span><br></pre></td></tr></table></figure><h3 id="修改multinode文件"><a href="#修改multinode文件" class="headerlink" title="修改multinode文件"></a>修改multinode文件</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> vim multinode</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Multipathd</span></span><br><span class="line">[multipathd:children]</span><br><span class="line">control</span><br><span class="line">compute</span><br></pre></td></tr></table></figure><h3 id="修改config下的cinder-conf文件"><a href="#修改config下的cinder-conf文件" class="headerlink" title="修改config下的cinder.conf文件"></a>修改config下的cinder.conf文件</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> cat /etc/kolla/config/cinder.conf</span> </span><br><span class="line">[DEFAULT]</span><br><span class="line">enabled_backends = rbd-1,18000V1_FC,18000V1_FC_SAS01,18000V1_FC_SAS02</span><br><span class="line">host = drkmcontrol</span><br><span class="line"></span><br><span class="line">[18000V1_FC]</span><br><span class="line">volume_driver = cinder.volume.drivers.huawei.huawei_driver.HuaweiFCDriver</span><br><span class="line">cinder_huawei_conf_file = /etc/cinder/cinder_huawei_conf.xml</span><br><span class="line">volume_backend_name = 18000V1_FC</span><br><span class="line"></span><br><span class="line">[18000V1_FC_SAS01]</span><br><span class="line">volume_driver = cinder.volume.drivers.huawei.huawei_driver.HuaweiFCDriver</span><br><span class="line">cinder_huawei_conf_file = /etc/cinder/cinder_huawei_sas01_conf.xml</span><br><span class="line">volume_backend_name = 18000V1_FC_SAS01</span><br><span class="line"></span><br><span class="line">[18000V1_FC_SAS02]</span><br><span class="line">volume_driver = cinder.volume.drivers.huawei.huawei_driver.HuaweiFCDriver</span><br><span class="line">cinder_huawei_conf_file = /etc/cinder/cinder_huawei_sas02_conf.xml</span><br><span class="line">volume_backend_name = 18000V1_FC_SAS02</span><br></pre></td></tr></table></figure><h3 id="编写xml文件"><a href="#编写xml文件" class="headerlink" title="编写xml文件"></a>编写xml文件</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> cat /etc/kolla/cinder-volume/cinder_huawei_sas01_conf.xml</span> </span><br><span class="line">&lt;?xml version=&#x27;1.0&#x27; encoding=&#x27;UTF-8&#x27;?&gt;</span><br><span class="line">&lt;config&gt;</span><br><span class="line">    &lt;Storage&gt;</span><br><span class="line">        &lt;Product&gt;18000&lt;/Product&gt;</span><br><span class="line">        &lt;Protocol&gt;FC&lt;/Protocol&gt;</span><br><span class="line">        &lt;RestURL&gt;https://10.15.188.100:8088/deviceManager/rest/&lt;/RestURL&gt;</span><br><span class="line">        &lt;UserName&gt;openstack&lt;/UserName&gt;</span><br><span class="line">        &lt;UserPassword&gt;VFR$5tgb3$&lt;/UserPassword&gt;</span><br><span class="line">    &lt;/Storage&gt;</span><br><span class="line">    &lt;LUN&gt; </span><br><span class="line">        &lt;StoragePool&gt;StoragePool_SMB0_1;StoragePool_SMB0_2;StoragePool_SMB0_3;StoragePool_SMB0_4&lt;/StoragePool&gt;</span><br><span class="line">    &lt;/LUN&gt;</span><br><span class="line">&lt;/config&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> cat /etc/kolla/cinder-volume/cinder_huawei_sas02_conf.xml</span> </span><br><span class="line">&lt;?xml version=&#x27;1.0&#x27; encoding=&#x27;UTF-8&#x27;?&gt;</span><br><span class="line">&lt;config&gt;</span><br><span class="line">    &lt;Storage&gt;</span><br><span class="line">        &lt;Product&gt;18000&lt;/Product&gt;</span><br><span class="line">        &lt;Protocol&gt;FC&lt;/Protocol&gt;</span><br><span class="line">        &lt;RestURL&gt;https://10.15.188.100:8088/deviceManager/rest/&lt;/RestURL&gt;</span><br><span class="line">        &lt;UserName&gt;openstack&lt;/UserName&gt;</span><br><span class="line">        &lt;UserPassword&gt;VFR$5tgb3$&lt;/UserPassword&gt;</span><br><span class="line">    &lt;/Storage&gt;</span><br><span class="line">    &lt;LUN&gt; </span><br><span class="line">        &lt;StoragePool&gt;StoragePool_SMB0_5;StoragePool_SMB1_1;StoragePool_SMB1_2&lt;/StoragePool&gt;</span><br><span class="line">    &lt;/LUN&gt;</span><br><span class="line">&lt;/config&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> cat /etc/kolla/cinder-volume/cinder_huawei_conf.xml</span> </span><br><span class="line">&lt;?xml version=&#x27;1.0&#x27; encoding=&#x27;UTF-8&#x27;?&gt;</span><br><span class="line">&lt;config&gt;</span><br><span class="line">    &lt;Storage&gt;</span><br><span class="line">        &lt;Product&gt;18000&lt;/Product&gt;</span><br><span class="line">        &lt;Protocol&gt;FC&lt;/Protocol&gt;</span><br><span class="line">        &lt;RestURL&gt;https://10.15.188.100:8088/deviceManager/rest/&lt;/RestURL&gt;</span><br><span class="line">        &lt;UserName&gt;openstack&lt;/UserName&gt;</span><br><span class="line">        &lt;UserPassword&gt;VFR$5tgb3$&lt;/UserPassword&gt;</span><br><span class="line">    &lt;/Storage&gt;</span><br><span class="line">    &lt;LUN&gt; </span><br><span class="line">        &lt;StoragePool&gt;StoragePool_SMB0_8&lt;/StoragePool&gt;</span><br><span class="line">    &lt;/LUN&gt;</span><br><span class="line">&lt;/config&gt;</span><br></pre></td></tr></table></figure><h3 id="修改cinder-volume-json-j2-增加以下内容"><a href="#修改cinder-volume-json-j2-增加以下内容" class="headerlink" title="修改cinder-volume.json.j2,增加以下内容"></a>修改cinder-volume.json.j2,增加以下内容</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> vim /usr/share/kolla-ansible/ansible/roles/cinder/templates/cinder-volume.json.j2</span></span><br><span class="line">        &#123;</span><br><span class="line">            &quot;source&quot;: &quot;&#123;&#123; container_config_directory &#125;&#125;/cinder_*_conf.xml&quot;,</span><br><span class="line">            &quot;dest&quot;: &quot;/etc/cinder/&quot;,</span><br><span class="line">            &quot;owner&quot;: &quot;cinder&quot;,</span><br><span class="line">            &quot;perm&quot;: &quot;0600&quot;</span><br><span class="line">        &#125;,</span><br></pre></td></tr></table></figure><h3 id="利用ansible下发配置文件"><a href="#利用ansible下发配置文件" class="headerlink" title="利用ansible下发配置文件"></a>利用ansible下发配置文件</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> ansible -i multinode control -m copy -a <span class="string">&quot;src=/etc/kolla/cinder-volume/cinder_huawei_* dest=/etc/kolla/cinder-volume/&quot;</span></span></span><br></pre></td></tr></table></figure><h3 id="部署cinder"><a href="#部署cinder" class="headerlink" title="部署cinder"></a>部署cinder</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> kolla-ansible -i multinode reconfigure -t cinder</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> cinder service-list</span></span><br><span class="line">+------------------+------------------------------+------+---------+-------+----------------------------+-----------------+</span><br><span class="line">| Binary           | Host                         | Zone | Status  | State | Updated_at                 | Disabled Reason |</span><br><span class="line">+------------------+------------------------------+------+---------+-------+----------------------------+-----------------+</span><br><span class="line">| cinder-backup    | drkmcontrol                  | nova | enabled | up    | 2018-06-27T02:13:58.000000 | -               |</span><br><span class="line">| cinder-scheduler | drkmcontrol                  | nova | enabled | up    | 2018-06-27T02:13:52.000000 | -               |</span><br><span class="line">| cinder-volume    | drkmcontrol@18000V1_FC       | nova | enabled | up    | 2018-06-27T02:13:58.000000 | -               |</span><br><span class="line">| cinder-volume    | drkmcontrol@18000V1_FC_SAS01 | nova | enabled | up    | 2018-06-27T02:13:51.000000 | -               |</span><br><span class="line">| cinder-volume    | drkmcontrol@18000V1_FC_SAS02 | nova | enabled | up    | 2018-06-27T02:13:51.000000 | -               |</span><br><span class="line">| cinder-volume    | drkmcontrol@rbd-1            | nova | enabled | up    | 2018-06-27T02:13:56.000000 | -               |</span><br><span class="line">+------------------+------------------------------+------+---------+-------+----------------------------+-----------------+</span><br></pre></td></tr></table></figure><h3 id="查看HBA卡ww暗号"><a href="#查看HBA卡ww暗号" class="headerlink" title="查看HBA卡ww暗号"></a>查看HBA卡ww暗号</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@drkm01070104u config]# cat /sys/class/fc_host/host2/port_id </span><br><span class="line">0x3f0000</span><br><span class="line">[root@drkm01070104u config]# cat /sys/class/fc_host/host1/port_id </span><br><span class="line">0x350000</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="配置cinder多后端"><a href="#配置cinder多后端" class="headerlink" title="配置cinder多后端"></a>配置cinder多后端</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> cinder type-create Ceph</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> cinder type-list</span></span><br><span class="line">+--------------------------------------+------------------+-------------+-----------+</span><br><span class="line">| ID                                   | Name             | Description | Is_Public |</span><br><span class="line">+--------------------------------------+------------------+-------------+-----------+</span><br><span class="line">| 2996c5a8-e312-42b8-b7f1-ca649a32eadb | 标准云盘FC01-SAN |             | True      |</span><br><span class="line">| 8f77dc78-689d-40ae-a673-dba37eaf7004 | 标准云盘FC02-SAN |             | True      |</span><br><span class="line">| ac318ed1-7de2-4ebb-8233-c35301ba77bd | Ceph             |             | True      |</span><br><span class="line">| bb5298dc-3790-4f06-8e26-b28c3fc7b56e | 高性能FC-SAN     |             | True      |</span><br><span class="line">+--------------------------------------+------------------+-------------+-----------+</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> cinder type-key Ceph <span class="built_in">set</span> volume_backend_name=rbd-1</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> cinder  extra-specs-list</span></span><br><span class="line">+--------------------------------------+------------------+---------------------------------------------+</span><br><span class="line">| ID                                   | Name             | extra_specs                                 |</span><br><span class="line">+--------------------------------------+------------------+---------------------------------------------+</span><br><span class="line">| 2996c5a8-e312-42b8-b7f1-ca649a32eadb | 标准云盘FC01-SAN | &#123;&#x27;volume_backend_name&#x27;: &#x27;18000V1_FC_SAS01&#x27;&#125; |</span><br><span class="line">| 8f77dc78-689d-40ae-a673-dba37eaf7004 | 标准云盘FC02-SAN | &#123;&#x27;volume_backend_name&#x27;: &#x27;18000V1_FC_SAS02&#x27;&#125; |</span><br><span class="line">| ac318ed1-7de2-4ebb-8233-c35301ba77bd | Ceph             | &#123;&#x27;volume_backend_name&#x27;: &#x27;rbd-1&#x27;&#125;            |</span><br><span class="line">| bb5298dc-3790-4f06-8e26-b28c3fc7b56e | 高性能FC-SAN     | &#123;&#x27;volume_backend_name&#x27;: &#x27;18000V1_FC&#x27;&#125;       |</span><br><span class="line">+--------------------------------------+------------------+---------------------------------------------+</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;kolla部署openstack对接华为san存储&quot;&gt;&lt;a href=&quot;#kolla部署openstack对接华为san存储&quot; class=&quot;headerlink&quot; title=&quot;kolla部署openstack对接华为san存储&quot;&gt;&lt;/a&gt;kolla部署openstack对接华为san存储&lt;/h3&gt;&lt;p&gt;openstack O版对接华为SAN存储（kolla部署模式）&lt;/p&gt;</summary>
    
    
    
    <category term="openstack" scheme="https://www.lijiawang.org/categories/openstack/"/>
    
    
    <category term="openstack" scheme="https://www.lijiawang.org/tags/openstack/"/>
    
  </entry>
  
  <entry>
    <title>docker网络</title>
    <link href="https://www.lijiawang.org/docker%E7%BD%91%E7%BB%9C"/>
    <id>https://www.lijiawang.org/docker%E7%BD%91%E7%BB%9C</id>
    <published>2018-06-14T01:44:58.000Z</published>
    <updated>2018-06-14T07:19:58.139Z</updated>
    
    <content type="html"><![CDATA[<h3 id="docker四种单节点网络"><a href="#docker四种单节点网络" class="headerlink" title="docker四种单节点网络"></a>docker四种单节点网络</h3><p><code>Docker</code>在创建容器时有四种网络模式，<code>bridge</code>为默认不需要用<code>-–net</code>去指定，其他三种模式需要在创建容器时使用<code>–-net</code>去指定。<br>四种单节点网络模式如下：<br><code>none</code>模式，使用<code>–-net=none</code>指定；<br><code>host</code>模式，使用<code>–-net=host</code>指定；<br><code>bridge</code>模式，使用<code>–-net=bridge</code>指定，默认设置；<br><code>container</code>模式，使用<code>–-net=container</code>:容器名称或ID指定。<br>在<code>Docker</code>安装时会自动在<code>host</code>上创建三个网络,我们可以通过<code>docker network ls</code>查看：<br><img src="https://ljw.howieli.cn/blog/2018-06-12/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20180612122600.png"></p><a id="more"></a><h3 id="none网络"><a href="#none网络" class="headerlink" title="none网络"></a>none网络</h3><p><code>none</code> 网络就是什么都没有的网络。挂在这个网络下的容器除了 <code>lo</code>，没有其他任何网卡。容器创建时，可以通过 <code>--network=none</code> 指定使用 <code>none</code> 网络。<br><img src="https://ljw.howieli.cn/blog/2018-06-12/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20180612155934.png"><br>此模式下创建容器是不会为容器配置任何网络参数的，如：容器网卡、<code>IP</code>、通信路由等，全部需要自己去配置。<br>其应用场景，封闭意味着隔离，一些对安全性要求高并且不需要联网的应用可以使用 <code>none</code> 网络，比如唯一用途是生成随机密码，就可以放到 <code>none</code> 网络中避免密码被窃取。</p><h3 id="host网络"><a href="#host网络" class="headerlink" title="host网络"></a>host网络</h3><p>连接到 <code>host</code> 网络的容器共享 <code>Docker host</code> 的网络栈，容器的网络配置与 host 完全一样。可以通过 <code>--network=host</code>指定使用 <code>host</code> 网络。<br><code>Host</code> 模式并没有为容器创建一个隔离的网络环境。而之所以称之为<code>host</code>模式，是因为该模式下的 <code>Docker</code> 容器会和 <code>host</code> 宿主机共享同一个网络 <code>namespace</code>，故 <code>Docker Container</code>可以和宿主机一样，使用宿主机的<code>eth0</code>，实现和外界的通信。换言之，<code>Docker Container</code>的 <code>IP</code> 地址即为宿主机 <code>eth0</code> 的 <code>IP</code> 地址。其特点包括：</p><ul><li>这种模式下的容器没有隔离的 <code>network namespace</code></li><li>容器的 <code>IP</code> 地址同 <code>Docker host</code> 的 <code>IP</code> 地址</li><li>需要注意容器中服务的端口号不能与 <code>Docker host</code> 上已经使用的端口号相冲突</li><li><code>host</code> 模式能够和其它模式共存</li></ul><p><img src="https://ljw.howieli.cn/blog/2018-06-12/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20180612160912.png"></p><p>直接使用 <code>Docker host</code> 的网络最大的好处就是性能，如果容器对网络传输效率有较高要求，则可以选择 <code>host</code> 网络。当然不便之处就是牺牲一些灵活性，比如要考虑端口冲突问题，<code>Docker host</code> 上已经使用的端口就不能再用了.<br><code>Docker host</code> 的另一个用途是让容器可以直接配置 <code>host</code> 网路。比如某些跨 <code>host</code> 的网络解决方案，其本身也是以容器方式运行的，这些方案需要对网络进行配置，比如管理 <code>iptables</code>。</p><h3 id="bridge网络"><a href="#bridge网络" class="headerlink" title="bridge网络"></a>bridge网络</h3><p><code>Docker</code> 安装时会创建一个命名为 <code>docker0</code> 的 <code>linux bridge</code>。如果不指定<code>--network</code>，创建的容器默认都会挂到 <code>docker0</code> 上。<br><img src="https://ljw.howieli.cn/blog/2018-06-12/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20180612165740.png"><br>当前 <code>docker0</code> 上没有任何其他网络设备，我们创建一个容器看看有什么变化。<br><img src="https://ljw.howieli.cn/blog/2018-06-12/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20180612170225.png"></p><p>当创建httpd容器后，一个新的网络接口<code>veth303a751</code>被挂到了<code>docker0</code>上，<code>veth303a751</code>就是新创建的虚拟网卡。当<code>stop</code>掉<code>httpd</code>容器虚拟网卡消失。<br>下图是<code>docker bridge</code>模式网络架构图<br><img src="https://ljw.howieli.cn/blog/2018-06-12/docker0.png"></p><p><code>host</code>网络关键就是<code>NAT</code>，我们可以查看一下<code>docker host</code>上的<code>iptables</code>规则<br><img src="https://ljw.howieli.cn/blog/2018-06-12/docker3.png"><br>其含义是：如果网桥 <code>docker0</code> 收到来自 <code>172.17.0.0/16</code> 网段的外出包，把它交给 <code>MASQUERADE</code> 处理。而 <code>MASQUERADE</code> 的处理方式是将包的源地址替换成 <code>host</code> 的地址发送出去，即做了一次网络地址转换（<code>NAT</code>）。</p><h3 id="container网络"><a href="#container网络" class="headerlink" title="container网络"></a>container网络</h3><p>这个模式指定新创建的容器和已经存在的一个容器共享一个<code>Network Namespace</code>，而不是和宿主机共享。新创建的容器不会创建自己的网卡，配置自己的<code>IP</code>，而是和一个指定的容器共享IP、端口范围等。同样，两个容器除了网络方面，其他的如文件系统、进程列表等还是隔离的。两个容器的进程可以通过<code>lo</code>网卡设备通信。<br>使用<code>–-net=container:CONIAINER ID</code>模式启动容器：<br><img src="https://ljw.howieli.cn/blog/2018-06-12/docker1.png"></p><p><img src="https://ljw.howieli.cn/blog/2018-06-12/docker2.png"></p>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;docker四种单节点网络&quot;&gt;&lt;a href=&quot;#docker四种单节点网络&quot; class=&quot;headerlink&quot; title=&quot;docker四种单节点网络&quot;&gt;&lt;/a&gt;docker四种单节点网络&lt;/h3&gt;&lt;p&gt;&lt;code&gt;Docker&lt;/code&gt;在创建容器时有四种网络模式，&lt;code&gt;bridge&lt;/code&gt;为默认不需要用&lt;code&gt;-–net&lt;/code&gt;去指定，其他三种模式需要在创建容器时使用&lt;code&gt;–-net&lt;/code&gt;去指定。&lt;br&gt;四种单节点网络模式如下：&lt;br&gt;&lt;code&gt;none&lt;/code&gt;模式，使用&lt;code&gt;–-net=none&lt;/code&gt;指定；&lt;br&gt;&lt;code&gt;host&lt;/code&gt;模式，使用&lt;code&gt;–-net=host&lt;/code&gt;指定；&lt;br&gt;&lt;code&gt;bridge&lt;/code&gt;模式，使用&lt;code&gt;–-net=bridge&lt;/code&gt;指定，默认设置；&lt;br&gt;&lt;code&gt;container&lt;/code&gt;模式，使用&lt;code&gt;–-net=container&lt;/code&gt;:容器名称或ID指定。&lt;br&gt;在&lt;code&gt;Docker&lt;/code&gt;安装时会自动在&lt;code&gt;host&lt;/code&gt;上创建三个网络,我们可以通过&lt;code&gt;docker network ls&lt;/code&gt;查看：&lt;br&gt;&lt;img src=&quot;https://ljw.howieli.cn/blog/2018-06-12/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20180612122600.png&quot;&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="docker" scheme="https://www.lijiawang.org/categories/docker/"/>
    
    
    <category term="docker" scheme="https://www.lijiawang.org/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>ceph-for-openstack-storage</title>
    <link href="https://www.lijiawang.org/openstack%E5%AF%B9%E6%8E%A5ceph%E5%AD%98%E5%82%A8"/>
    <id>https://www.lijiawang.org/openstack%E5%AF%B9%E6%8E%A5ceph%E5%AD%98%E5%82%A8</id>
    <published>2018-05-03T06:08:17.000Z</published>
    <updated>2018-05-03T06:17:53.891Z</updated>
    
    <content type="html"><![CDATA[<h2 id="openstack对接ceph存储"><a href="#openstack对接ceph存储" class="headerlink" title="openstack对接ceph存储"></a><code>openstack</code>对接<code>ceph</code>存储</h2><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><ul><li>截至<code>2018</code>年<code>5</code>月<code>3</code>日，<code>OpenStack</code>也已经伴随着我们走过了<code>8</code>个年头了，并且成为了云计算领域中最火热的项目之一，逐渐成为<code>IaaS</code>的事实标准，私有云项目的部署首选；</li><li><code>OpenStack</code>发展如此的迅速，以至于部署规模愈发的庞大，此时就该思量<code>OpenStack</code>集群的部署支持以及持续可扩展性；</li><li><code>OpenStack</code>和<code>Ceph</code>的集成更让开源项目锦上添花，<code>Ceph</code>作为优秀的分布式存储系统，实现对<code>OpenStack</code>相关子项目进行集成或替代，目前在<code>OpenStack</code>中扮演者非常重要的角色；</li></ul><a id="more"></a><h2 id="基础配置"><a href="#基础配置" class="headerlink" title="基础配置"></a>基础配置</h2><ul><li><code>Ceph</code>已经成为<code>OpenStack</code>后端存储标配，<code>OpenStack</code>作为<code>IaaS</code>系统，涉及到存储的部分主要是块存储服务模块、对象存储服务模块、镜像管理模块和计算服务模块，对应为其中的<code>Cinder</code>、<code>Swift</code>、<code>Glance</code>和<code>Nova</code>四个项目；</li><li>配置前，请将<code>OpenStack</code>中已存在的<code>VM</code>、<code>Image</code>与<code>Vloume</code>清理掉；</li></ul><h3 id="创建相应的Pool"><a href="#创建相应的Pool" class="headerlink" title="创建相应的Pool"></a>创建相应的<code>Pool</code></h3><ul><li>若少于<code>5</code>个<code>OSD</code>, 设置<code>pg_num</code>为<code>128</code>；</li><li><code>5 ~ 10</code>个<code>OSD</code>，设置<code>pg_num</code>为<code>512</code>；</li><li><code>10 ~ 50</code>个<code>OSD</code>，设置<code>pg_num</code>为<code>4096</code>；</li><li>超过<code>50</code>个<code>OSD</code>, 根据(<code>PG数 = OSD数 * 100/ 副本数 / POOL数</code>)来计算；</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ ceph osd pool create images 1024</span><br><span class="line">$ ceph osd pool create volumes 1024</span><br><span class="line">$ ceph osd pool create vms 1024</span><br><span class="line">$ ceph osd pool create backups 1024</span><br><span class="line"></span><br><span class="line">$ ceph osd pool create images.cache 1024</span><br><span class="line">$ ceph osd pool create volumes.cache 1024</span><br><span class="line">$ ceph osd pool create vms.cache 1024</span><br><span class="line">$ ceph osd pool create backups.cache 1024</span><br></pre></td></tr></table></figure><h3 id="删除相应的Pool-备用"><a href="#删除相应的Pool-备用" class="headerlink" title="删除相应的Pool(备用)"></a>删除相应的<code>Pool</code>(备用)</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ ceph osd pool delete images images --yes-i-really-really-mean-it</span><br><span class="line">$ ceph osd pool delete volumes volumes --yes-i-really-really-mean-it</span><br><span class="line">$ ceph osd pool delete vms vms --yes-i-really-really-mean-it</span><br><span class="line">$ ceph osd pool delete backups backups --yes-i-really-really-mean-it</span><br><span class="line"></span><br><span class="line">$ ceph osd pool delete images.cache images.cache --yes-i-really-really-mean-it</span><br><span class="line">$ ceph osd pool delete volumes.cache volumes.cache --yes-i-really-really-mean-it</span><br><span class="line">$ ceph osd pool delete vms.cache vms.cache --yes-i-really-really-mean-it</span><br><span class="line">$ ceph osd pool delete backups.cache backups.cache --yes-i-really-really-mean-it</span><br></pre></td></tr></table></figure><h3 id="更新相应Pool的属性值-备用"><a href="#更新相应Pool的属性值-备用" class="headerlink" title="更新相应Pool的属性值(备用)"></a>更新相应<code>Pool</code>的属性值(备用)</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ ceph osd pool <span class="built_in">set</span> images pg_num 512</span><br><span class="line">$ ceph osd pool <span class="built_in">set</span> volumes pg_num 512</span><br><span class="line">$ ceph osd pool <span class="built_in">set</span> vms pg_num 512</span><br><span class="line">$ ceph osd pool <span class="built_in">set</span> backups pg_num 512</span><br><span class="line"></span><br><span class="line">$ ceph osd pool <span class="built_in">set</span> images pgp_num 512</span><br><span class="line">$ ceph osd pool <span class="built_in">set</span> volumes pgp_num 512</span><br><span class="line">$ ceph osd pool <span class="built_in">set</span> vms pgp_num 512</span><br><span class="line">$ ceph osd pool <span class="built_in">set</span> backups pgp_num 512</span><br></pre></td></tr></table></figure><h3 id="将不同的Pool加入到对应的Crush-Map中"><a href="#将不同的Pool加入到对应的Crush-Map中" class="headerlink" title="将不同的Pool加入到对应的Crush Map中"></a>将不同的<code>Pool</code>加入到对应的<code>Crush Map</code>中</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 后备存储-SATA盘</span></span><br><span class="line">$ ceph osd pool <span class="built_in">set</span> images crush_ruleset 0</span><br><span class="line">$ ceph osd pool <span class="built_in">set</span> volumes crush_ruleset 0</span><br><span class="line">$ ceph osd pool <span class="built_in">set</span> vms crush_ruleset 0</span><br><span class="line">$ ceph osd pool <span class="built_in">set</span> backups crush_ruleset 0</span><br><span class="line"></span><br><span class="line"><span class="comment"># 缓存存储-SSD盘</span></span><br><span class="line">$ ceph osd pool <span class="built_in">set</span> images.cache crush_ruleset 1</span><br><span class="line">$ ceph osd pool <span class="built_in">set</span> volumes.cache crush_ruleset 1</span><br><span class="line">$ ceph osd pool <span class="built_in">set</span> vms.cache crush_ruleset 1</span><br><span class="line">$ ceph osd pool <span class="built_in">set</span> backups.cache crush_ruleset 1</span><br></pre></td></tr></table></figure><h3 id="将缓存层与后备存储池相关联"><a href="#将缓存层与后备存储池相关联" class="headerlink" title="将缓存层与后备存储池相关联"></a>将缓存层与后备存储池相关联</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ ceph osd tier add images images.cache</span><br><span class="line">$ ceph osd tier add volumes volumes.cache</span><br><span class="line">$ ceph osd tier add vms vms.cache</span><br><span class="line">$ ceph osd tier add backups backups.cache</span><br></pre></td></tr></table></figure><h3 id="设置缓存模式-热存储回写"><a href="#设置缓存模式-热存储回写" class="headerlink" title="设置缓存模式(热存储回写)"></a>设置缓存模式(热存储回写)</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ ceph osd tier cache-mode images.cache writeback</span><br><span class="line">$ ceph osd tier cache-mode volumes.cache writeback</span><br><span class="line">$ ceph osd tier cache-mode vms.cache writeback</span><br><span class="line">$ ceph osd tier cache-mode backups.cache writeback</span><br></pre></td></tr></table></figure><h3 id="高速缓存层覆盖后备存储池"><a href="#高速缓存层覆盖后备存储池" class="headerlink" title="高速缓存层覆盖后备存储池"></a>高速缓存层覆盖后备存储池</h3><ul><li>直接将客户端流量引导到缓存池</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ ceph osd tier set-overlay images images.cache</span><br><span class="line">$ ceph osd tier set-overlay volumes volumes.cache</span><br><span class="line">$ ceph osd tier set-overlay vms vms.cache</span><br><span class="line">$ ceph osd tier set-overlay backups backups.cache</span><br></pre></td></tr></table></figure><h3 id="启用缓存存储池的命中集跟踪"><a href="#启用缓存存储池的命中集跟踪" class="headerlink" title="启用缓存存储池的命中集跟踪"></a>启用缓存存储池的命中集跟踪</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ ceph osd pool <span class="built_in">set</span> images.cache hit_set_type bloom</span><br><span class="line">$ ceph osd pool <span class="built_in">set</span> volumes.cache hit_set_type bloom</span><br><span class="line">$ ceph osd pool <span class="built_in">set</span> vms.cache hit_set_type bloom</span><br><span class="line">$ ceph osd pool <span class="built_in">set</span> backups.cache hit_set_type bloom</span><br></pre></td></tr></table></figure><h3 id="为缓存存储池的保留的命中集数量"><a href="#为缓存存储池的保留的命中集数量" class="headerlink" title="为缓存存储池的保留的命中集数量"></a>为缓存存储池的保留的命中集数量</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ ceph osd pool <span class="built_in">set</span> images.cache hit_set_count 10</span><br><span class="line">$ ceph osd pool <span class="built_in">set</span> volumes.cache hit_set_count 10</span><br><span class="line">$ ceph osd pool <span class="built_in">set</span> vms.cache hit_set_count 10</span><br><span class="line">$ ceph osd pool <span class="built_in">set</span> backups.cache hit_set_count 10</span><br></pre></td></tr></table></figure><h3 id="为缓存存储池保留的命中集有效期"><a href="#为缓存存储池保留的命中集有效期" class="headerlink" title="为缓存存储池保留的命中集有效期"></a>为缓存存储池保留的命中集有效期</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ ceph osd pool <span class="built_in">set</span> images.cache hit_set_period 14400</span><br><span class="line">$ ceph osd pool <span class="built_in">set</span> volumes.cache hit_set_period 14400</span><br><span class="line">$ ceph osd pool <span class="built_in">set</span> vms.cache hit_set_period 14400</span><br><span class="line">$ ceph osd pool <span class="built_in">set</span> backups.cache hit_set_period 14400</span><br></pre></td></tr></table></figure><h3 id="缓存层回写数据"><a href="#缓存层回写数据" class="headerlink" title="缓存层回写数据"></a>缓存层回写数据</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 最大达到50G时，回写数据</span></span><br><span class="line">$ ceph osd pool <span class="built_in">set</span> images.cache target_max_bytes 53687091200</span><br><span class="line">$ ceph osd pool <span class="built_in">set</span> volumes.cache target_max_bytes 53687091200</span><br><span class="line">$ ceph osd pool <span class="built_in">set</span> vms.cache target_max_bytes 53687091200</span><br><span class="line">$ ceph osd pool <span class="built_in">set</span> backups.cache target_max_bytes 53687091200</span><br><span class="line"></span><br><span class="line"><span class="comment"># 最大达到100万时，回写数据</span></span><br><span class="line">$ ceph osd pool <span class="built_in">set</span> images.cache target_max_objects 1000000</span><br><span class="line">$ ceph osd pool <span class="built_in">set</span> volumes.cache target_max_objects 1000000</span><br><span class="line">$ ceph osd pool <span class="built_in">set</span> vms.cache target_max_objects 1000000</span><br><span class="line">$ ceph osd pool <span class="built_in">set</span> backups.cache target_max_objects 1000000</span><br></pre></td></tr></table></figure><h3 id="保留访问记录"><a href="#保留访问记录" class="headerlink" title="保留访问记录"></a>保留访问记录</h3><ul><li>取值越高，消耗的内存就越多；</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ ceph osd pool <span class="built_in">set</span> images.cache min_read_recency_for_promote 2</span><br><span class="line">$ ceph osd pool <span class="built_in">set</span> images.cache min_write_recency_for_promote 2</span><br><span class="line">$ ceph osd pool <span class="built_in">set</span> volumes.cache min_read_recency_for_promote 2</span><br><span class="line">$ ceph osd pool <span class="built_in">set</span> volumes.cache min_write_recency_for_promote 2</span><br><span class="line">$ ceph osd pool <span class="built_in">set</span> vms.cache min_read_recency_for_promote 2</span><br><span class="line">$ ceph osd pool <span class="built_in">set</span> vms.cache min_write_recency_for_promote 2</span><br><span class="line">$ ceph osd pool <span class="built_in">set</span> backups.cache min_read_recency_for_promote 2</span><br><span class="line">$ ceph osd pool <span class="built_in">set</span> backups.cache min_write_recency_for_promote 2</span><br></pre></td></tr></table></figure><h2 id="认证配置"><a href="#认证配置" class="headerlink" title="认证配置"></a>认证配置</h2><h3 id="创建认证密钥"><a href="#创建认证密钥" class="headerlink" title="创建认证密钥"></a>创建认证密钥</h3><h4 id="Nova用户"><a href="#Nova用户" class="headerlink" title="Nova用户"></a>Nova用户</h4><ul><li>允许访问<code>volumes</code>、<code>vms</code>、<code>images</code>池；</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ceph auth get-or-create client.nova mon <span class="string">&#x27;allow r&#x27;</span> osd <span class="string">&#x27;allow class-read object_prefix rbd_children, allow rwx pool=volumes, allow rwx pool=vms, allow rwx pool=images ,allow rwx pool=volumes.cache, allow rwx pool=vms.cache, allow rwx pool=images.cache&#x27;</span></span><br></pre></td></tr></table></figure><h4 id="Cinder用户"><a href="#Cinder用户" class="headerlink" title="Cinder用户"></a>Cinder用户</h4><ul><li>允许访问<code>volumes</code>、<code>backups</code>池；</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph auth get-or-create client.cinder mon <span class="string">&#x27;allow r&#x27;</span> osd <span class="string">&#x27;allow class-read object_prefix rbd_children, allow rwx pool=volumes, allow rwx pool=volumes.cache ,allow rwx pool=backups ,allow rwx pool=backups.cache&#x27;</span></span><br></pre></td></tr></table></figure><h4 id="Glance用户"><a href="#Glance用户" class="headerlink" title="Glance用户"></a>Glance用户</h4><ul><li>允许访问<code>images</code>池；</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ceph auth get-or-create client.glance mon <span class="string">&#x27;allow r&#x27;</span> osd <span class="string">&#x27;allow class-read object_prefix rbd_children, allow rwx pool=images, allow rwx pool=images.cache&#x27;</span></span><br></pre></td></tr></table></figure><h3 id="获取认证密钥"><a href="#获取认证密钥" class="headerlink" title="获取认证密钥"></a>获取认证密钥</h3><h4 id="Controller节点"><a href="#Controller节点" class="headerlink" title="Controller节点"></a><code>Controller</code>节点</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ ceph auth get-or-create client.nova | ssh root@controller tee /etc/ceph/ceph.client.nova.keyring</span><br><span class="line">$ ssh root@controller chown nova:nova /etc/ceph/ceph.client.nova.keyring</span><br><span class="line"></span><br><span class="line">$ ceph auth get-or-create client.cinder | ssh root@controller tee /etc/ceph/ceph.client.cinder.keyring</span><br><span class="line">$ ssh root@controller chown cinder:cinder /etc/ceph/ceph.client.cinder.keyring</span><br><span class="line"></span><br><span class="line">$ ceph auth get-or-create client.glance | ssh root@controller tee /etc/ceph/ceph.client.glance.keyring</span><br><span class="line">$ ssh root@controller chown glance:glance /etc/ceph/ceph.client.glance.keyring</span><br></pre></td></tr></table></figure><h4 id="Compute节点"><a href="#Compute节点" class="headerlink" title="Compute节点"></a><code>Compute</code>节点</h4><ul><li>请使用<code>Compute</code>节点的主机名代替<code>&#123;ComputeNode&#125;</code>；</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ ceph auth get-or-create client.nova | ssh root@&#123;ComputeNode&#125; tee /etc/ceph/ceph.client.nova.keyring</span><br><span class="line">$ ssh root@&#123;ComputeNode&#125; chown nova:nova /etc/ceph/ceph.client.nova.keyring</span><br></pre></td></tr></table></figure><h2 id="OpenStack配置"><a href="#OpenStack配置" class="headerlink" title="OpenStack配置"></a><code>OpenStack</code>配置</h2><h3 id="配置Nova服务"><a href="#配置Nova服务" class="headerlink" title="配置Nova服务"></a>配置<code>Nova</code>服务</h3><h4 id="配置libvirt认证"><a href="#配置libvirt认证" class="headerlink" title="配置libvirt认证"></a>配置<code>libvirt</code>认证</h4><h5 id="获取密钥"><a href="#获取密钥" class="headerlink" title="获取密钥"></a>获取密钥</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ceph auth get-key client.nova | tee ~/nova.key</span><br></pre></td></tr></table></figure><h5 id="生成UUID，用于认证"><a href="#生成UUID，用于认证" class="headerlink" title="生成UUID，用于认证"></a>生成<code>UUID</code>，用于认证</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ UUID=$(cat /etc/ceph/ceph.conf | grep <span class="string">&quot;fsid&quot;</span> | awk -F <span class="string">&quot; = &quot;</span> <span class="string">&#x27;&#123;print $2&#125;&#x27;</span>)</span><br></pre></td></tr></table></figure><h5 id="生成secret-xml文件"><a href="#生成secret-xml文件" class="headerlink" title="生成secret.xml文件"></a>生成<code>secret.xml</code>文件</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ cat &gt; ~/secret.xml &lt;&lt;<span class="string">EOF</span></span><br><span class="line"><span class="string">&lt;secret ephemeral=&#x27;no&#x27; private=&#x27;no&#x27;&gt;</span></span><br><span class="line"><span class="string">  &lt;uuid&gt;$&#123;UUID&#125;&lt;/uuid&gt;</span></span><br><span class="line"><span class="string">  &lt;usage type=&#x27;ceph&#x27;&gt;</span></span><br><span class="line"><span class="string">        &lt;name&gt;client.nova secret&lt;/name&gt;</span></span><br><span class="line"><span class="string">  &lt;/usage&gt;</span></span><br><span class="line"><span class="string">&lt;/secret&gt;</span></span><br><span class="line"><span class="string">EOF</span></span><br></pre></td></tr></table></figure><h5 id="定义secret-xml文件"><a href="#定义secret-xml文件" class="headerlink" title="定义secret.xml文件"></a>定义<code>secret.xml</code>文件</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ virsh secret-define --file ~/secret.xml</span><br></pre></td></tr></table></figure><h5 id="列出已定义的项目"><a href="#列出已定义的项目" class="headerlink" title="列出已定义的项目"></a>列出已定义的项目</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ virsh secret-list</span><br></pre></td></tr></table></figure><h5 id="取消定义-备用"><a href="#取消定义-备用" class="headerlink" title="取消定义(备用)"></a>取消定义(备用)</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ virsh secret-undefine <span class="variable">$&#123;UUID&#125;</span></span><br></pre></td></tr></table></figure><h5 id="关联密钥与UUID"><a href="#关联密钥与UUID" class="headerlink" title="关联密钥与UUID"></a>关联密钥与<code>UUID</code></h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ virsh secret-set-value --secret <span class="variable">$&#123;UUID&#125;</span> --base64 $(cat ~/nova.key) &amp;&amp; rm -f ~/&#123;nova.key,secret.xml&#125;</span><br></pre></td></tr></table></figure><h4 id="配置Nova服务-1"><a href="#配置Nova服务-1" class="headerlink" title="配置Nova服务"></a>配置<code>Nova</code>服务</h4><h5 id="编辑配置文件"><a href="#编辑配置文件" class="headerlink" title="编辑配置文件"></a>编辑配置文件</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ vim /etc/nova/nova.conf</span><br></pre></td></tr></table></figure><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[libvirt]</span><br><span class="line">images_type = rbd</span><br><span class="line">images_rbd_pool = vms</span><br><span class="line">images_rbd_ceph_conf = /etc/ceph/ceph.conf</span><br><span class="line">rbd_user = nova</span><br><span class="line">rbd_secret_uuid = UUID</span><br><span class="line">disk_cachemodes = &quot;network=writeback&quot;</span><br><span class="line">hw_disk_discard = unmap</span><br><span class="line">inject_password = true</span><br><span class="line">inject_key = true</span><br><span class="line">inject_partition = -2</span><br><span class="line">live_migration_flag = &quot;VIR_MIGRATE_UNDEFINE_SOURCE,VIR_MIGRATE_PEER2PEER,VIR_MIGRATE_LIVE,VIR_MIGRATE_PERSIST_DEST,VIR_MIGRATE_TUNNELLED&quot;</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sed -i <span class="string">&quot;s#UUID#<span class="variable">$&#123;UUID&#125;</span>#g&quot;</span> /etc/nova/nova.conf</span><br></pre></td></tr></table></figure><h5 id="重启服务"><a href="#重启服务" class="headerlink" title="重启服务"></a>重启服务</h5><ul><li><code>Ubuntu</code>系统：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ service nova-compute restart</span><br></pre></td></tr></table></figure><ul><li><code>CentOS</code>系统：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ systemctl restart libvirtd.service openstack-nova-compute.service</span><br></pre></td></tr></table></figure><h3 id="配置Glance服务"><a href="#配置Glance服务" class="headerlink" title="配置Glance服务"></a>配置<code>Glance</code>服务</h3><h4 id="编辑配置文件-1"><a href="#编辑配置文件-1" class="headerlink" title="编辑配置文件"></a>编辑配置文件</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ vim /etc/glance/glance-api.conf</span><br></pre></td></tr></table></figure><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">[DEFAULT]</span><br><span class="line">show_image_direct_url = True</span><br><span class="line"></span><br><span class="line">[glance_store]</span><br><span class="line">stores = rbd</span><br><span class="line">rbd_store_pool = images</span><br><span class="line">rbd_store_user = glance</span><br><span class="line">rbd_store_ceph_conf = /etc/ceph/ceph.conf</span><br><span class="line">rbd_store_chunk_size = 8</span><br><span class="line">default_store =rbd</span><br><span class="line"></span><br><span class="line">[task]</span><br><span class="line">task_executor = taskflow</span><br><span class="line">work_dir=/tmp</span><br><span class="line"></span><br><span class="line">[taskflow_executor]</span><br><span class="line">engine_mode = serial</span><br><span class="line">max_workers = 10</span><br><span class="line">conversion_format=raw</span><br></pre></td></tr></table></figure><h4 id="重启服务-1"><a href="#重启服务-1" class="headerlink" title="重启服务"></a>重启服务</h4><ul><li><code>Ubuntu</code>系统：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ service glance-api restart</span><br><span class="line">$ service glance-registry restart</span><br></pre></td></tr></table></figure><ul><li><code>CentOS</code>系统：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ systemctl restart openstack-glance-api.service openstack-glance-registry.service</span><br></pre></td></tr></table></figure><h3 id="配置Cinder服务"><a href="#配置Cinder服务" class="headerlink" title="配置Cinder服务"></a>配置<code>Cinder</code>服务</h3><ul><li>仅需在<code>Controller</code>节点安装<code>cinder</code>的所有的服务即可；</li></ul><h4 id="编辑配置文件-2"><a href="#编辑配置文件-2" class="headerlink" title="编辑配置文件"></a>编辑配置文件</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ vim /etc/cinder/cinder.conf</span><br></pre></td></tr></table></figure><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">[DEFAULT]</span><br><span class="line">enabled_backends = ceph</span><br><span class="line">backup_driver = cinder.backup.drivers.ceph</span><br><span class="line">backup_ceph_conf = /etc/ceph/ceph.conf</span><br><span class="line">backup_ceph_user = cinder-backup</span><br><span class="line">backup_ceph_chunk_size = 134217728</span><br><span class="line">backup_ceph_pool = backups</span><br><span class="line">backup_ceph_stripe_unit = 0</span><br><span class="line">backup_ceph_stripe_count = 0</span><br><span class="line">restore_discard_excess_bytes = true</span><br><span class="line"></span><br><span class="line">[ceph]</span><br><span class="line">volume_driver = cinder.volume.drivers.rbd.RBDDriver</span><br><span class="line">rbd_pool = volumes</span><br><span class="line">rbd_ceph_conf = /etc/ceph/ceph.conf</span><br><span class="line">rbd_flatten_volume_from_snapshot = false</span><br><span class="line">rbd_max_clone_depth = 5</span><br><span class="line">rbd_store_chunk_size = 4</span><br><span class="line">rados_connect_timeout = -1</span><br><span class="line">glance_api_version = 2</span><br><span class="line">rbd_user = cinder</span><br><span class="line">rbd_secret_uuid = UUID</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sed -i <span class="string">&quot;s#UUID#<span class="variable">$&#123;UUID&#125;</span>#g&quot;</span> /etc/cinder/cinder.conf</span><br></pre></td></tr></table></figure><h4 id="重启服务-2"><a href="#重启服务-2" class="headerlink" title="重启服务"></a>重启服务</h4><ul><li><code>Ubuntu</code>系统：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ service cinder-api restart</span><br><span class="line">$ service cinder-scheduler restart</span><br><span class="line">$ service cinder-volume restart</span><br><span class="line">$ service cinder-backup restart</span><br><span class="line">$ service tgt restart</span><br></pre></td></tr></table></figure><ul><li><code>CentOS</code>系统：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ systemctl restart openstack-cinder-volume.service target.service</span><br></pre></td></tr></table></figure><h2 id="测试操作"><a href="#测试操作" class="headerlink" title="测试操作"></a>测试操作</h2><h3 id="下载镜像"><a href="#下载镜像" class="headerlink" title="下载镜像"></a>下载镜像</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ wget http://download.cirros-cloud.net/0.3.5/cirros-0.3.5-x86_64-disk.img</span><br></pre></td></tr></table></figure><h3 id="上传镜像"><a href="#上传镜像" class="headerlink" title="上传镜像"></a>上传镜像</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ openstack image create <span class="string">&quot;cirros&quot;</span> --file cirros-0.3.5-x86_64-disk.img --disk-format qcow2 --container-format bare --public</span><br></pre></td></tr></table></figure><h3 id="获取镜像"><a href="#获取镜像" class="headerlink" title="获取镜像"></a>获取镜像</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ openstack image list</span><br></pre></td></tr></table></figure><h3 id="获取Ceph的存储使用率"><a href="#获取Ceph的存储使用率" class="headerlink" title="获取Ceph的存储使用率"></a>获取<code>Ceph</code>的存储使用率</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ceph df</span><br></pre></td></tr></table></figure><hr>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;openstack对接ceph存储&quot;&gt;&lt;a href=&quot;#openstack对接ceph存储&quot; class=&quot;headerlink&quot; title=&quot;openstack对接ceph存储&quot;&gt;&lt;/a&gt;&lt;code&gt;openstack&lt;/code&gt;对接&lt;code&gt;ceph&lt;/code&gt;存储&lt;/h2&gt;&lt;h2 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;截至&lt;code&gt;2018&lt;/code&gt;年&lt;code&gt;5&lt;/code&gt;月&lt;code&gt;3&lt;/code&gt;日，&lt;code&gt;OpenStack&lt;/code&gt;也已经伴随着我们走过了&lt;code&gt;8&lt;/code&gt;个年头了，并且成为了云计算领域中最火热的项目之一，逐渐成为&lt;code&gt;IaaS&lt;/code&gt;的事实标准，私有云项目的部署首选；&lt;/li&gt;
&lt;li&gt;&lt;code&gt;OpenStack&lt;/code&gt;发展如此的迅速，以至于部署规模愈发的庞大，此时就该思量&lt;code&gt;OpenStack&lt;/code&gt;集群的部署支持以及持续可扩展性；&lt;/li&gt;
&lt;li&gt;&lt;code&gt;OpenStack&lt;/code&gt;和&lt;code&gt;Ceph&lt;/code&gt;的集成更让开源项目锦上添花，&lt;code&gt;Ceph&lt;/code&gt;作为优秀的分布式存储系统，实现对&lt;code&gt;OpenStack&lt;/code&gt;相关子项目进行集成或替代，目前在&lt;code&gt;OpenStack&lt;/code&gt;中扮演者非常重要的角色；&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    <category term="openstack" scheme="https://www.lijiawang.org/categories/openstack/"/>
    
    
    <category term="openstack ceph" scheme="https://www.lijiawang.org/tags/openstack-ceph/"/>
    
  </entry>
  
  <entry>
    <title>kolla 多region搭建</title>
    <link href="https://www.lijiawang.org/posts/kolla-%E5%A4%9Aregion%E6%90%AD%E5%BB%BA.html"/>
    <id>https://www.lijiawang.org/posts/kolla-%E5%A4%9Aregion%E6%90%AD%E5%BB%BA.html</id>
    <published>2018-04-10T01:44:43.000Z</published>
    <updated>2018-04-10T02:30:27.120Z</updated>
    
    <content type="html"><![CDATA[<h3 id="kolla部署openstack多region"><a href="#kolla部署openstack多region" class="headerlink" title="kolla部署openstack多region"></a>kolla部署openstack多region</h3><h3 id="openstack多region基础知识"><a href="#openstack多region基础知识" class="headerlink" title="openstack多region基础知识"></a>openstack多region基础知识</h3><p>我们都知道<code>Region</code>是<code>OpenStack</code>里面用于隔离资源的一个重要概念。简单来说，一个<code>Region</code>对应一套完整的<code>OpenStack</code>环境，而<code>Region</code>和<code>Region</code>之间可以是跨机房的集群，也可以是一个大规模物理机集群分割后的集群。<code>OpenStack</code>在设计之初就是支持多<code>Region</code>的情况，由于<code>Region</code>之间资源（<code>Mariadb</code>,<code>RabbitMQ</code>等）的独立的，所以他们之间并不存在资源交互开销的情况。<br>简单的讲所谓<code>openstack</code>多<code>region</code>，就是多套<code>openstack</code>共享一个<code>keystone</code>和<code>horizon</code>。每个区域一套<code>openstack</code>环境，可以分布在不同的地理位置，只要网络可达就行。个人认为目的就是为了提供环境隔离的功能，选择启虚拟机的时候可以根据自己所处的位置就近选择。<br><img src="https://ljw.howieli.cn/blog/2018-4-3/%E5%A4%9Aregion1.png"></p><a id="more"></a><h3 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h3><p>我这里有两台<code>kolla</code>环境的部署节点分别是<code>172.18.24.150</code>，<code>172.18.24.130</code>两个台虚拟机。<br>| column | column |<br>|——–|————|<br>|RegionOne|172.18.24.150|<br>|RegionTWO|172.18.24.130|<br><code>kolla</code>环境部署节点安装请参考<a href="https://www.lijiawang.org/posts/kolla%20queens%20on%20centos7.41.html">kolla搭建</a>，这次我用<code>kolla</code>部署<code>openstack</code>的<code>queens</code>版本。</p><h3 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h3><h4 id="用Keystone和Horizon部署第一个区域"><a href="#用Keystone和Horizon部署第一个区域" class="headerlink" title="用Keystone和Horizon部署第一个区域"></a>用<code>Keystone</code>和<code>Horizon</code>部署第一个区域</h4><p>我这这里把<code>keystone</code>,<code>horizon</code>这两个服务放到<code>RegionOne</code>虚拟机上。修改配置<code>/etc/kolla/globals.yml</code>文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> egrep <span class="string">&quot;^[^#]&quot;</span> /etc/kolla/globals.yml</span></span><br><span class="line">---</span><br><span class="line">kolla_base_distro: &quot;centos&quot;</span><br><span class="line">kolla_install_type: &quot;source&quot;</span><br><span class="line">openstack_release: &quot;queens&quot;</span><br><span class="line">kolla_internal_vip_address: &quot;172.18.24.150&quot;</span><br><span class="line">docker_namespace: &quot;kolla&quot;</span><br><span class="line">network_interface: &quot;eth0&quot;</span><br><span class="line">neutron_external_interface: &quot;eth1&quot;</span><br><span class="line">enable_haproxy: &quot;no&quot;</span><br><span class="line">enable_grafana: &quot;yes&quot;</span><br><span class="line">enable_horizon_ironic: &quot;&#123;&#123; enable_ironic | bool &#125;&#125;&quot;</span><br><span class="line">ironic_dnsmasq_dhcp_range:</span><br><span class="line">tempest_image_id:</span><br><span class="line">tempest_flavor_ref_id:</span><br><span class="line">tempest_public_network_id:</span><br><span class="line">tempest_floating_network_name:</span><br><span class="line">openstack_region_name: &quot;RegionOne&quot;</span><br><span class="line">multiple_regions_names:</span><br><span class="line">    - &quot;&#123;&#123; openstack_region_name &#125;&#125;&quot;</span><br><span class="line">    - &quot;RegionTwo&quot;</span><br></pre></td></tr></table></figure><p>部署<code>RegionOne</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> kolla-ansible deploy</span></span><br></pre></td></tr></table></figure><h4 id="部署其他区域"><a href="#部署其他区域" class="headerlink" title="部署其他区域"></a>部署其他区域</h4><p>修改<code>RegionTWO</code>的<code>/etc/kolla/globals.yml</code>文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> egrep <span class="string">&quot;^[^#]&quot;</span> /etc/kolla/globals.yml</span></span><br><span class="line">---</span><br><span class="line">kolla_base_distro: &quot;centos&quot;</span><br><span class="line">kolla_install_type: &quot;source&quot;</span><br><span class="line">openstack_release: &quot;queens&quot;</span><br><span class="line">kolla_internal_vip_address: &quot;172.18.24.130&quot;</span><br><span class="line">docker_namespace: &quot;kolla&quot;</span><br><span class="line">network_interface: &quot;eth0&quot;</span><br><span class="line">neutron_external_interface: &quot;eth1&quot;</span><br><span class="line">nova_console: &quot;novnc&quot;</span><br><span class="line">enable_keystone: &quot;no&quot;</span><br><span class="line">enable_haproxy: &quot;no&quot;</span><br><span class="line">enable_horizon: &quot;no&quot;</span><br><span class="line">nova_compute_virt_type: &quot;qemu&quot;</span><br><span class="line">tempest_image_id:</span><br><span class="line">tempest_flavor_ref_id:</span><br><span class="line">tempest_public_network_id:</span><br><span class="line">tempest_floating_network_name:</span><br><span class="line"><span class="meta">#</span><span class="bash"> kolla_internal_fqdn_r1是指RegionOne中kolla_internal_fqdn的值</span></span><br><span class="line">kolla_internal_fqdn_r1: 172.18.24.150</span><br><span class="line">keystone_admin_url: &quot;&#123;&#123; admin_protocol &#125;&#125;://&#123;&#123; kolla_internal_fqdn_r1 &#125;&#125;:&#123;&#123; keystone_admin_port &#125;&#125;&quot;</span><br><span class="line">keystone_internal_url: &quot;&#123;&#123; internal_protocol &#125;&#125;://&#123;&#123; kolla_internal_fqdn_r1 &#125;&#125;:&#123;&#123; keystone_public_port &#125;&#125;&quot;</span><br><span class="line">openstack_auth:</span><br><span class="line">    auth_url: &quot;&#123;&#123; admin_protocol &#125;&#125;://&#123;&#123; kolla_internal_fqdn_r1 &#125;&#125;:&#123;&#123; keystone_admin_port &#125;&#125;&quot;</span><br><span class="line">    username: &quot;admin&quot;</span><br><span class="line">    password: &quot;&#123;&#123; keystone_admin_password &#125;&#125;&quot;</span><br><span class="line">    project_name: &quot;admin&quot;</span><br><span class="line">    domain_name: &quot;default&quot;</span><br><span class="line">openstack_region_name: &quot;RegionTwo&quot;</span><br></pre></td></tr></table></figure><h4 id="RegionTWO上的openstack组件连接RegionOne上的keystone"><a href="#RegionTWO上的openstack组件连接RegionOne上的keystone" class="headerlink" title="RegionTWO上的openstack组件连接RegionOne上的keystone"></a>RegionTWO上的openstack组件连接RegionOne上的keystone</h4><p>需要修改<code>cinder</code>，<code>nova</code>，<code>neutron</code>，<code>glance</code>等配置文件才能联系<code>RegionOne</code>的<code>Keystone</code>。在<code>RegionTWO</code>虚拟机上创建<code>/etc/kolla/config/</code>目录,然后穿件需要连接<code>Keystone</code>服务的配置文件。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> mkdir -p /etc/kolla/config/</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">cd</span> /etc/kolla/config/</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> tree</span></span><br><span class="line">.</span><br><span class="line">├── glance.conf</span><br><span class="line">├── heat.conf</span><br><span class="line">├── neutron.conf</span><br><span class="line">└── nova.conf</span><br></pre></td></tr></table></figure><p>更新配置文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> cat glance.conf</span></span><br><span class="line">[keystone_authtoken]</span><br><span class="line">auth_uri = &#123;&#123; keystone_internal_url &#125;&#125;</span><br><span class="line">auth_url = &#123;&#123; keystone_admin_url &#125;&#125;</span><br><span class="line"><span class="meta">#</span><span class="bash"> cat nova.conf</span></span><br><span class="line">[keystone_authtoken]</span><br><span class="line">auth_uri = &#123;&#123; keystone_internal_url &#125;&#125;</span><br><span class="line">auth_url = &#123;&#123; keystone_admin_url &#125;&#125;</span><br><span class="line">[placement]</span><br><span class="line">auth_url = &#123;&#123; keystone_admin_url &#125;&#125;</span><br><span class="line"><span class="meta">#</span><span class="bash"> cat neutron.conf</span></span><br><span class="line">[keystone_authtoken]</span><br><span class="line">auth_uri = &#123;&#123; keystone_internal_url &#125;&#125;</span><br><span class="line">auth_url = &#123;&#123; keystone_admin_url &#125;&#125;</span><br><span class="line"><span class="meta">#</span><span class="bash"> cat heat.conf</span></span><br><span class="line">[keystone_authtoken]</span><br><span class="line">auth_uri = &#123;&#123; keystone_internal_url &#125;&#125;</span><br><span class="line">auth_url = &#123;&#123; keystone_admin_url &#125;&#125;</span><br><span class="line">[trustee]</span><br><span class="line">auth_uri = &#123;&#123; keystone_internal_url &#125;&#125;</span><br><span class="line">auth_url = &#123;&#123; keystone_internal_url &#125;&#125;</span><br><span class="line"></span><br><span class="line">[ec2authtoken]</span><br><span class="line">auth_uri = &#123;&#123; keystone_internal_url &#125;&#125;</span><br><span class="line"></span><br><span class="line">[clients_keystone]</span><br><span class="line">auth_uri = &#123;&#123; keystone_internal_url &#125;&#125;</span><br></pre></td></tr></table></figure><p>部署<code>RegionTWO</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> kolla-ansible deploy</span></span><br></pre></td></tr></table></figure><h3 id="Dashboard"><a href="#Dashboard" class="headerlink" title="Dashboard"></a>Dashboard</h3><p>如果以上过程顺利，恭喜你，你的<code>OpenStack</code>现在也支持多区域了。<br><img src="https://ljw.howieli.cn/blog/2018-4-3/%E5%A4%9Aregion.png"><br><img src="https://ljw.howieli.cn/blog/2018-4-3/%E5%A4%9Aregion2.png"></p><h3 id="Ocata版本多region搭建"><a href="#Ocata版本多region搭建" class="headerlink" title="Ocata版本多region搭建"></a>Ocata版本多region搭建</h3><p>如果您用的是<code>openstack</code>的<code>Ocata</code>版本，需要打一下<code>patch</code>，在部署，<a href="https://review.openstack.org/#/q/project:openstack/kolla-ansible+region">这里可以看到所有多region相关bug</a>，除了下面这个没放进去，都<code>merge</code>到<code>ocata</code>。需要把这个<code>patch</code>自己打进去即可。<br><a href="https://review.openstack.org/#/c/431658/">https://review.openstack.org/#/c/431658/</a></p>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;kolla部署openstack多region&quot;&gt;&lt;a href=&quot;#kolla部署openstack多region&quot; class=&quot;headerlink&quot; title=&quot;kolla部署openstack多region&quot;&gt;&lt;/a&gt;kolla部署openstack多region&lt;/h3&gt;&lt;h3 id=&quot;openstack多region基础知识&quot;&gt;&lt;a href=&quot;#openstack多region基础知识&quot; class=&quot;headerlink&quot; title=&quot;openstack多region基础知识&quot;&gt;&lt;/a&gt;openstack多region基础知识&lt;/h3&gt;&lt;p&gt;我们都知道&lt;code&gt;Region&lt;/code&gt;是&lt;code&gt;OpenStack&lt;/code&gt;里面用于隔离资源的一个重要概念。简单来说，一个&lt;code&gt;Region&lt;/code&gt;对应一套完整的&lt;code&gt;OpenStack&lt;/code&gt;环境，而&lt;code&gt;Region&lt;/code&gt;和&lt;code&gt;Region&lt;/code&gt;之间可以是跨机房的集群，也可以是一个大规模物理机集群分割后的集群。&lt;code&gt;OpenStack&lt;/code&gt;在设计之初就是支持多&lt;code&gt;Region&lt;/code&gt;的情况，由于&lt;code&gt;Region&lt;/code&gt;之间资源（&lt;code&gt;Mariadb&lt;/code&gt;,&lt;code&gt;RabbitMQ&lt;/code&gt;等）的独立的，所以他们之间并不存在资源交互开销的情况。&lt;br&gt;简单的讲所谓&lt;code&gt;openstack&lt;/code&gt;多&lt;code&gt;region&lt;/code&gt;，就是多套&lt;code&gt;openstack&lt;/code&gt;共享一个&lt;code&gt;keystone&lt;/code&gt;和&lt;code&gt;horizon&lt;/code&gt;。每个区域一套&lt;code&gt;openstack&lt;/code&gt;环境，可以分布在不同的地理位置，只要网络可达就行。个人认为目的就是为了提供环境隔离的功能，选择启虚拟机的时候可以根据自己所处的位置就近选择。&lt;br&gt;&lt;img src=&quot;https://ljw.howieli.cn/blog/2018-4-3/%E5%A4%9Aregion1.png&quot;&gt;&lt;/p&gt;</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>kolla queens on centos7.4</title>
    <link href="https://www.lijiawang.org/kolla%20queens%20on%20centos7.41"/>
    <id>https://www.lijiawang.org/kolla%20queens%20on%20centos7.41</id>
    <published>2018-03-29T11:37:20.000Z</published>
    <updated>2018-03-29T13:09:35.975Z</updated>
    
    <content type="html"><![CDATA[<h3 id="kolla-queens-on-centos7-4"><a href="#kolla-queens-on-centos7-4" class="headerlink" title="kolla queens on centos7.4"></a>kolla queens on centos7.4</h3><p><img src="https://ljw.howieli.cn/blog/2018-03-29/WX20180329-171345.png"></p><a id="more"></a><h3 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h3><p>我这里用<code>workstation</code> 创建了一个虚拟机，安装<code>centos7.4</code>系统，这台虚拟机上有两张网卡，一张做<code>openstack</code>管理网，一张做为虚拟机的业务网卡。</p><h3 id="确定环境信息"><a href="#确定环境信息" class="headerlink" title="确定环境信息"></a>确定环境信息</h3><ul><li><p>确认系统版本信息</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> cat /etc/redhat-release</span></span><br><span class="line">CentOS Linux release 7.4.1708 (Core)</span><br><span class="line"><span class="meta">#</span><span class="bash"> uname -r</span></span><br><span class="line">3.10.0-693.5.2.el7.x86_64</span><br></pre></td></tr></table></figure></li><li><p> 确认网卡个数和状态</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> ip a</span></span><br><span class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1</span><br><span class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">    inet 127.0.0.1/8 scope host lo</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 ::1/128 scope host</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">2: ens33: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000</span><br><span class="line">    link/ether 00:0c:29:a7:8c:91 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 192.168.254.15/24 brd 192.168.254.255 scope global dynamic ens33</span><br><span class="line">       valid_lft 1555sec preferred_lft 1555sec</span><br><span class="line">    inet6 fe80::5057:38c5:6b65:b5/64 scope link</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">3: ens34: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000</span><br><span class="line">    link/ether 00:0c:29:a7:8c:9b brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet6 fe80::20c:29ff:fea7:8c9b/64 scope link</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br></pre></td></tr></table></figure><p>上面可以看出有两张网卡<code>ens33</code>和<code>ens34</code>，这里我用<code>ens33</code>做管理网，<code>ens34</code>做业务网，这里不需要配置<code>ip</code>，把<code>ens34</code>网卡<code>up</code>起来就好。</p></li><li><p>查看主机名</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> hostname</span></span><br><span class="line">queens</span><br></pre></td></tr></table></figure><h3 id="环境初始化"><a href="#环境初始化" class="headerlink" title="环境初始化"></a>环境初始化</h3></li><li><p>关闭<code>NetworkManager</code>,<code>firewalld</code>,<code>selinux</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> systemctl stop NetworkManager</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> systemctl <span class="built_in">disable</span> NetworkManager</span></span><br><span class="line">Removed symlink /etc/systemd/system/multi-user.target.wants/NetworkManager.service.</span><br><span class="line">Removed symlink /etc/systemd/system/dbus-org.freedesktop.NetworkManager.service.</span><br><span class="line">Removed symlink /etc/systemd/system/dbus-org.freedesktop.nm-dispatcher.service.</span><br><span class="line"><span class="meta">#</span><span class="bash"> systemctl stop firewalld</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> systemctl <span class="built_in">disable</span> firewalld</span></span><br><span class="line">Removed symlink /etc/systemd/system/multi-user.target.wants/firewalld.service.</span><br><span class="line">Removed symlink /etc/systemd/system/dbus-org.fedoraproject.FirewallD1.service.</span><br><span class="line"><span class="meta">#</span><span class="bash"> sed -i <span class="string">&quot;s/SELINUX=enforcing/SELINUX=disabled/&quot;</span> /etc/selinux/config</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> setenforce 0</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> getenforce</span></span><br><span class="line">Permissive</span><br></pre></td></tr></table></figure></li><li><p>查看是否开启了虚拟化</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> egrep <span class="string">&quot;vmx|svm&quot;</span> /proc/cpuinfo</span></span><br></pre></td></tr></table></figure><h3 id="安装基础软件包"><a href="#安装基础软件包" class="headerlink" title="安装基础软件包"></a>安装基础软件包</h3><p>配置配置<code>epel</code>源安装基础包</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> yum install epel-release</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> yum install axel vim git curl wget lrzsz gcc  python-devel python-pip</span></span><br></pre></td></tr></table></figure><h3 id="安装配置docker"><a href="#安装配置docker" class="headerlink" title="安装配置docker"></a>安装配置<code>docker</code></h3></li><li><p>安装<code>docker</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> wget -O /etc/yum.repos.d/docker-ce.repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> yum install -y docker-ce</span></span><br></pre></td></tr></table></figure></li><li><p>配置<code>docker</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> mkdir /etc/systemd/system/docker.service.d</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> tee /etc/systemd/system/docker.service.d/kolla.conf &lt;&lt; <span class="string">&#x27;EOF&#x27;</span></span></span><br><span class="line">[Service]</span><br><span class="line">MountFlags=shared</span><br><span class="line">EOF</span><br><span class="line"><span class="meta">#</span><span class="bash"> vim /usr/lib/systemd/system/docker.service</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> ExecStart=/usr/bin/dockerd</span></span><br><span class="line">ExecStart=/usr/bin/dockerd --registry-mirror=http://f2d6cb40.m.daocloud.io --storage-driver=overlay2</span><br></pre></td></tr></table></figure><p>这里<code>docker</code>的文件系统我用<code>overlay2</code></p></li><li><p>启动<code>docker</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> systemctl daemon-reload</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> systemctl restart docker</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> systemctl <span class="built_in">enable</span> docker</span></span><br><span class="line">Created symlink from /etc/systemd/system/multi-user.target.wants/docker.service to /usr/lib/systemd/system/docker.service.</span><br><span class="line"><span class="meta">#</span><span class="bash"> systemctl status docker</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> docker info</span></span><br></pre></td></tr></table></figure><h3 id="安装ansible"><a href="#安装ansible" class="headerlink" title="安装ansible"></a>安装<code>ansible</code></h3><p><code>ansible</code>版本必须在2.0以上</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> yum -y install ansible -y</span></span><br></pre></td></tr></table></figure><h3 id="下载kolla-ansible，并安装配置"><a href="#下载kolla-ansible，并安装配置" class="headerlink" title="下载kolla-ansible，并安装配置"></a>下载<code>kolla-ansible</code>，并安装配置</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> git <span class="built_in">clone</span> https://github.com/openstack/kolla-ansible -b stable/queens</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">cd</span> kolla-ansible/</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> cp -r etc/kolla/ /etc/kolla/</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> pip install . -i https://pypi.tuna.tsinghua.edu.cn/simple</span></span><br></pre></td></tr></table></figure><h3 id="配置globals-yml文件"><a href="#配置globals-yml文件" class="headerlink" title="配置globals.yml文件"></a>配置<code>globals.yml</code>文件</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># egrep &quot;^[^#]&quot; &#x2F;etc&#x2F;kolla&#x2F;globals.yml</span><br><span class="line">---</span><br><span class="line">kolla_base_distro: &quot;centos&quot;</span><br><span class="line">kolla_install_type: &quot;source&quot;</span><br><span class="line">openstack_release: &quot;queens&quot;</span><br><span class="line">kolla_internal_vip_address: &quot;192.168.254.11&quot;</span><br><span class="line">docker_namespace: &quot;kolla&quot;</span><br><span class="line">network_interface: &quot;ens33&quot;</span><br><span class="line">neutron_external_interface: &quot;ens34&quot;</span><br><span class="line">enable_haproxy: &quot;no&quot;</span><br><span class="line">nova_compute_virt_type: &quot;qemu&quot;</span><br><span class="line">ironic_dnsmasq_dhcp_range:</span><br><span class="line">tempest_image_id:</span><br><span class="line">tempest_flavor_ref_id:</span><br><span class="line">tempest_public_network_id:</span><br><span class="line">tempest_floating_network_name:</span><br></pre></td></tr></table></figure><p>说明：这里我直接在<a href="https://hub.docker.com/search/?isAutomated=0&isOfficial=0&page=1&pullCount=0&q=kolla&starCount=0">docker hub</a>上拉镜像。如果是在虚拟机里安装 <code>Kolla</code>，希望可以在 <code>OpenStack</code> 平台上创建虚拟机，那么你需要在 <code>globals.yml</code> 文件中把 <code>nova_compute_virt_type</code> 配置项设置为 <code>qemu</code>，默认是 <code>KVM</code>。</p></li></ul><h3 id="安装kolla"><a href="#安装kolla" class="headerlink" title="安装kolla"></a>安装<code>kolla</code></h3><ul><li><p>生成密码文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># kolla-genpwd</span><br></pre></td></tr></table></figure></li><li><p>编辑 <code>/etc/kolla/passwords.yml</code> 文件，配置 <code>keystone</code> 管理员用户的密码。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keystone_admin_password: password</span><br></pre></td></tr></table></figure><p>同时，也是登录 <code>Dashboard，admin</code> 使用的密码，你可以根据自己需要进行修改。</p></li><li><p>运行 <code>prechecks</code> 检查配置是否正确，如果有错误，可以先忽略。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> kolla-ansible prechecks</span></span><br></pre></td></tr></table></figure></li><li><p>从<code>docker hub</code>上<code>pull</code>镜像</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> kolla-ansible pull</span></span><br></pre></td></tr></table></figure></li><li><p>部署<code>openstack</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> kolla-ansible deploy</span></span><br></pre></td></tr></table></figure></li><li><p>创建环境变量文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> kolla-ansible post-deploy</span></span><br></pre></td></tr></table></figure><p>这样就创建了<code>/etc/kolla/admin-openrc.sh</code> 环境变量文件。</p></li><li><p>安装 <code>OpenStack Client</code> 端</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> pip install python-openstackclient</span></span><br></pre></td></tr></table></figure></li><li><p>编辑<code>init-runonce</code>文件,设置<code>public network</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> vim /usr/share/kolla-ansible/init-runonce</span></span><br><span class="line">EXT_NET_CIDR=&#x27;10.10.20.0/24&#x27;</span><br><span class="line">EXT_NET_RANGE=&#x27;start=10.10.20.110,end=10.10.20.254&#x27;</span><br><span class="line">EXT_NET_GATEWAY=&#x27;10.10.20.1&#x27;</span><br></pre></td></tr></table></figure></li><li><p>加载<code>OpenStack CLI</code>所需的环境变量</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">source</span> /etc/kolla/admin-openrc.sh</span></span><br></pre></td></tr></table></figure></li><li><p>初始化部署</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">cd</span> /usr/share/kolla-ansible/ &amp;&amp; ./init-runonce</span></span><br></pre></td></tr></table></figure></li><li><p>登陆<code>Dashboard</code><br>用浏览器访问<code>192.168.254.15</code>登陆<code>Dashboard</code><br><img src="https://ljw.howieli.cn/blog/2018-03-29/WX20180329-170306.png"><br><img src="https://ljw.howieli.cn/blog/2018-03-29/WX20180329-170337.png"></p></li><li><p>如果，在部署过程中失败了，亦或是变更了配置信息，需要重新部署，则执行如下命令，清除掉已部署的 <code>Docker</code> 容器，即 <code>OpenStack</code> 服务</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># kolla-ansible destroy -i &#x2F;home&#x2F;multinode --yes-i-really-really-mean-it</span><br></pre></td></tr></table></figure></li><li><p>除此外，还有一些小工具，在需要时，可以使用</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 可用于从系统中移除部署的容器:</span></span><br><span class="line">tools/cleanup-containers</span><br><span class="line"><span class="meta">#</span><span class="bash"> 可用于移除由于网络变化引发的 Docker 启动的neutron-agents 主机：</span></span><br><span class="line">tools/cleanup-host</span><br><span class="line"><span class="meta">#</span><span class="bash"> 可用于从本地缓存中移除所有的 Docker image</span></span><br><span class="line">tools/cleanup-images</span><br></pre></td></tr></table></figure><h3 id="故障诊断与排除"><a href="#故障诊断与排除" class="headerlink" title="故障诊断与排除"></a>故障诊断与排除</h3><p>通过 <code>Kolla</code> 和 <code>Ansible</code> 部署或运行 <code>OpenStack</code> 环境时，如果出现问题，通常可以使用如下一些方法来排查/解决</p></li><li><p>查看指定容器（即指定的服务）的输出日志信息</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> docker logs container_name</span></span><br></pre></td></tr></table></figure></li><li><p>查看指定服务的日志<br>直接 <code>CD</code> 到主机的 <code>/var/lib/docker/volumes/kollalogs/data/</code> 目录下，查看指定服务的日志信息。</p></li><li><p>查看容器的状态</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> docker ps</span></span><br></pre></td></tr></table></figure><p>可以使用 <code>docker ps -a</code> 命令查看到安装的 <code>OpenStack</code> 所有服务的容器</p></li><li><p>进入某个容器</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> docker <span class="built_in">exec</span> -it -u root container_name bash</span></span><br></pre></td></tr></table></figure></li></ul>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;kolla-queens-on-centos7-4&quot;&gt;&lt;a href=&quot;#kolla-queens-on-centos7-4&quot; class=&quot;headerlink&quot; title=&quot;kolla queens on centos7.4&quot;&gt;&lt;/a&gt;kolla queens on centos7.4&lt;/h3&gt;&lt;p&gt;&lt;img src=&quot;https://ljw.howieli.cn/blog/2018-03-29/WX20180329-171345.png&quot;&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="openstack" scheme="https://www.lijiawang.org/categories/openstack/"/>
    
    
    <category term="kolla" scheme="https://www.lijiawang.org/tags/kolla/"/>
    
  </entry>
  
  <entry>
    <title>cinder删除lvm卷很慢的问题</title>
    <link href="https://www.lijiawang.org/cinder%E5%88%A0%E9%99%A4lvm%E5%8D%B7%E5%BE%88%E6%85%A2%E7%9A%84%E9%97%AE%E9%A2%98"/>
    <id>https://www.lijiawang.org/cinder%E5%88%A0%E9%99%A4lvm%E5%8D%B7%E5%BE%88%E6%85%A2%E7%9A%84%E9%97%AE%E9%A2%98</id>
    <published>2018-03-22T06:15:00.000Z</published>
    <updated>2018-03-22T14:22:56.709Z</updated>
    
    <content type="html"><![CDATA[<h3 id="cinder删除lvm卷很慢的问题"><a href="#cinder删除lvm卷很慢的问题" class="headerlink" title="cinder删除lvm卷很慢的问题"></a>cinder删除lvm卷很慢的问题</h3><p>最近搞<code>kolla</code>部署<code>cinder</code>，使用<code>lvm</code>做后端，在<code>cinder-volume</code>服务删除数据卷，数据卷会删除的很慢，在删除这段时间发现宿主机的<code>load</code>飙高，但是检查虚拟机负载没有异常。</p><a id="more"></a><h3 id="查找原因"><a href="#查找原因" class="headerlink" title="查找原因"></a>查找原因</h3><p>使用<code>iotop</code>查看<br><img src="https://ljw.howieli.cn/blog/2018-03-20/WechatIMG33.jpeg"></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DEBUG oslo_concurrency.processutils [req-beeeb583-c07e-4f23-8b63-1f76fc729c9a 0a266a36e53e4469a376f5baa4375064 0a266a36e53e4469a376f5baa4375064 - default default] CMD &quot;sudo cinder-rootwrap /etc/cinder/rootwrap.conf dd count=0 if=/dev/zero of=/dev/mapper/cinder--volumes-volume--beeeb583--c07e--4f23--8b63--1f76fc729c9a  oflag=direct&quot; returned: 0 in 0.127s execute /usr/lib/python2.7/site-packages/oslo_concurrency/processutils.py:374</span><br></pre></td></tr></table></figure><p>从日志里面看到，<code>volume</code>服务调用<code>dd</code>命令对数据卷进行数据清空，同时采用<code>oflag=direct</code>参数直接操作块设备。所以，<code>cinder</code>在删除卷的时间长短和卷的大小有着直接关系。（删除期间<code>iowait</code>值会增高，怪不得<code>load</code>也会飙高）</p><h3 id="原因分析"><a href="#原因分析" class="headerlink" title="原因分析"></a>原因分析</h3><p><code>cinder</code>在删除卷之前要对其做<code>dd</code>操作，是因为<code>LVM</code>在<code>lvremove</code>后重新<code>lvcreate</code>，新的<code>lv</code>里面还保留着老的数据，原来直接删除的<code>lv</code>并不会抹除卷上的数据。<code>cinder</code>之所以要用<code>dd zero</code>,就是为了避免租户<code>A</code>删除卷后，数据不会串到租户<code>B</code>新创建的数据卷上。这样一方面保证租户<code>A</code>的数据安全性，另一方面也避免租户<code>B</code>在使用数据卷的产生的疑惑。</p><h3 id="处理问题"><a href="#处理问题" class="headerlink" title="处理问题"></a>处理问题</h3><ul><li><p>关闭volume安全删除</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># Allowed values: none, zero, shred</span><br><span class="line">volume_clear &#x3D; none</span><br></pre></td></tr></table></figure><p><code>volume_clear</code>采用<code>shred</code>永久删除,</p></li><li><p>置零<code>volume</code>头部<code>100MB</code>左右数据</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># Size in MiB to wipe at start of old volumes. 0 &#x3D;&gt; all (integer value)</span><br><span class="line">volume_clear_size &#x3D; 100</span><br></pre></td></tr></table></figure></li><li><p>调整<code>dd</code>进程<code>io</code>调度优先策略</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># The flag to pass to ionice to alter the i&#x2F;o priority of the process used to</span><br><span class="line"># zero a volume after deletion, for example &quot;-c3&quot; for idle only priority.</span><br><span class="line"># (string value)</span><br><span class="line">volume_clear_ionice &#x3D; -c3</span><br></pre></td></tr></table></figure><h4 id="保证数据绝对安全"><a href="#保证数据绝对安全" class="headerlink" title="保证数据绝对安全"></a>保证数据绝对安全</h4><p><code>volume_clear</code>采用<code>shred</code>永久删除，根据服务器情况适当调整<code>volume_clear_ionice</code>的值。<br>shred会用一些随机内容覆盖文件所在的节点和数据块，<code>cinder</code>在这里条用<code>shred</code>默认参数采用<code>-n 3</code>即重写<code>3</code>次。</p><h4 id="数据相对安全的同时，降低数据卷删除时间"><a href="#数据相对安全的同时，降低数据卷删除时间" class="headerlink" title="数据相对安全的同时，降低数据卷删除时间"></a>数据相对安全的同时，降低数据卷删除时间</h4><p><code>volume_clear</code>采用<code>zero</code>填充，根据情况设置<code>volume_clear_size</code>大小，我们都知道，磁盘的开头部分保存着文件系统的元数据以及索引，清空这部分可以一定程度上保证数据的安全。</p></li></ul><p>至于<code>volume_clear=none</code>这种直接删除<code>lv</code>而不清空数据的方式，我就不建议采用了</p>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;cinder删除lvm卷很慢的问题&quot;&gt;&lt;a href=&quot;#cinder删除lvm卷很慢的问题&quot; class=&quot;headerlink&quot; title=&quot;cinder删除lvm卷很慢的问题&quot;&gt;&lt;/a&gt;cinder删除lvm卷很慢的问题&lt;/h3&gt;&lt;p&gt;最近搞&lt;code&gt;kolla&lt;/code&gt;部署&lt;code&gt;cinder&lt;/code&gt;，使用&lt;code&gt;lvm&lt;/code&gt;做后端，在&lt;code&gt;cinder-volume&lt;/code&gt;服务删除数据卷，数据卷会删除的很慢，在删除这段时间发现宿主机的&lt;code&gt;load&lt;/code&gt;飙高，但是检查虚拟机负载没有异常。&lt;/p&gt;</summary>
    
    
    
    <category term="openstack" scheme="https://www.lijiawang.org/categories/openstack/"/>
    
    
    <category term="openstack" scheme="https://www.lijiawang.org/tags/openstack/"/>
    
  </entry>
  
  <entry>
    <title>ceph写流程分析</title>
    <link href="https://www.lijiawang.org/ceph%E5%86%99%E6%B5%81%E7%A8%8B%E5%88%86%E6%9E%90"/>
    <id>https://www.lijiawang.org/ceph%E5%86%99%E6%B5%81%E7%A8%8B%E5%88%86%E6%9E%90</id>
    <published>2018-02-09T11:33:11.000Z</published>
    <updated>2018-02-09T11:35:18.634Z</updated>
    
    <content type="html"><![CDATA[<h3 id="ceph写流程分析"><a href="#ceph写流程分析" class="headerlink" title="ceph写流程分析"></a>ceph写流程分析</h3><blockquote><p>最近年底了，工作也不太忙了，这几天看看ceph，这几天总结的ceph写流程分析笔记（基于jewel版本&gt;10.2.0），现分享出来，欢迎指点。</p></blockquote><p><img src="https://ljw.howieli.cn/blog/2018-02-09/ceph.jpg"></p><a id="more"></a><h3 id="rbd到OSD映射关系"><a href="#rbd到OSD映射关系" class="headerlink" title="rbd到OSD映射关系"></a>rbd到OSD映射关系</h3><p>客户端使用<code>RBD</code>设备，使用<code>librbd</code>、<code>librados</code>库进行访问管理块设备。</p><ul><li><p>1、创建 一个<code>pool</code>，为这个<code>pool</code>指定<code>pg</code>的数量，同时在这个<code>pool</code>中指明保存数据的副本数（通常为<code>3</code>个副本）。</p></li><li><p>2、在这个<code>pool</code>中创建一个<code>rbd</code>设备<code>rbd0</code>，那么这个<code>rbd0</code>都会保存三份，在创建<code>rbd0</code>时必须指定<code>rbd</code>的<code>size</code>，对于这个<code>rbd0</code>的任何操作不能超过这个<code>size</code>。</p></li><li><p>3、将这个块设备进行切块，每个块的大小默认为<code>4M</code>，并且每个块都有一个名字，名字就是<code>object+</code>序号。</p></li><li><p>4、将每个<code>object</code>通过<code>pg</code>进行副本位置的分配，<code>pg</code>会寻找<code>3</code>个<code>osd</code>，把这个<code>object</code>分别保存在这三个<code>osd</code>上。最后对于<code>object</code>的存储就变成了存储一个文件 <code>rbd0.object1.file</code>。数据层次映射图如下：<br><img src="https://ljw.howieli.cn/blog/2018-02-09/WX20180209-105337.png"></p></li></ul><p>经过<code>pool</code>，<code>rbd</code>，<code>object</code>、<code>pg</code>的层层映射关系，在<code>PG</code>这一层中，已经知道存储数据的<code>3</code>个<code>OSD</code>所在位置及主从关系。<br>客户端与<code>primay OSD</code>建立<code>SOCKET</code> 通信，将要写入的数据传给<code>primary OSD</code>，由<code>primary OSD</code>再将数据发送给其他<code>replica OSD</code>数据节点。</p><h4 id="RBD保存形式"><a href="#RBD保存形式" class="headerlink" title="RBD保存形式"></a>RBD保存形式</h4><p>如下图所示，<code>Ceph</code> 系统中不同层次的组件/用户所看到的数据的形式是不一样的：<br><img src="https://ljw.howieli.cn/blog/2018-02-09/697113-20150928152213918-1722034221.jpg"></p><ul><li><code>Ceph</code> 客户端所见的是一个完整的连续的二进制数据块，其大小为创建 <code>RBD image</code> 是设置的大小或者 <code>resize</code> 的大小，客户端可以从头或者从某个位置开始写入二进制数据。</li><li><code>librados</code> 负责在 <code>RADOS</code> 中创建对象（<code>object</code>），其大小为 <code>pool</code> 的 <code>order</code> 决定，默认情况下 <code>order = 22</code> 此时 <code>object</code> 大小为 <code>4MB</code>；以及负责将客户端传入的二进制块条带化为若干个条带（<code>stripe</code>）。</li><li><code>librados</code> 控制哪个条带由哪个 <code>OSD</code> 写入（条带 —写入哪个—-&gt; <code>object</code> —-位于哪个 —-&gt; <code>OSD</code>）</li></ul><p><code>OSD</code> 负责创建在文件系统中创建文件，并将 <code>librados</code> 传入的数据写入数据。</p><ol><li><p><code>Ceph client</code> 调用 <code>librados</code> 创建一个 <code>RBD image</code>，这时候不会做存储空间分配，而是创建若干元数据对象来保存元数据信息。</p></li><li><p><code>Ceph client</code> 调用 <code>librados</code> 开始写数据。<code>librados</code> 计算条带、<code>object</code> 等，然后开始写第一个 <code>stripe</code> 到特定的目标 <code>object</code>。</p></li><li><p><code>librados</code> 根据 <code>CRUSH</code> 算法，计算出 <code>object</code> 所对应的主 <code>OSD ID</code>，并将二进制数据发给它。</p></li><li><p>主 <code>OSD</code> 负责调用文件系统接口将二进制数据写入磁盘上的文件（每个 <code>object</code> 对应一个 <code>file，file</code> 的内容是一个或者多个 <code>stripe</code>）。</p></li><li><p>主 <code>ODS</code> 完成数据写入后，它使用 <code>CRUSH</code> 算啊计算出第二个<code>OSD</code>（<code>secondary OSD</code>）和第三个<code>OSD</code>（<code>tertiary OSD</code>）的位置，然后向这两个 <code>OSD</code> 拷贝对象。都完成后，它向 <code>ceph client</code>反馈该 <code>object</code> 保存完毕。</p><p><code>Ceph client</code> 向一个 <code>RBD image</code> 写入二进制数据（假设 pool 的拷贝份数为 3）<br><img src="https://ljw.howieli.cn/blog/2018-02-09/WX20180209-110527.png"></p></li></ol><h3 id="客户的写流程操作"><a href="#客户的写流程操作" class="headerlink" title="客户的写流程操作"></a>客户的写流程操作</h3><p>在客户端使用 rbd 时一般有两种方法：</p><ul><li>第一种 是 <code>Kernel rbd</code>。就是创建了<code>rbd</code>设备后，把<code>rbd</code>设备<code>map</code>到内核中，形成一个虚拟的块设备，这时这个块设备同其他通用块设备一样，一般的设备文件为<code>/dev/rbd0</code>，后续直接使用这个块设备文件就可以了，可以把 <code>/dev/rbd0</code> 格式化后 <code>mount</code> 到某个目录，也可以直接作为裸设备使用。这时对<code>rbd</code>设备的操作都通过<code>kernel rbd</code>操作方法进行的。 </li><li>第二种是 <code>librbd</code> 方式。就是创建了<code>rbd</code>设备后，这时可以使用<code>librbd</code>、<code>librados</code>库进行访问管理块设备。这种方式不会<code>map</code>到内核，直接调用<code>librbd</code>提供的接口，可以实现对<code>rbd</code>设备的访问和管理，但是不会在客户端产生块设备文件。</li></ul><p>应用写入<code>rbd</code>块设备的过程：</p><ol><li>应用调用 <code>librbd</code> 接口或者对<code>linux</code> 内核虚拟块设备写入二进制块。下面以 <code>librbd</code> 为例。</li><li><code>librbd</code> 对二进制块进行分块，默认块大小为 <code>4M</code>，每一块都有名字，成为一个对象</li><li><code>librbd</code> 调用 <code>librados</code> 将对象写入 <code>Ceph</code> 集群</li><li><code>librados</code> 向主 <code>OSD</code> 写入分好块的二进制数据块 (先建立<code>TCP/IP</code>连接，然后发送消息给 <code>OSD</code>，<code>OSD</code> 接收后写入其磁盘)</li><li>主 <code>OSD</code> 负责同时向一个或者多个次 <code>OSD</code> 写入副本。注意这里是写到日志（<code>Journal</code>）就返回，因此，使用<code>SSD</code>作为<code>Journal</code>的话，可以提高响应速度，做到服务器端对客户端的快速同步返回写结果（<code>ack</code>）。</li><li>当主次<code>OSD</code>都写入完成后，主 <code>OSD</code> 向客户端返回写入成功。</li><li>当一段时间（也许得几秒钟）后<code>Journal</code> 中的数据向磁盘写入成功后，<code>Ceph</code>通过事件通知客户端数据写入磁盘成功（<code>commit</code>），此时，客户端可以将写缓存中的数据彻底清除掉了。</li><li>默认地，<code>Ceph</code> 客户端会缓存写入的数据直到收到集群的<code>commit</code>通知。如果此阶段内（在写方法返回到收到<code>commit</code>通知之间）<code>OSD</code> 出故障导致数据写入文件系统失败，<code>Ceph</code> 将会允许客户端重做尚未提交的操作（<code>replay</code>）。因此，<code>PG</code> 有个状态叫 <code>replay：“The placement group is waiting for clients to replay operations after an OSD crashed.”</code>。<br><img src="https://ljw.howieli.cn/blog/2018-02-09/697113-20160507205321796-1810415144.jpg"><br>也就是，文件系统负责文件处理，<code>librbd</code> 负责块处理，<code>librados</code> 负责对象处理，<code>OSD</code> 负责将数据写入在<code>Journal</code>和磁盘中。</li></ol><h3 id="osd端写操作处理流程"><a href="#osd端写操作处理流程" class="headerlink" title="osd端写操作处理流程"></a>osd端写操作处理流程</h3><p>而对于写操作而言，由于要保证数据写入的同步性就会复杂很多：</p><ol><li><p>首先客户端会将数据发送给主osd，</p></li><li><p>主<code>osd</code>同样要先进行写操作预处理，完成后它要发送写消息给其他的从<code>osd</code>，让他们对副本<code>pg</code>进行更改，</p></li><li><p>从<code>osd</code>通过<code>FileJournal</code>完成写操作到<code>Journal</code>中后发送消息告诉主<code>osd</code>说完成，进入<code>5</code></p></li><li><p>当主<code>osd</code>收到所有的从<code>osd</code>完成写操作的消息后，会通过<code>FileJournal</code>完成自身的写操作到<code>Journal</code>中。完成后会通知客户端，已经完成了写操作。</p></li><li><p>主<code>osd</code>，从<code>osd</code>的线程开始工作调用<code>Filestore</code>将<code>Journal</code>中的数据写入到底层文件系统中。</p></li></ol><p>今天就先总结这么多，总结的不太好，请谅解</p>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;ceph写流程分析&quot;&gt;&lt;a href=&quot;#ceph写流程分析&quot; class=&quot;headerlink&quot; title=&quot;ceph写流程分析&quot;&gt;&lt;/a&gt;ceph写流程分析&lt;/h3&gt;&lt;blockquote&gt;
&lt;p&gt;最近年底了，工作也不太忙了，这几天看看ceph，这几天总结的ceph写流程分析笔记（基于jewel版本&amp;gt;10.2.0），现分享出来，欢迎指点。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&quot;https://ljw.howieli.cn/blog/2018-02-09/ceph.jpg&quot;&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="ceph" scheme="https://www.lijiawang.org/categories/ceph/"/>
    
    
    <category term="ceph" scheme="https://www.lijiawang.org/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title>手动构建Openstack镜像</title>
    <link href="https://www.lijiawang.org/%E6%89%8B%E5%8A%A8%E6%9E%84%E5%BB%BAOpenstack%E9%95%9C%E5%83%8F"/>
    <id>https://www.lijiawang.org/%E6%89%8B%E5%8A%A8%E6%9E%84%E5%BB%BAOpenstack%E9%95%9C%E5%83%8F</id>
    <published>2018-01-12T06:29:08.000Z</published>
    <updated>2018-01-12T06:57:56.878Z</updated>
    
    <content type="html"><![CDATA[<h3 id="手动构建Openstack镜像"><a href="#手动构建Openstack镜像" class="headerlink" title="手动构建Openstack镜像"></a>手动构建Openstack镜像</h3><p>本文以centos7镜像为例，详细介绍手动制作Openstack镜像的步骤，镜像支持一下几个功能：</p><ul><li>支持密码注入功能(nova boot时通过–admin-pass参数指定设置初始密码）</li><li>支持根分区自动调整(根分区自动调整为flavor disk大小，而不是原始镜像分区大小)</li><li>支持动态修改密码(使用nova set-password命令可以修改管理员密码)</li></ul><p>镜像的宿主机操作系统为CentOs7.4，开启了VT功能(使用kvm-ok命令验证)并安装了libvirt系列工具，包括virsh、virt-manager、libguestfs-tools等。</p><a id="more"></a><h3 id="下载iso镜像"><a href="#下载iso镜像" class="headerlink" title="下载iso镜像"></a>下载iso镜像</h3><p>下载centos7.4.iso镜像，可以选择中国镜像源，相对国外镜像源下载速度会快很多，这里我已经下载好了。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> qemu-img info CentOS-7-x86_64-DVD-1708.iso</span> </span><br><span class="line">image: CentOS-7-x86_64-DVD-1708.iso</span><br><span class="line">file format: raw</span><br><span class="line">virtual size: 4.2G (4521459712 bytes)</span><br><span class="line">disk size: 4.2G</span><br></pre></td></tr></table></figure><h3 id="创建虚拟机"><a href="#创建虚拟机" class="headerlink" title="创建虚拟机"></a>创建虚拟机</h3><p>首先创建一个qcow2格式镜像文件，用于虚拟机的根磁盘，大小10G就够了。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> qemu-img create -f qcow2 centos.qcow2 10G</span></span><br><span class="line">Formatting &#x27;centos.qcow2&#x27;, fmt=qcow2 size=10737418240 encryption=off cluster_size=65536 lazy_refcounts=off </span><br></pre></td></tr></table></figure><p>使用以下脚本创建并启动虚拟机：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">NAME&#x3D;centos</span><br><span class="line">ROOT_DISK&#x3D;centos.qcow2</span><br><span class="line">CDROM&#x3D;&#96;pwd&#96;&#x2F;CentOS-7-x86_64-DVD-1708.iso</span><br><span class="line">virt-install --virt-type kvm --name $NAME --ram 1024 \</span><br><span class="line">--disk $ROOT_DISK,format&#x3D;qcow2 \</span><br><span class="line">--network network&#x3D;default \</span><br><span class="line">--graphics vnc,listen&#x3D;0.0.0.0 --noautoconsole \</span><br><span class="line">--os-type&#x3D;linux --os-variant&#x3D;rhel7 \</span><br><span class="line">--cdrom&#x3D;$CDROM</span><br></pre></td></tr></table></figure><p>启动完成后，使用vnc client连接或者使用virt-manager、virt-viewer连接,这里我直接用virt-manager。</p><h3 id="安装OS"><a href="#安装OS" class="headerlink" title="安装OS"></a>安装OS</h3><p>进入虚拟机控制台可以看到CentOS的启动菜单，选择Install Centos 7，继续选择语言后将进入INSTALLION SUMMARY，其中大多数配置默认即可，SOFTWARE SELECTION选择Minimal Install，INSTALLATION DESTINATION需要选择手动配置分区，我们只需要一个根分区即可，不需要swap分区，文件系统选择ext4或者xfs，存储驱动选择Virtio Block Device，如图：<br><img src="https://ljw.howieli.cn/blog/2018-1-12/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20180112113728.png"></p><p>配置完成后就可以开始安装了，在CONFIGURATION中设置root临时密码，只需要暂时记住这个临时密码，制作完后cloud-init会重新设置root初始密码。</p><p>大约几分钟后，即可自动完成安装配置工作，最后点击右下角的reboot重启退出虚拟机。</p><h3 id="配置OS"><a href="#配置OS" class="headerlink" title="配置OS"></a>配置OS</h3><p>安装好操作系统后，需要进行配置才能作为glance镜像使用，启动虚拟机</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> virsh list --all</span></span><br><span class="line"> Id    名称                         状态</span><br><span class="line">----------------------------------------------------</span><br><span class="line"> -     centos                         关闭</span><br><span class="line"><span class="meta">#</span><span class="bash"> virsh start centos</span></span><br></pre></td></tr></table></figure><p>如果云主机不需要支持root ssh远程登录，需要关闭root远程ssh登录功能，修改配置文件/etc/ssh/sshd_config并修改PermitRootLogin值为no，默认是开启的（这里我不做更改，如果更改需要重启ssh服务生效）。</p><p>为了加快安装速度，可以配置为本地软件源仓库，若没有本地镜像仓库，则选择国内的软件源，相对官网的速度下载要快。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> yum -y install wget</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">cd</span> /etc/yum.repos.d/ &amp;&amp; rm -rf * &amp;&amp; <span class="built_in">cd</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo</span></span><br></pre></td></tr></table></figure><h3 id="acpid"><a href="#acpid" class="headerlink" title="acpid"></a>acpid</h3><p><a href="https://wiki.archlinux.org/index.php/Acpid">acpid</a>是一个用户空间的服务进程, 用来处理电源相关事件,比如将kernel中的电源事件转发给应用程序，告诉应用程序安全的退出，防止应用程序异常退出导致数据损坏。libvirt可以通过向guest虚拟机发送acpid事件触发电源操作，使虚拟机安全关机、重启等操作，相对于强制执行关闭电源操作更安全。通过acpid事件发送开关机信号即我们经常所说的软重启或者软关机。<br>为了支持软操作，虚拟机需要安装acpid服务，并设置开机自启动：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> yum -y install acpid</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> systemctl <span class="built_in">enable</span> acpid</span></span><br></pre></td></tr></table></figure><p>提示：</p><ul><li>用户执行重启或者关机操作时，OpenStack会首先尝试调用libvirt的shutdown方法，即软关机。</li><li>当软关机执行失败或者超时(默认120秒)，则会调动libvirt的destroy方法，即强制关机，因此如果虚拟机关机或者重启很慢，很可能是acpid没有正常运行。</li><li>为了使虚拟机进程安全退出，减少数据损坏风险，尽量使用软操作，硬操作可能导致程序崩溃或者数据丢失。</li></ul><h3 id="console-log"><a href="#console-log" class="headerlink" title="console log"></a>console log</h3><p>当操作系统内核崩溃时会报出内核系统crash出错信息，通常启动的时候一闪而过, 而此时系统还没有起来，不能通过远程工具(比如ssh)进入系统查看，我们可以通过配置grub，把这些日志重定向到Serial Console中，这样我们就可以通过Serial console来访问错误信息，以供分析和排错使用。<br>修改配置文件/etc/default/grub，设置GRUB_CMDLINE_LINUX，：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">GRUB_CMDLINE_LINUX=&quot;crashkernel=auto console=tty0 console=ttyS0,115200n8&quot;</span><br></pre></td></tr></table></figure><p>通过这个配置，内核信息会以115200的波特率同时发送到tty0和ttyS0串行端口设备。libvirt可以通过一个普通文件模拟这个串行端口：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">serial</span> <span class="attr">type</span>=<span class="string">&#x27;file&#x27;</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">source</span> <span class="attr">path</span>=<span class="string">&#x27;/var/lib/nova/instances/99579ce1-f4c4-4031-a56c-68e85a3d037a/console.log&#x27;</span>/&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">target</span> <span class="attr">port</span>=<span class="string">&#x27;0&#x27;</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">serial</span>&gt;</span></span><br></pre></td></tr></table></figure><p>这样内核产生的日志发到ttyS0，实际上写到console.log文件中。</p><p>OpenStack通过nova console-log命令可以获取该文件内容，查看错误日志。</p><h3 id="qemu-guest-agent"><a href="#qemu-guest-agent" class="headerlink" title="qemu-guest-agent"></a>qemu-guest-agent</h3><p>qemu-guest-agent是运行在虚拟机内部的一个服务，libvirt会在本地创建一个unix socket，模拟为虚拟机内部的一个串口设备，从而实现了宿主机与虚拟机通信，这种方式不依赖于TCP/IP网络，实现方式简单方便。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">channel</span> <span class="attr">type</span>=<span class="string">&#x27;unix&#x27;</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">source</span> <span class="attr">mode</span>=<span class="string">&#x27;bind&#x27;</span> <span class="attr">path</span>=<span class="string">&#x27;/var/lib/libvirt/qemu/org.qemu.guest_agent.0.instance-00003c2c.sock&#x27;</span>/&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">target</span> <span class="attr">type</span>=<span class="string">&#x27;virtio&#x27;</span> <span class="attr">name</span>=<span class="string">&#x27;org.qemu.guest_agent.0&#x27;</span>/&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">address</span> <span class="attr">type</span>=<span class="string">&#x27;virtio-serial&#x27;</span> <span class="attr">controller</span>=<span class="string">&#x27;0&#x27;</span> <span class="attr">bus</span>=<span class="string">&#x27;0&#x27;</span> <span class="attr">port</span>=<span class="string">&#x27;1&#x27;</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">channel</span>&gt;</span></span><br></pre></td></tr></table></figure><p>如上宿主机的socket文件为org.qemu.guest_agent.0.instance-00003c2c.sock，在虚拟机内部为/dev/virtio-ports/org.qemu.guest_agent.0。</p><p>通过这种方式，宿主机可以发送指令写到socket文件中，虚拟机内部的qemu-guest-agent会轮询查看这个串行设备是否有指令，一旦接收到指令就可以执行对应的脚本，从而实现了宿主机控制虚拟机执行命令的功能，其中最常用的指令就是通过libvirt修改虚拟机密码。更多关于qemu-guest-agent请参考<a href="http://link.zhihu.com/?target=http://wiki.qemu.org/Features/QAPI/GuestAgent">官方文档</a>。</p><p>为了支持OpenStack平台动态修改虚拟机密码功能，我们需要手动安装qemu-guest-agent：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> yum install -y qemu-guest-agent</span></span><br></pre></td></tr></table></figure><p>修改/etc/sysconfig/qemu-ga配置文件:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">virsh qemu-agent-command instance-000028d5 &#x27;&#123;&quot;execute&quot;:&quot;guest-info&quot;&#125;&#x27; | python -m json.tool | grep &#x27;name&#x27; | cut -d &#x27;:&#x27; -f 2 | tr -d &#x27;&quot;,&#x27;</span><br><span class="line"> ...</span><br><span class="line"> guest-set-user-password</span><br><span class="line"> guest-get-fsinfo</span><br><span class="line"> guest-set-vcpus</span><br><span class="line"> guest-get-vcpus</span><br><span class="line"> ...</span><br></pre></td></tr></table></figure><p>确认包含guest-set-user-password指令，支持修改管理员密码。</p><h3 id="zeroconf"><a href="#zeroconf" class="headerlink" title="zeroconf"></a>zeroconf</h3><p>zeroconf启动时会自动创建一条路由169.254.0.0/16，而虚拟机访问metadata服务的地址正好是169.254.169.254，如果启动了zeroconf服务，由于路由冲突，虚拟机不能通过169.254.169.254路由到网络节点的metadata服务了。OpenStack虚拟机通常都是通过DHCP获取IP的，因此我们并不需要zeroconf服务。为了虚拟机能够访问metadata服务，我们必须禁止zeroconf服务，关于该问题的更详细讨论可参考<a href="https://zhuanlan.zhihu.com/p/30385809">bug#983611</a>：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">echo</span> <span class="string">&quot;NOZEROCONF=yes&quot;</span> &gt;&gt; /etc/sysconfig/network</span></span><br></pre></td></tr></table></figure><h3 id="cloud-init"><a href="#cloud-init" class="headerlink" title="cloud-init"></a>cloud-init</h3><p>接下来安装cloud-init，cloud-init是虚拟机第一次启动时执行的脚本，主要负责从metadata服务中拉取配置信息，完成虚拟机的初始化工作，比如设置主机名、初始化密码以及注入密钥等。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> yum install -y cloud-init</span></span><br></pre></td></tr></table></figure><h3 id="growpart"><a href="#growpart" class="headerlink" title="growpart"></a>growpart</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> yum update -y</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> yum install -y epel-release</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> yum install -y cloud-utils-growpart.x86.64</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> rpm -qa kernel | sed <span class="string">&#x27;s/^kernel-//&#x27;</span>  | xargs -I &#123;&#125; dracut -f /boot/initramfs-&#123;&#125;.img &#123;&#125;</span></span><br></pre></td></tr></table></figure><p>完成以上工作后，我们的镜像配置基本结束，删除一些无用文件，清理history命令后执行关机：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> shutdown -h now</span></span><br></pre></td></tr></table></figure><h3 id="移除本地信息"><a href="#移除本地信息" class="headerlink" title="移除本地信息"></a>移除本地信息</h3><p>在宿主机上运行以下命名，移除宿主机信息，比如mac地址等。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> virt-sysprep -d centos</span></span><br><span class="line">[   0.0] Examining the guest ...</span><br><span class="line">[  41.1] Performing &quot;abrt-data&quot; ...</span><br><span class="line">[  42.4] Performing &quot;backup-files&quot; ...</span><br><span class="line">[  44.4] Performing &quot;bash-history&quot; ...</span><br><span class="line">[  44.4] Performing &quot;blkid-tab&quot; ...</span><br><span class="line">[  44.5] Performing &quot;crash-data&quot; ...</span><br><span class="line">[  44.5] Performing &quot;cron-spool&quot; ...</span><br><span class="line">[  44.5] Performing &quot;dhcp-client-state&quot; ...</span><br><span class="line">[  44.5] Performing &quot;dhcp-server-state&quot; ...</span><br><span class="line">[  44.5] Performing &quot;dovecot-data&quot; ...</span><br><span class="line">[  44.5] Performing &quot;logfiles&quot; ...</span><br><span class="line">[  44.7] Performing &quot;machine-id&quot; ...</span><br><span class="line">[  44.7] Performing &quot;mail-spool&quot; ...</span><br><span class="line">[  44.7] Performing &quot;net-hostname&quot; ...</span><br><span class="line">[  44.8] Performing &quot;net-hwaddr&quot; ...</span><br><span class="line">[  44.8] Performing &quot;pacct-log&quot; ...</span><br><span class="line">[  44.8] Performing &quot;package-manager-cache&quot; ...</span><br><span class="line">[  44.8] Performing &quot;pam-data&quot; ...</span><br><span class="line">[  44.8] Performing &quot;passwd-backups&quot; ...</span><br><span class="line">[  44.8] Performing &quot;puppet-data-log&quot; ...</span><br><span class="line">[  44.8] Performing &quot;rh-subscription-manager&quot; ...</span><br><span class="line">[  44.8] Performing &quot;rhn-systemid&quot; ...</span><br><span class="line">[  44.8] Performing &quot;rpm-db&quot; ...</span><br><span class="line">[  44.8] Performing &quot;samba-db-log&quot; ...</span><br><span class="line">[  44.8] Performing &quot;script&quot; ...</span><br><span class="line">[  44.8] Performing &quot;smolt-uuid&quot; ...</span><br><span class="line">[  44.8] Performing &quot;ssh-hostkeys&quot; ...</span><br><span class="line">[  44.8] Performing &quot;ssh-userdir&quot; ...</span><br><span class="line">[  44.8] Performing &quot;sssd-db-log&quot; ...</span><br><span class="line">[  44.9] Performing &quot;tmp-files&quot; ...</span><br><span class="line">[  44.9] Performing &quot;udev-persistent-net&quot; ...</span><br><span class="line">[  44.9] Performing &quot;utmp&quot; ...</span><br><span class="line">[  44.9] Performing &quot;yum-uuid&quot; ...</span><br><span class="line">[  44.9] Performing &quot;customize&quot; ...</span><br><span class="line">[  44.9] Setting a random seed</span><br><span class="line">[  45.4] Performing &quot;lvm-uuids&quot; ...</span><br></pre></td></tr></table></figure><p>删除虚拟机，镜像制作完成。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> virsh list --all</span></span><br><span class="line"> Id    名称                         状态</span><br><span class="line">----------------------------------------------------</span><br><span class="line"> -     centos                         关闭</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> virsh undefine centos</span></span><br><span class="line">域 centos 已经被取消定义</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> virsh list --all</span></span><br><span class="line"> Id    名称                         状态</span><br><span class="line">----------------------------------------------------</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="上传镜像"><a href="#上传镜像" class="headerlink" title="上传镜像"></a>上传镜像</h3><p>镜像制作完成，上传centos.qcow2到glance服务中</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> glance image-create --file ./centos.qcow2 --disk-format qcow2 \</span></span><br><span class="line"><span class="bash">    --container-format bare --name CentOS-7.2 --progress</span></span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;手动构建Openstack镜像&quot;&gt;&lt;a href=&quot;#手动构建Openstack镜像&quot; class=&quot;headerlink&quot; title=&quot;手动构建Openstack镜像&quot;&gt;&lt;/a&gt;手动构建Openstack镜像&lt;/h3&gt;&lt;p&gt;本文以centos7镜像为例，详细介绍手动制作Openstack镜像的步骤，镜像支持一下几个功能：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;支持密码注入功能(nova boot时通过–admin-pass参数指定设置初始密码）&lt;/li&gt;
&lt;li&gt;支持根分区自动调整(根分区自动调整为flavor disk大小，而不是原始镜像分区大小)&lt;/li&gt;
&lt;li&gt;支持动态修改密码(使用nova set-password命令可以修改管理员密码)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;镜像的宿主机操作系统为CentOs7.4，开启了VT功能(使用kvm-ok命令验证)并安装了libvirt系列工具，包括virsh、virt-manager、libguestfs-tools等。&lt;/p&gt;</summary>
    
    
    
    <category term="openstack" scheme="https://www.lijiawang.org/categories/openstack/"/>
    
    
    <category term="openstack" scheme="https://www.lijiawang.org/tags/openstack/"/>
    
  </entry>
  
  <entry>
    <title>ceph ( requests are blocked ) 异常解决方法</title>
    <link href="https://www.lijiawang.org/ceph%20(%20requests%20are%20blocked%20)%20%E5%BC%82%E5%B8%B8%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95"/>
    <id>https://www.lijiawang.org/ceph%20(%20requests%20are%20blocked%20)%20%E5%BC%82%E5%B8%B8%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95</id>
    <published>2018-01-08T09:00:13.000Z</published>
    <updated>2018-01-10T06:06:16.074Z</updated>
    
    <content type="html"><![CDATA[<h3 id="ceph-requests-are-blocked-异常解决方法"><a href="#ceph-requests-are-blocked-异常解决方法" class="headerlink" title="ceph ( requests are blocked ) 异常解决方法"></a>ceph ( requests are blocked ) 异常解决方法</h3><p><img src="https://ljw.howieli.cn/blog/2018-1-8/%E4%BF%AE%E7%90%86%E5%B7%A5.png"></p><a id="more"></a><p>最近发现客户kolla平台的ceph环境遇到下面异常错误</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">(ceph-mon)[root@control01 &#x2F;]# ceph -s</span><br><span class="line">    cluster b233a0b7-4e21-4375-bca8-e215c056cc25</span><br><span class="line">     health HEALTH_WARN</span><br><span class="line">            2 requests are blocked &gt; 32 sec</span><br><span class="line">     monmap e1: 3 mons at &#123;10.254.253.1&#x3D;10.254.253.1:6789&#x2F;0,10.254.253.2&#x3D;10.254.253.2:6789&#x2F;0,10.254.253.3&#x3D;10.254.253.3:6789&#x2F;0&#125;</span><br><span class="line">            election epoch 26, quorum 0,1,2 10.254.253.1,10.254.253.2,10.254.253.3</span><br><span class="line">     osdmap e380: 90 osds: 90 up, 90 in</span><br><span class="line">            flags sortbitwise,require_jewel_osds</span><br><span class="line">      pgmap v1730087: 1008 pgs, 11 pools, 3502 GB data, 886 kobjects</span><br><span class="line">            10463 GB used, 235 TB &#x2F; 245 TB avail</span><br><span class="line">                1007 active+clean</span><br><span class="line">                   1 active+clean+scrubbing+deep</span><br><span class="line">  client io 1838 kB&#x2F;s rd, 100 MB&#x2F;s wr, 457 op&#x2F;s rd, 929 op&#x2F;s wr</span><br></pre></td></tr></table></figure><p>注意: 1 requests are blocked &gt; 32 sec 有可能是在数据迁移过程中, 用户正在对该数据块进行访问, 但访问还没有完成, 数据就迁移到别的 OSD 中, 那么就会导致有请求被 block, 对用户也是有影响的</p><h3 id="解决思路"><a href="#解决思路" class="headerlink" title="解决思路"></a>解决思路</h3><p>1.寻找block的请求</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(ceph-mon)[root@control01 &#x2F;]# ceph health detail</span><br><span class="line">HEALTH_WARN 2 requests are blocked &gt; 32 sec; 1 osds have slow requests</span><br><span class="line">2 ops are blocked &gt; 4194.3 sec on osd.5</span><br><span class="line">1 osds have slow requests</span><br></pre></td></tr></table></figure><p>可以考到osd.5具有一个操作block<br>2.查找osd对应的主机</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br></pre></td><td class="code"><pre><span class="line">(ceph-mon)[root@control01 &#x2F;]# ceph osd tree</span><br><span class="line">ID  WEIGHT    TYPE NAME              UP&#x2F;DOWN REWEIGHT PRIMARY-AFFINITY </span><br><span class="line"> -1 270.00000 root default                                             </span><br><span class="line"> -2  27.00000     host 10.254.253.1                                    </span><br><span class="line">  0   3.00000         osd.0               up  1.00000          1.00000 </span><br><span class="line">  5   3.00000         osd.5               up  1.00000          1.00000 </span><br><span class="line"> 10   3.00000         osd.10              up  1.00000          1.00000 </span><br><span class="line"> 15   3.00000         osd.15              up  1.00000          1.00000 </span><br><span class="line"> 21   3.00000         osd.21              up  1.00000          1.00000 </span><br><span class="line"> 27   3.00000         osd.27              up  1.00000          1.00000 </span><br><span class="line"> 30   3.00000         osd.30              up  1.00000          1.00000 </span><br><span class="line"> 36   3.00000         osd.36              up  1.00000          1.00000 </span><br><span class="line"> 42   3.00000         osd.42              up  1.00000          1.00000 </span><br><span class="line"> -3  27.00000     host 10.254.253.5                                    </span><br><span class="line">  2   3.00000         osd.2               up  1.00000          1.00000 </span><br><span class="line">  6   3.00000         osd.6               up  1.00000          1.00000 </span><br><span class="line"> 11   3.00000         osd.11              up  1.00000          1.00000 </span><br><span class="line"> 17   3.00000         osd.17              up  1.00000          1.00000 </span><br><span class="line"> 20   3.00000         osd.20              up  1.00000          1.00000 </span><br><span class="line"> 25   3.00000         osd.25              up  1.00000          1.00000 </span><br><span class="line"> 31   3.00000         osd.31              up  1.00000          1.00000 </span><br><span class="line"> 35   3.00000         osd.35              up  1.00000          1.00000 </span><br><span class="line"> 41   3.00000         osd.41              up  1.00000          1.00000 </span><br><span class="line"> -4  27.00000     host 10.254.253.2                                    </span><br><span class="line">  1   3.00000         osd.1               up  1.00000          1.00000 </span><br><span class="line">  7   3.00000         osd.7               up  1.00000          1.00000 </span><br><span class="line"> 12   3.00000         osd.12              up  1.00000          1.00000 </span><br><span class="line"> 16   3.00000         osd.16              up  1.00000          1.00000 </span><br><span class="line"> 22   3.00000         osd.22              up  1.00000          1.00000 </span><br><span class="line"> 26   3.00000         osd.26              up  1.00000          1.00000 </span><br><span class="line"> 32   3.00000         osd.32              up  1.00000          1.00000 </span><br><span class="line"> 37   3.00000         osd.37              up  1.00000          1.00000 </span><br><span class="line"> 40   3.00000         osd.40              up  1.00000          1.00000 </span><br><span class="line"> -6  27.00000     host 10.254.253.4                                    </span><br><span class="line">  3   3.00000         osd.3               up  1.00000          1.00000 </span><br><span class="line">  8   3.00000         osd.8               up  1.00000          1.00000 </span><br><span class="line"> 13   3.00000         osd.13              up  1.00000          1.00000 </span><br><span class="line"> 18   3.00000         osd.18              up  1.00000          1.00000 </span><br><span class="line"> 24   3.00000         osd.24              up  1.00000          1.00000 </span><br><span class="line"> 29   3.00000         osd.29              up  1.00000          1.00000 </span><br><span class="line"> 33   3.00000         osd.33              up  1.00000          1.00000 </span><br><span class="line"> 39   3.00000         osd.39              up  1.00000          1.00000 </span><br><span class="line"> 43   3.00000         osd.43              up  1.00000          1.00000 </span><br><span class="line"> -5  27.00000     host 10.254.253.3                                    </span><br><span class="line">  4   3.00000         osd.4               up  1.00000          1.00000 </span><br><span class="line">  9   3.00000         osd.9               up  1.00000          1.00000 </span><br><span class="line"> 14   3.00000         osd.14              up  1.00000          1.00000 </span><br><span class="line"> 19   3.00000         osd.19              up  1.00000          1.00000 </span><br><span class="line"> 23   3.00000         osd.23              up  1.00000          1.00000 </span><br><span class="line"> 28   3.00000         osd.28              up  1.00000          1.00000 </span><br><span class="line"> 34   3.00000         osd.34              up  1.00000          1.00000 </span><br><span class="line"> 38   3.00000         osd.38              up  1.00000          1.00000 </span><br><span class="line"> 44   3.00000         osd.44              up  1.00000          1.00000 </span><br><span class="line"> -7  27.00000     host 10.254.253.8                                    </span><br><span class="line"> 47   3.00000         osd.47              up  1.00000          1.00000 </span><br><span class="line"> 51   3.00000         osd.51              up  1.00000          1.00000 </span><br><span class="line"> 56   3.00000         osd.56              up  1.00000          1.00000 </span><br><span class="line"> 61   3.00000         osd.61              up  1.00000          1.00000 </span><br><span class="line"> 66   3.00000         osd.66              up  1.00000          1.00000 </span><br><span class="line"> 71   3.00000         osd.71              up  1.00000          1.00000 </span><br><span class="line"> 76   3.00000         osd.76              up  1.00000          1.00000 </span><br><span class="line"> 81   3.00000         osd.81              up  1.00000          1.00000 </span><br><span class="line"> 86   3.00000         osd.86              up  1.00000          1.00000 </span><br><span class="line"> -8  27.00000     host 10.254.253.7                                    </span><br><span class="line"> 46   3.00000         osd.46              up  1.00000          1.00000 </span><br><span class="line"> 52   3.00000         osd.52              up  1.00000          1.00000 </span><br><span class="line"> 57   3.00000         osd.57              up  1.00000          1.00000 </span><br><span class="line"> 62   3.00000         osd.62              up  1.00000          1.00000 </span><br><span class="line"> 67   3.00000         osd.67              up  1.00000          1.00000 </span><br><span class="line"> 72   3.00000         osd.72              up  1.00000          1.00000 </span><br><span class="line"> 77   3.00000         osd.77              up  1.00000          1.00000 </span><br><span class="line"> 82   3.00000         osd.82              up  1.00000          1.00000 </span><br><span class="line"> 87   3.00000         osd.87              up  1.00000          1.00000 </span><br><span class="line"> -9  27.00000     host 10.254.253.9                                    </span><br><span class="line"> 48   3.00000         osd.48              up  1.00000          1.00000 </span><br><span class="line"> 53   3.00000         osd.53              up  1.00000          1.00000 </span><br><span class="line"> 58   3.00000         osd.58              up  1.00000          1.00000 </span><br><span class="line"> 63   3.00000         osd.63              up  1.00000          1.00000 </span><br><span class="line"> 68   3.00000         osd.68              up  1.00000          1.00000 </span><br><span class="line"> 73   3.00000         osd.73              up  1.00000          1.00000 </span><br><span class="line"> 78   3.00000         osd.78              up  1.00000          1.00000 </span><br><span class="line"> 83   3.00000         osd.83              up  1.00000          1.00000 </span><br><span class="line"> 88   3.00000         osd.88              up  1.00000          1.00000 </span><br><span class="line">-10  27.00000     host 10.254.253.10                                   </span><br><span class="line"> 49   3.00000         osd.49              up  1.00000          1.00000 </span><br><span class="line"> 54   3.00000         osd.54              up  1.00000          1.00000 </span><br><span class="line"> 59   3.00000         osd.59              up  1.00000          1.00000 </span><br><span class="line"> 64   3.00000         osd.64              up  1.00000          1.00000 </span><br><span class="line"> 69   3.00000         osd.69              up  1.00000          1.00000 </span><br><span class="line"> 74   3.00000         osd.74              up  1.00000          1.00000 </span><br><span class="line"> 79   3.00000         osd.79              up  1.00000          1.00000 </span><br><span class="line"> 84   3.00000         osd.84              up  1.00000          1.00000 </span><br><span class="line"> 89   3.00000         osd.89              up  1.00000          1.00000 </span><br><span class="line">-11  27.00000     host 10.254.253.6                                    </span><br><span class="line"> 50   3.00000         osd.50              up  1.00000          1.00000 </span><br><span class="line"> 55   3.00000         osd.55              up  1.00000          1.00000 </span><br><span class="line"> 60   3.00000         osd.60              up  1.00000          1.00000 </span><br><span class="line"> 65   3.00000         osd.65              up  1.00000          1.00000 </span><br><span class="line"> 70   3.00000         osd.70              up  1.00000          1.00000 </span><br><span class="line"> 75   3.00000         osd.75              up  1.00000          1.00000 </span><br><span class="line"> 80   3.00000         osd.80              up  1.00000          1.00000 </span><br><span class="line"> 85   3.00000         osd.85              up  1.00000          1.00000 </span><br><span class="line"> 90   3.00000         osd.90              up  1.00000          1.00000 </span><br></pre></td></tr></table></figure><h3 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法"></a>解决办法</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@control01 ~]# docker restart ceph_osd_5</span><br><span class="line">ceph_osd_5</span><br><span class="line"></span><br><span class="line">[root@control01 ~]# docker ps|grep ceph_osd_5</span><br><span class="line">9c4fe5eb0090        10.254.254.1:4000&#x2F;99cloud&#x2F;centos-source-ceph-osd:animbus-5.4.0                    &quot;kolla_start&quot;            3 weeks ago         Up 13 seconds </span><br></pre></td></tr></table></figure><p>系统会对该 osd 执行 recovery 操作, recovery 过程中, 会断开 block request, 那么这个 request 将会重新请求 mon 节点, 并重新获得新的 pg map, 得到最新的数据访问位置, 从而解决上述问题。</p><h3 id="查看集群状态"><a href="#查看集群状态" class="headerlink" title="查看集群状态"></a>查看集群状态</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">(ceph-mon)[root@control01 &#x2F;]# ceph -s</span><br><span class="line">    cluster b233a0b7-4e21-4375-bca8-e215c056cc25</span><br><span class="line">     health HEALTH_OK</span><br><span class="line">     monmap e1: 3 mons at &#123;10.254.253.1&#x3D;10.254.253.1:6789&#x2F;0,10.254.253.2&#x3D;10.254.253.2:6789&#x2F;0,10.254.253.3&#x3D;10.254.253.3:6789&#x2F;0&#125;</span><br><span class="line">            election epoch 26, quorum 0,1,2 10.254.253.1,10.254.253.2,10.254.253.3</span><br><span class="line">     osdmap e387: 90 osds: 90 up, 90 in</span><br><span class="line">            flags sortbitwise,require_jewel_osds</span><br><span class="line">      pgmap v1730238: 1008 pgs, 11 pools, 3498 GB data, 886 kobjects</span><br><span class="line">            10453 GB used, 235 TB &#x2F; 245 TB avail</span><br><span class="line">                1006 active+clean</span><br><span class="line">                   2 active+clean+scrubbing+deep</span><br><span class="line">  client io 1090 kB&#x2F;s rd, 92507 kB&#x2F;s wr, 778 op&#x2F;s rd, 904 op&#x2F;s wr</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;ceph-requests-are-blocked-异常解决方法&quot;&gt;&lt;a href=&quot;#ceph-requests-are-blocked-异常解决方法&quot; class=&quot;headerlink&quot; title=&quot;ceph ( requests are blocked ) 异常解决方法&quot;&gt;&lt;/a&gt;ceph ( requests are blocked ) 异常解决方法&lt;/h3&gt;&lt;p&gt;&lt;img src=&quot;https://ljw.howieli.cn/blog/2018-1-8/%E4%BF%AE%E7%90%86%E5%B7%A5.png&quot;&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="ceph" scheme="https://www.lijiawang.org/categories/ceph/"/>
    
    
    <category term="ceph" scheme="https://www.lijiawang.org/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title>shell实现局域网IP扫描</title>
    <link href="https://www.lijiawang.org/%E7%AE%80%E5%8D%95shell%E5%AE%9E%E7%8E%B0%E5%B1%80%E5%9F%9F%E7%BD%91IP%E6%89%AB%E6%8F%8F"/>
    <id>https://www.lijiawang.org/%E7%AE%80%E5%8D%95shell%E5%AE%9E%E7%8E%B0%E5%B1%80%E5%9F%9F%E7%BD%91IP%E6%89%AB%E6%8F%8F</id>
    <published>2017-12-29T03:23:06.000Z</published>
    <updated>2018-01-10T06:05:41.874Z</updated>
    
    <content type="html"><![CDATA[<h3 id="简单shell实现局域网IP扫描"><a href="#简单shell实现局域网IP扫描" class="headerlink" title="简单shell实现局域网IP扫描"></a>简单shell实现局域网IP扫描</h3><p>简单shell实现局域网IP扫描<br>局域网主机联通性的扫描</p><a id="more"></a><h3 id="脚本"><a href="#脚本" class="headerlink" title="脚本"></a>脚本</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">#简单shell实现局域网IP扫描</span><br><span class="line">#Functions: 局域网主机联通性的扫描</span><br><span class="line">#Author: ljw</span><br><span class="line"></span><br><span class="line">network&#x3D;$1</span><br><span class="line">time&#x3D;$(date +%H%M%S)</span><br><span class="line"></span><br><span class="line">for i in $(seq $2 $3)</span><br><span class="line">do</span><br><span class="line">    ping -c 1 -w 2 $network.$i &gt; &#x2F;dev&#x2F;null</span><br><span class="line">    if [ $? -eq 0 ]; then</span><br><span class="line">          arp $network.$i | grep &quot;:&quot; | awk &#39;&#123;print $1,$3&#125;&#39; &gt;&gt; $time.log</span><br><span class="line">          echo &quot;host $network.$i is up&quot;</span><br><span class="line">   else</span><br><span class="line">          echo &quot;host $network.$i is down&quot;</span><br><span class="line">   fi</span><br><span class="line">done</span><br></pre></td></tr></table></figure><h3 id="执行"><a href="#执行" class="headerlink" title="执行"></a>执行</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">.&#x2F;netscan.sh 172.18.22 100 130</span><br></pre></td></tr></table></figure><h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># sh netscan.sh 172.18.22 100 130</span><br><span class="line">host 172.18.22.100 is up</span><br><span class="line">host 172.18.22.101 is up</span><br><span class="line">host 172.18.22.102 is up</span><br><span class="line">host 172.18.22.103 is up</span><br><span class="line">host 172.18.22.104 is down</span><br><span class="line">host 172.18.22.105 is down</span><br><span class="line">host 172.18.22.106 is down</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;简单shell实现局域网IP扫描&quot;&gt;&lt;a href=&quot;#简单shell实现局域网IP扫描&quot; class=&quot;headerlink&quot; title=&quot;简单shell实现局域网IP扫描&quot;&gt;&lt;/a&gt;简单shell实现局域网IP扫描&lt;/h3&gt;&lt;p&gt;简单shell实现局域网IP扫描&lt;br&gt;局域网主机联通性的扫描&lt;/p&gt;</summary>
    
    
    
    <category term="shell" scheme="https://www.lijiawang.org/categories/shell/"/>
    
    
    <category term="shell" scheme="https://www.lijiawang.org/tags/shell/"/>
    
  </entry>
  
  <entry>
    <title>在kolla中配置cinder ceph多后端</title>
    <link href="https://www.lijiawang.org/%E5%9C%A8kolla%E4%B8%AD%E9%85%8D%E7%BD%AEcinder%20ceph%E5%A4%9A%E5%90%8E%E7%AB%AF"/>
    <id>https://www.lijiawang.org/%E5%9C%A8kolla%E4%B8%AD%E9%85%8D%E7%BD%AEcinder%20ceph%E5%A4%9A%E5%90%8E%E7%AB%AF</id>
    <published>2017-11-30T03:10:44.000Z</published>
    <updated>2018-01-10T06:05:03.643Z</updated>
    
    <content type="html"><![CDATA[<h3 id="利用kolla中配置cinder-ceph多后端"><a href="#利用kolla中配置cinder-ceph多后端" class="headerlink" title="利用kolla中配置cinder ceph多后端"></a>利用kolla中配置cinder ceph多后端</h3><p>我一台有2块硬盘，1块ssd（性能盘）和1块sata（容量盘），共有3台同样的服务器（控制计算存储融合）<br>/dev/sdb  ssd<br>/dev/sdc  sata<br><img src="https://ljw.howieli.cn/blog/2017-11-30/ceph.png"></p><a id="more"></a><h3 id="配置globals-yml-文件"><a href="#配置globals-yml-文件" class="headerlink" title="配置globals.yml 文件"></a>配置globals.yml 文件</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ceph_enable_cache: &quot;yes&quot;</span><br><span class="line">enable_ceph: &quot;yes&quot;</span><br><span class="line">enable_cinder: &quot;yes&quot;</span><br><span class="line">cinder_backend_ceph: &quot;&#123;&#123; enable_ceph &#125;&#125;&quot;</span><br></pre></td></tr></table></figure><h3 id="修改cinder配置（代码修改都在部署节点）"><a href="#修改cinder配置（代码修改都在部署节点）" class="headerlink" title="修改cinder配置（代码修改都在部署节点）"></a>修改cinder配置（代码修改都在部署节点）</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> vim /etc/kolla/config/cinder.conf</span></span><br><span class="line">[DEFAULT]</span><br><span class="line">enabled_backends = rbd-1,ssd</span><br><span class="line">[ssd]</span><br><span class="line">volume_driver = cinder.volume.drivers.rbd.RBDDriver</span><br><span class="line">volume_backend_name = ssd</span><br><span class="line">rbd_pool = volumes-cache</span><br><span class="line">rbd_ceph_conf = /etc/ceph/ceph.conf</span><br><span class="line">rbd_flatten_volume_from_snapshot = false</span><br><span class="line">rbd_max_clone_depth = 5</span><br><span class="line">rbd_store_chunk_size = 4</span><br><span class="line">rados_connect_timeout = 5</span><br><span class="line">rbd_user = cinder</span><br><span class="line">rbd_secret_uuid = b89a2a40-c009-47da-ba5b-7b6414a1f759</span><br><span class="line"><span class="meta">#</span><span class="bash"> rbd_secret_uuid = xxxxxxxxxxxxxxxxxxxxxxxxxxx</span></span><br><span class="line">report_discard_supported = True</span><br><span class="line">image_upload_use_cinder_backend = True</span><br></pre></td></tr></table></figure><h3 id="给磁盘打标签"><a href="#给磁盘打标签" class="headerlink" title="给磁盘打标签"></a>给磁盘打标签</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">parted &#x2F;dev&#x2F;sdb -s -- mklabel gpt mkpart KOLLA_CEPH_OSD_BOOTSTRAP 1 -1</span><br><span class="line"></span><br><span class="line">parted &#x2F;dev&#x2F;sdc -s -- mklabel gpt mkpart KOLLA_CEPH_OSD_CACHE_BOOTSTRAP 1 -1</span><br></pre></td></tr></table></figure><h3 id="开始部署"><a href="#开始部署" class="headerlink" title="开始部署"></a>开始部署</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kolla-ansible deploy  -i /root/multinode</span><br></pre></td></tr></table></figure><h3 id="crushmap"><a href="#crushmap" class="headerlink" title="crushmap"></a>crushmap</h3><h3 id="获取crushmap"><a href="#获取crushmap" class="headerlink" title="获取crushmap"></a>获取crushmap</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> docker <span class="built_in">exec</span> -it ceph_mon bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> ceph osd getcrushmap -o crushmap.old</span></span><br></pre></td></tr></table></figure><h4 id="反编译：此后就可以编辑crushmap文件了"><a href="#反编译：此后就可以编辑crushmap文件了" class="headerlink" title="反编译：此后就可以编辑crushmap文件了"></a>反编译：此后就可以编辑crushmap文件了</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> crushtool -d crushmap.old -o crushmap.new</span></span><br></pre></td></tr></table></figure><h4 id="修改crushmap"><a href="#修改crushmap" class="headerlink" title="修改crushmap"></a>修改crushmap</h4><p>根据实际情况编写crushmap</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> vi crushmap.new</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> begin crush map</span></span><br><span class="line">tunable choose_local_tries 0</span><br><span class="line">tunable choose_local_fallback_tries 0</span><br><span class="line">tunable choose_total_tries 50</span><br><span class="line">tunable chooseleaf_descend_once 1</span><br><span class="line">tunable chooseleaf_vary_r 1</span><br><span class="line">tunable straw_calc_version 1</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> devices</span></span><br><span class="line">device 0 osd.0</span><br><span class="line">device 1 osd.1</span><br><span class="line">device 2 osd.2</span><br><span class="line">device 3 osd.3</span><br><span class="line">device 4 osd.4</span><br><span class="line">device 5 osd.5</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> types</span></span><br><span class="line">type 0 osd</span><br><span class="line">type 1 host</span><br><span class="line">type 2 chassis</span><br><span class="line">type 3 rack</span><br><span class="line">type 4 row</span><br><span class="line">type 5 pdu</span><br><span class="line">type 6 pod</span><br><span class="line">type 7 room</span><br><span class="line">type 8 datacenter</span><br><span class="line">type 9 region</span><br><span class="line">type 10 root</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> buckets</span></span><br><span class="line">host 1.1.1.163 &#123;</span><br><span class="line">        id -2           # do not change unnecessarily</span><br><span class="line">        # weight 0.990</span><br><span class="line">        alg straw</span><br><span class="line">        hash 0  # rjenkins1</span><br><span class="line">        item osd.0 weight 0.990</span><br><span class="line">&#125;</span><br><span class="line">host 1.1.1.161 &#123;</span><br><span class="line">        id -3           # do not change unnecessarily</span><br><span class="line">        # weight 0.990</span><br><span class="line">        alg straw</span><br><span class="line">        hash 0  # rjenkins1</span><br><span class="line">        item osd.1 weight 0.990</span><br><span class="line">&#125;</span><br><span class="line">host 1.1.1.162 &#123;</span><br><span class="line">        id -4           # do not change unnecessarily</span><br><span class="line">        # weight 0.990</span><br><span class="line">        alg straw</span><br><span class="line">        hash 0  # rjenkins1</span><br><span class="line">        item osd.2 weight 0.990</span><br><span class="line">&#125;</span><br><span class="line">root default &#123;</span><br><span class="line">        id -1           # do not change unnecessarily</span><br><span class="line">        # weight 2.970</span><br><span class="line">        alg straw</span><br><span class="line">        hash 0  # rjenkins1</span><br><span class="line">        item 1.1.1.163 weight 0.990</span><br><span class="line">        item 1.1.1.161 weight 0.990</span><br><span class="line">        item 1.1.1.162 weight 0.990</span><br><span class="line">&#125;</span><br><span class="line">host 1.1.1.163-ssd &#123;</span><br><span class="line">        id -5           # do not change unnecessarily</span><br><span class="line">        # weight 0.470</span><br><span class="line">        alg straw</span><br><span class="line">        hash 0  # rjenkins1</span><br><span class="line">        item osd.4 weight 0.470</span><br><span class="line">&#125;</span><br><span class="line">host 1.1.1.161-ssd &#123;</span><br><span class="line">        id -6           # do not change unnecessarily</span><br><span class="line">        # weight 0.470</span><br><span class="line">        alg straw</span><br><span class="line">        hash 0  # rjenkins1</span><br><span class="line">        item osd.3 weight 0.470</span><br><span class="line">&#125;</span><br><span class="line">host 1.1.1.162-ssd &#123;</span><br><span class="line">        id -8           # do not change unnecessarily</span><br><span class="line">        # weight 0.470</span><br><span class="line">        alg straw</span><br><span class="line">        hash 0  # rjenkins1</span><br><span class="line">        item osd.5 weight 0.470</span><br><span class="line">&#125;</span><br><span class="line">root ssd &#123;</span><br><span class="line">        id -7           # do not change unnecessarily</span><br><span class="line">        # weight 1.410</span><br><span class="line">        alg straw</span><br><span class="line">        hash 0  # rjenkins1</span><br><span class="line">        item 1.1.1.163-ssd weight 0.470</span><br><span class="line">        item 1.1.1.161-ssd weight 0.470</span><br><span class="line">        item 1.1.1.162-ssd weight 0.470</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> rules</span></span><br><span class="line">rule replicated_ruleset &#123;</span><br><span class="line">        ruleset 0</span><br><span class="line">        type replicated</span><br><span class="line">        min_size 1</span><br><span class="line">        max_size 10</span><br><span class="line">        step take default</span><br><span class="line">        step chooseleaf firstn 0 type host</span><br><span class="line">        step emit</span><br><span class="line">&#125;</span><br><span class="line">rule disks &#123;</span><br><span class="line">        ruleset 1</span><br><span class="line">        type replicated</span><br><span class="line">        min_size 1</span><br><span class="line">        max_size 10</span><br><span class="line">        step take default</span><br><span class="line">        step chooseleaf firstn 0 type host</span><br><span class="line">        step emit</span><br><span class="line">&#125;</span><br><span class="line">rule ssd_releset &#123;</span><br><span class="line">        ruleset 2</span><br><span class="line">        type replicated</span><br><span class="line">        min_size 1</span><br><span class="line">        max_size 10</span><br><span class="line">        step take ssd</span><br><span class="line">        step chooseleaf firstn 0 type host</span><br><span class="line">        step emit</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> end crush map</span></span><br></pre></td></tr></table></figure><h4 id="重新编译"><a href="#重新编译" class="headerlink" title="重新编译"></a>重新编译</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> crushtool -c crushmap.new -o crushmap.bin</span></span><br></pre></td></tr></table></figure><h4 id="导入新的crushmap"><a href="#导入新的crushmap" class="headerlink" title="导入新的crushmap"></a>导入新的crushmap</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> ceph osd setcrushmap -i crushmap.bin</span></span><br></pre></td></tr></table></figure><h4 id="查看"><a href="#查看" class="headerlink" title="查看"></a>查看</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># ceph osd tree</span><br><span class="line">ID WEIGHT  TYPE NAME              UP&#x2F;DOWN REWEIGHT PRIMARY-AFFINITY </span><br><span class="line">-7 1.40996 root ssd                                                 </span><br><span class="line">-5 0.46999     host 1.1.1.163-ssd                                   </span><br><span class="line"> 4 0.46999         osd.4               up  1.00000          1.00000 </span><br><span class="line">-6 0.46999     host 1.1.1.161-ssd                                   </span><br><span class="line"> 3 0.46999         osd.3               up  1.00000          1.00000 </span><br><span class="line">-8 0.46999     host 1.1.1.162-ssd                                   </span><br><span class="line"> 5 0.46999         osd.5               up  1.00000          1.00000 </span><br><span class="line">-1 2.96997 root default                                             </span><br><span class="line">-2 0.98999     host 1.1.1.163                                       </span><br><span class="line"> 0 0.98999         osd.0               up  1.00000          1.00000 </span><br><span class="line">-3 0.98999     host 1.1.1.161                                       </span><br><span class="line"> 1 0.98999         osd.1               up  1.00000          1.00000 </span><br><span class="line">-4 0.98999     host 1.1.1.162                                       </span><br><span class="line"> 2 0.98999         osd.2               up  1.00000          1.00000</span><br></pre></td></tr></table></figure><h3 id="创建pool"><a href="#创建pool" class="headerlink" title="创建pool"></a>创建pool</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> ceph osd pool create volumes-cache 64 64</span></span><br></pre></td></tr></table></figure><h3 id="设置pool的crushmap"><a href="#设置pool的crushmap" class="headerlink" title="设置pool的crushmap"></a>设置pool的crushmap</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> ceph osd pool <span class="built_in">set</span> volumes-cache crush_ruleset 2</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> ceph osd dump |grep -i pool</span></span><br><span class="line">pool 1 &#x27;images&#x27; replicated size 1 min_size 1 crush_ruleset 1 object_hash rjenkins pg_num 128 pgp_num 128 last_change 285 flags hashpspool stripe_width 0</span><br><span class="line">pool 2 &#x27;volumes&#x27; replicated size 1 min_size 1 crush_ruleset 1 object_hash rjenkins pg_num 128 pgp_num 128 last_change 281 flags hashpspool stripe_width 0</span><br><span class="line">pool 3 &#x27;backups&#x27; replicated size 1 min_size 1 crush_ruleset 1 object_hash rjenkins pg_num 128 pgp_num 128 last_change 165 flags hashpspool stripe_width 0</span><br><span class="line">pool 4 &#x27;vms&#x27; replicated size 1 min_size 1 crush_ruleset 1 object_hash rjenkins pg_num 128 pgp_num 128 last_change 44 flags hashpspool stripe_width 0</span><br><span class="line">pool 5 &#x27;gnocchi&#x27; replicated size 1 min_size 1 crush_ruleset 1 object_hash rjenkins pg_num 128 pgp_num 128 last_change 35 flags hashpspool stripe_width 0</span><br><span class="line">pool 6 &#x27;volumes-cache&#x27; replicated size 3 min_size 2 crush_ruleset 2 object_hash rjenkins pg_num 64 pgp_num 64 last_change 62 flags hashpspool stripe_width 0</span><br></pre></td></tr></table></figure><h3 id="创建两个cinder卷类型"><a href="#创建两个cinder卷类型" class="headerlink" title="创建两个cinder卷类型"></a>创建两个cinder卷类型</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">source</span> /etc/kolla/admin-openrc.sh</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> cinder type-create SATA</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> cinder type-create SSD</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> cinder type-list</span></span><br><span class="line">+--------------------------------------+------+-------------+-----------+</span><br><span class="line">| ID                                   | Name | Description | Is_Public |</span><br><span class="line">+--------------------------------------+------+-------------+-----------+</span><br><span class="line">| 8c1079e5-90a3-4f6d-bdb7-2f25b70bc2c8 | SSD  |             | True      |</span><br><span class="line">| a605c569-1e88-486d-bd8e-7aba43ce1ef2 | SATA |             | True      |</span><br><span class="line">+--------------------------------------+------+-------------+-----------+</span><br></pre></td></tr></table></figure><h3 id="设置卷类型的key键值"><a href="#设置卷类型的key键值" class="headerlink" title="设置卷类型的key键值"></a>设置卷类型的key键值</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">cinder type-key SSD set volume_backend_name=ssd</span><br><span class="line">cinder type-key SATA set volume_backend_name=rbd-1</span><br><span class="line"><span class="meta">#</span><span class="bash"> cinder  extra-specs-list</span></span><br><span class="line">+--------------------------------------+------+----------------------------------+</span><br><span class="line">| ID                                   | Name | extra_specs                      |</span><br><span class="line">+--------------------------------------+------+----------------------------------+</span><br><span class="line">| 8c1079e5-90a3-4f6d-bdb7-2f25b70bc2c8 | SSD  | &#123;&#x27;volume_backend_name&#x27;: &#x27;ssd&#x27;&#125;   |</span><br><span class="line">| a605c569-1e88-486d-bd8e-7aba43ce1ef2 | SATA | &#123;&#x27;volume_backend_name&#x27;: &#x27;rbd-1&#x27;&#125; |</span><br><span class="line">+--------------------------------------+------+----------------------------------+</span><br></pre></td></tr></table></figure><h3 id="创建云硬盘验证"><a href="#创建云硬盘验证" class="headerlink" title="创建云硬盘验证"></a>创建云硬盘验证</h3><p><img src="https://ljw.howieli.cn/blog/2017-11-30/%E5%88%9B%E5%BB%BA%E4%BA%91%E7%A1%AC%E7%9B%98.png"></p><h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> cinder create --volume_type SSD --display_name vol-ssd 1</span></span><br><span class="line">+--------------------------------+--------------------------------------+</span><br><span class="line">| Property                       | Value                                |</span><br><span class="line">+--------------------------------+--------------------------------------+</span><br><span class="line">| attachments                    | []                                   |</span><br><span class="line">| availability_zone              | nova                                 |</span><br><span class="line">| bootable                       | false                                |</span><br><span class="line">| consistencygroup_id            | None                                 |</span><br><span class="line">| created_at                     | 2017-11-30T02:58:17.000000           |</span><br><span class="line">| description                    | None                                 |</span><br><span class="line">| encrypted                      | False                                |</span><br><span class="line">| id                             | e2df2b47-13ec-4503-a78b-2ab60cb5a34b |</span><br><span class="line">| metadata                       | &#123;&#125;                                   |</span><br><span class="line">| migration_status               | None                                 |</span><br><span class="line">| multiattach                    | False                                |</span><br><span class="line">| name                           | vol-ssd                              |</span><br><span class="line">| os-vol-host-attr:host          | control@ssd#ssd                      |</span><br><span class="line">| os-vol-mig-status-attr:migstat | None                                 |</span><br><span class="line">| os-vol-mig-status-attr:name_id | None                                 |</span><br><span class="line">| os-vol-tenant-attr:tenant_id   | 21c161dda51147fb9ff527aadfe1d81a     |</span><br><span class="line">| replication_status             | None                                 |</span><br><span class="line">| size                           | 1                                    |</span><br><span class="line">| snapshot_id                    | None                                 |</span><br><span class="line">| source_volid                   | None                                 |</span><br><span class="line">| status                         | creating                             |</span><br><span class="line">| updated_at                     | 2017-11-30T02:58:18.000000           |</span><br><span class="line">| user_id                        | 68601348f1264a4cb69f8f8f162e3f2a     |</span><br><span class="line">| volume_type                    | SSD                                  |</span><br><span class="line">+--------------------------------+--------------------------------------+</span><br><span class="line"><span class="meta">#</span><span class="bash"> cinder create --volume_type SATA --display_name vol-sata 1</span></span><br><span class="line">+--------------------------------+--------------------------------------+</span><br><span class="line">| Property                       | Value                                |</span><br><span class="line">+--------------------------------+--------------------------------------+</span><br><span class="line">| attachments                    | []                                   |</span><br><span class="line">| availability_zone              | nova                                 |</span><br><span class="line">| bootable                       | false                                |</span><br><span class="line">| consistencygroup_id            | None                                 |</span><br><span class="line">| created_at                     | 2017-11-30T02:59:07.000000           |</span><br><span class="line">| description                    | None                                 |</span><br><span class="line">| encrypted                      | False                                |</span><br><span class="line">| id                             | f3f53733-348d-4ff9-a472-445aed77c111 |</span><br><span class="line">| metadata                       | &#123;&#125;                                   |</span><br><span class="line">| migration_status               | None                                 |</span><br><span class="line">| multiattach                    | False                                |</span><br><span class="line">| name                           | vol-sata                             |</span><br><span class="line">| os-vol-host-attr:host          | None                                 |</span><br><span class="line">| os-vol-mig-status-attr:migstat | None                                 |</span><br><span class="line">| os-vol-mig-status-attr:name_id | None                                 |</span><br><span class="line">| os-vol-tenant-attr:tenant_id   | 21c161dda51147fb9ff527aadfe1d81a     |</span><br><span class="line">| replication_status             | None                                 |</span><br><span class="line">| size                           | 1                                    |</span><br><span class="line">| snapshot_id                    | None                                 |</span><br><span class="line">| source_volid                   | None                                 |</span><br><span class="line">| status                         | creating                             |</span><br><span class="line">| updated_at                     | None                                 |</span><br><span class="line">| user_id                        | 68601348f1264a4cb69f8f8f162e3f2a     |</span><br><span class="line">| volume_type                    | SATA                                 |</span><br><span class="line">+--------------------------------+--------------------------------------+</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> rbd -p volumes-cache ls|grep e2df2b47-13ec-4503-a78b-2ab60cb5a34b</span></span><br><span class="line">volume-e2df2b47-13ec-4503-a78b-2ab60cb5a34b</span><br><span class="line"><span class="meta">#</span><span class="bash"> rbd -p volumes ls|grep f3f53733-348d-4ff9-a472-445aed77c111</span></span><br><span class="line">volume-f3f53733-348d-4ff9-a472-445aed77c111</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;利用kolla中配置cinder-ceph多后端&quot;&gt;&lt;a href=&quot;#利用kolla中配置cinder-ceph多后端&quot; class=&quot;headerlink&quot; title=&quot;利用kolla中配置cinder ceph多后端&quot;&gt;&lt;/a&gt;利用kolla中配置cinder ceph多后端&lt;/h3&gt;&lt;p&gt;我一台有2块硬盘，1块ssd（性能盘）和1块sata（容量盘），共有3台同样的服务器（控制计算存储融合）&lt;br&gt;/dev/sdb  ssd&lt;br&gt;/dev/sdc  sata&lt;br&gt;&lt;img src=&quot;https://ljw.howieli.cn/blog/2017-11-30/ceph.png&quot;&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="openstack" scheme="https://www.lijiawang.org/categories/openstack/"/>
    
    
    <category term="ceph cinder" scheme="https://www.lijiawang.org/tags/ceph-cinder/"/>
    
  </entry>
  
  <entry>
    <title>kolla部署swift</title>
    <link href="https://www.lijiawang.org/kolla%E9%83%A8%E7%BD%B2swift"/>
    <id>https://www.lijiawang.org/kolla%E9%83%A8%E7%BD%B2swift</id>
    <published>2017-11-29T07:12:57.000Z</published>
    <updated>2018-01-10T06:03:05.949Z</updated>
    
    <content type="html"><![CDATA[<h3 id="kolla部署swift"><a href="#kolla部署swift" class="headerlink" title="kolla部署swift"></a>kolla部署swift</h3><p>kolla实现对象存储方式有两种：</p><ul><li>利用ceph_rgw实现</li><li>部署swift实现</li></ul><p>本篇博客介绍利用kolla部署swift实现openstack对象存储</p><a id="more"></a><h3 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h3><p>我这是一个all-in-one的环境,利用kolla部署了一个pike版本,部署方法请参考<a href="https://www.lijiawang.org/posts/kolla-pike-on-centos.html#more">kolla部署openstack Pike版</a><br>我这台虚拟机一共有5块盘，分别为sda,sdb,sdc,sdd,sde。sda为系统盘，sdb,sdc,sdd,为ceph的osd用，sde用于做swift</p><h3 id="打标签"><a href="#打标签" class="headerlink" title="打标签"></a>打标签</h3><p>swift要求可用的块设备用来存储，所以我事先准备了一个硬盘<code>sde</code>作为存储设备,在部署需要给这块盘打一个特殊的标签。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">index=0</span><br><span class="line">for d in sde; do</span><br><span class="line"><span class="meta">#</span><span class="bash"> 因为我这就有一块用于swift存储的盘sde。</span></span><br><span class="line">    parted /dev/$&#123;d&#125; -s -- mklabel gpt mkpart KOLLA_SWIFT_DATA 1 -1</span><br><span class="line">    sudo mkfs.xfs -f -L d$&#123;index&#125; /dev/$&#123;d&#125;1</span><br><span class="line">    (( index++ ))</span><br><span class="line">done</span><br></pre></td></tr></table></figure><h3 id="Rings"><a href="#Rings" class="headerlink" title="Rings"></a>Rings</h3><p>kolla在部署swift前，需要生成rings，这是一种二进制压缩文件，在较高的级别上，让各种快速服务知道集群中的数据在哪里，这里官方给了一个脚本，简单的配置一下即可。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> STORAGE_NODES=(192.168.0.2 192.168.0.3 192.168.0.4)</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 这里是swift走的存储网络的ip，因为我是all-in-one环境，所以IP只有一个，这里我的swift存储用网将和管理网在一起</span></span><br><span class="line">STORAGE_NODES=(192.168.10.139)</span><br><span class="line"><span class="meta">#</span><span class="bash"> KOLLA_SWIFT_BASE_IMAGE=<span class="string">&quot;docker_registry/docker_namespace/centos-source-swift-base:openstack_release&quot;</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 这里按照需求填写</span></span><br><span class="line">KOLLA_SWIFT_BASE_IMAGE=&quot;192.168.10.139:4000/lokolla/centos-source-swift-base:5.0.1&quot;</span><br><span class="line"></span><br><span class="line">mkdir -p /etc/kolla/config/swift</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Object ring</span></span><br><span class="line">docker run \</span><br><span class="line">  --rm \</span><br><span class="line">  -v /etc/kolla/config/swift/:/etc/kolla/config/swift/ \</span><br><span class="line"><span class="meta">  $</span><span class="bash">KOLLA_SWIFT_BASE_IMAGE \</span></span><br><span class="line"><span class="bash">  swift-ring-builder \</span></span><br><span class="line"><span class="bash">    /etc/kolla/config/swift/object.builder create 10 3 1</span></span><br><span class="line"></span><br><span class="line">for node in $&#123;STORAGE_NODES[@]&#125;; do</span><br><span class="line">    for i in &#123;0..2&#125;; do</span><br><span class="line">      docker run \</span><br><span class="line">        --rm \</span><br><span class="line">        -v /etc/kolla/config/swift/:/etc/kolla/config/swift/ \</span><br><span class="line">        $KOLLA_SWIFT_BASE_IMAGE \</span><br><span class="line">        swift-ring-builder \</span><br><span class="line">          /etc/kolla/config/swift/object.builder add r1z1-$&#123;node&#125;:6000/d$&#123;i&#125; 1;</span><br><span class="line">    done</span><br><span class="line">done</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Account ring</span></span><br><span class="line">docker run \</span><br><span class="line">  --rm \</span><br><span class="line">  -v /etc/kolla/config/swift/:/etc/kolla/config/swift/ \</span><br><span class="line"><span class="meta">  $</span><span class="bash">KOLLA_SWIFT_BASE_IMAGE \</span></span><br><span class="line"><span class="bash">  swift-ring-builder \</span></span><br><span class="line"><span class="bash">    /etc/kolla/config/swift/account.builder create 10 3 1</span></span><br><span class="line"></span><br><span class="line">for node in $&#123;STORAGE_NODES[@]&#125;; do</span><br><span class="line">    for i in &#123;0..2&#125;; do</span><br><span class="line">      docker run \</span><br><span class="line">        --rm \</span><br><span class="line">        -v /etc/kolla/config/swift/:/etc/kolla/config/swift/ \</span><br><span class="line">        $KOLLA_SWIFT_BASE_IMAGE \</span><br><span class="line">        swift-ring-builder \</span><br><span class="line">          /etc/kolla/config/swift/account.builder add r1z1-$&#123;node&#125;:6001/d$&#123;i&#125; 1;</span><br><span class="line">    done</span><br><span class="line">done</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Container ring</span></span><br><span class="line">docker run \</span><br><span class="line">  --rm \</span><br><span class="line">  -v /etc/kolla/config/swift/:/etc/kolla/config/swift/ \</span><br><span class="line"><span class="meta">  $</span><span class="bash">KOLLA_SWIFT_BASE_IMAGE \</span></span><br><span class="line"><span class="bash">  swift-ring-builder \</span></span><br><span class="line"><span class="bash">    /etc/kolla/config/swift/container.builder create 10 3 1</span></span><br><span class="line"></span><br><span class="line">for node in $&#123;STORAGE_NODES[@]&#125;; do</span><br><span class="line">    for i in &#123;0..2&#125;; do</span><br><span class="line">      docker run \</span><br><span class="line">        --rm \</span><br><span class="line">        -v /etc/kolla/config/swift/:/etc/kolla/config/swift/ \</span><br><span class="line">        $KOLLA_SWIFT_BASE_IMAGE \</span><br><span class="line">        swift-ring-builder \</span><br><span class="line">          /etc/kolla/config/swift/container.builder add r1z1-$&#123;node&#125;:6002/d$&#123;i&#125; 1;</span><br><span class="line">    done</span><br><span class="line">done</span><br><span class="line"></span><br><span class="line">for ring in object account container; do</span><br><span class="line">  docker run \</span><br><span class="line">    --rm \</span><br><span class="line">    -v /etc/kolla/config/swift/:/etc/kolla/config/swift/ \</span><br><span class="line">    $KOLLA_SWIFT_BASE_IMAGE \</span><br><span class="line">    swift-ring-builder \</span><br><span class="line">      /etc/kolla/config/swift/$&#123;ring&#125;.builder rebalance;</span><br><span class="line">done</span><br></pre></td></tr></table></figure><h3 id="globals-yml文件"><a href="#globals-yml文件" class="headerlink" title="globals.yml文件"></a>globals.yml文件</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> vim /etc/kolla/globals.yml</span></span><br><span class="line"><span class="meta">#</span><span class="bash">enable_ceph_rgw: <span class="string">&quot;no&quot;</span></span></span><br><span class="line">enable_swift: &quot;yes&quot;</span><br><span class="line"><span class="meta">#</span><span class="bash">enable_ceph_rgw_keystone: <span class="string">&quot;no&quot;</span></span></span><br></pre></td></tr></table></figure><p>这里需要说下，ceph_rgw实现的对象存储和swift实现的对象存储二者不能共存。如果要想用ceph_rgw实现以上步骤均不要做直接修改globals文件后deploy即可</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> vim /etc/kolla/globals.yml</span></span><br><span class="line">enable_ceph_rgw: &quot;yes&quot;</span><br><span class="line"><span class="meta">#</span><span class="bash"> enable_swift: <span class="string">&quot;no&quot;</span></span></span><br><span class="line">enable_ceph_rgw_keystone: &quot;yes&quot;</span><br></pre></td></tr></table></figure><h3 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> kolla-ansible deploy</span></span><br></pre></td></tr></table></figure><h3 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">source</span> /etc/kolla/admin-openrc.sh</span> </span><br><span class="line"><span class="meta">#</span><span class="bash"> openstack container create mycontainer</span></span><br><span class="line">+---------------------------------------+-------------+------------------------------------+</span><br><span class="line">| account                               | container   | x-trans-id                         |</span><br><span class="line">+---------------------------------------+-------------+------------------------------------+</span><br><span class="line">| AUTH_2a568be969b34c288ef52a2987973145 | mycontainer | txc68d723c0c934a7282cfe-005a1e5d11 |</span><br><span class="line">+---------------------------------------+-------------+------------------------------------+</span><br><span class="line">[root@pike ~]# openstack object create mycontainer README.rst</span><br><span class="line">+------------+-------------+----------------------------------+</span><br><span class="line">| object     | container   | etag                             |</span><br><span class="line">+------------+-------------+----------------------------------+</span><br><span class="line">| README.rst | mycontainer | a2f288979d0af55c5e9313f525c75615 |</span><br><span class="line">+------------+-------------+----------------------------------+</span><br><span class="line"><span class="meta">#</span><span class="bash"> openstack container show mycontainer</span></span><br><span class="line">+--------------+---------------------------------------+</span><br><span class="line">| Field        | Value                                 |</span><br><span class="line">+--------------+---------------------------------------+</span><br><span class="line">| account      | AUTH_2a568be969b34c288ef52a2987973145 |</span><br><span class="line">| bytes_used   | 39                                    |</span><br><span class="line">| container    | mycontainer                           |</span><br><span class="line">| object_count | 1                                     |</span><br><span class="line">+--------------+---------------------------------------+</span><br><span class="line"><span class="meta">#</span><span class="bash"> openstack object store account show</span></span><br><span class="line">+------------+---------------------------------------+</span><br><span class="line">| Field      | Value                                 |</span><br><span class="line">+------------+---------------------------------------+</span><br><span class="line">| Account    | AUTH_2a568be969b34c288ef52a2987973145 |</span><br><span class="line">| Bytes      | 0                                     |</span><br><span class="line">| Containers | 1                                     |</span><br><span class="line">| Objects    | 0                                     |</span><br><span class="line">+------------+---------------------------------------+</span><br></pre></td></tr></table></figure><p><img src="https://ljw.howieli.cn/blog/2017-11-29/swift.png"></p><h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h3><p><a href="https://docs.openstack.org/kolla-ansible/latest/reference/swift-guide.html#rings">https://docs.openstack.org/kolla-ansible/latest/reference/swift-guide.html#rings</a></p>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;kolla部署swift&quot;&gt;&lt;a href=&quot;#kolla部署swift&quot; class=&quot;headerlink&quot; title=&quot;kolla部署swift&quot;&gt;&lt;/a&gt;kolla部署swift&lt;/h3&gt;&lt;p&gt;kolla实现对象存储方式有两种：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;利用ceph_rgw实现&lt;/li&gt;
&lt;li&gt;部署swift实现&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;本篇博客介绍利用kolla部署swift实现openstack对象存储&lt;/p&gt;</summary>
    
    
    
    <category term="openstack" scheme="https://www.lijiawang.org/categories/openstack/"/>
    
    
    <category term="swift" scheme="https://www.lijiawang.org/tags/swift/"/>
    
  </entry>
  
  <entry>
    <title>openstack虚拟机VIP配置步骤</title>
    <link href="https://www.lijiawang.org/%E5%9C%A8openstack%E4%B8%8A%E7%9A%84%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%BB%91%E5%AE%9Avip"/>
    <id>https://www.lijiawang.org/%E5%9C%A8openstack%E4%B8%8A%E7%9A%84%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%BB%91%E5%AE%9Avip</id>
    <published>2017-11-28T02:57:55.000Z</published>
    <updated>2018-01-10T06:08:52.021Z</updated>
    
    <content type="html"><![CDATA[<h3 id="在openstack上的虚拟机绑定vip"><a href="#在openstack上的虚拟机绑定vip" class="headerlink" title="在openstack上的虚拟机绑定vip"></a>在openstack上的虚拟机绑定vip</h3><p>有些情况下，客户想在openstack的虚拟机上配置vip搭建高可用集群，下面我就简单的说下在openstack上的虚拟机如何绑定vip</p><a id="more"></a><h3 id="操作步骤"><a href="#操作步骤" class="headerlink" title="操作步骤"></a>操作步骤</h3><p>1、导入环境变量</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">source</span> admin-openrc.sh</span></span><br></pre></td></tr></table></figure><p>2、执行命令neutron net-list查看网络，找到自己需要设置的网络，获取subnet_id和network_id</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> neutron net-list</span></span><br><span class="line">+--------------------------------------+----------------------------------------------------+----------------------------------+-------------------------------------------------------+</span><br><span class="line">| id                                   | name                                               | tenant_id                        | subnets                                               |</span><br><span class="line">+--------------------------------------+----------------------------------------------------+----------------------------------+-------------------------------------------------------+</span><br><span class="line">| 32482d56-bb40-4b7f-85df-3be3a460e441 | HA network tenant 7ba30c1e519d4d6eb8f1ace2cfbf30d3 |                                  | 860bf95f-4775-4fac-af88-db392f254416 169.254.192.0/18 |</span><br><span class="line">| 7cc26554-2795-4a53-b053-34ec1b4c90f2 | web                                                | 7ba30c1e519d4d6eb8f1ace2cfbf30d3 | 4b1f707b-8842-4ce0-acba-4f0de304459b 192.168.1.0/24   |</span><br><span class="line">| d0ad534f-1bcd-43b0-aa0c-edee32520020 | public                                             | 21c161dda51147fb9ff527aadfe1d81a | 9a7f07e5-e906-4622-8bc6-def64b3622ec 172.18.23.0/24   |</span><br><span class="line">+--------------------------------------+----------------------------------------------------+----------------------------------+-------------------------------------------------------+</span><br></pre></td></tr></table></figure><p><img src="https://ljw.howieli.cn/blog/2017-11-28/net-list.png"><br>3、创建port来占用ip，保证neutron不会将此IP在分配出去，导致IP冲突问题。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">neutron port-create --fixed-ip subnet_id=&lt;subnet_id&gt;,ip_address=&lt;vip&gt; &lt;network_id&gt;</span><br><span class="line">注：</span><br><span class="line">替换subnet_id为neutron net-list中查看到的subnet_id</span><br><span class="line">替换vip为需要配置的vip地址</span><br><span class="line">替换network_ID为neutron net-list中查看到的network_id</span><br></pre></td></tr></table></figure><p>具体命令如下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> neutron port-create --fixed-ip subnet_id=9a7f07e5-e906-4622-8bc6-def64b3622ec,ip_address=172.18.23.10 d0ad534f-1bcd-43b0-aa0c-edee32520020</span></span><br><span class="line">Created a new port:</span><br><span class="line">+-----------------------+-------------------------------------------------------------------------------------+</span><br><span class="line">| Field                 | Value                                                                               |</span><br><span class="line">+-----------------------+-------------------------------------------------------------------------------------+</span><br><span class="line">| admin_state_up        | True                                                                                |</span><br><span class="line">| allowed_address_pairs |                                                                                     |</span><br><span class="line">| binding:host_id       |                                                                                     |</span><br><span class="line">| binding:profile       | &#123;&#125;                                                                                  |</span><br><span class="line">| binding:vif_details   | &#123;&#125;                                                                                  |</span><br><span class="line">| binding:vif_type      | unbound                                                                             |</span><br><span class="line">| binding:vnic_type     | normal                                                                              |</span><br><span class="line">| created_at            | 2017-11-28T02:35:17Z                                                                |</span><br><span class="line">| description           |                                                                                     |</span><br><span class="line">| device_id             |                                                                                     |</span><br><span class="line">| device_owner          |                                                                                     |</span><br><span class="line">| extra_dhcp_opts       |                                                                                     |</span><br><span class="line">| fixed_ips             | &#123;&quot;subnet_id&quot;: &quot;9a7f07e5-e906-4622-8bc6-def64b3622ec&quot;, &quot;ip_address&quot;: &quot;172.18.23.10&quot;&#125; |</span><br><span class="line">| id                    | 7c7ccc26-9ac9-4ef7-8178-2b97218b1d63                                                |</span><br><span class="line">| mac_address           | fa:16:3e:ea:81:a6                                                                   |</span><br><span class="line">| name                  |                                                                                     |</span><br><span class="line">| network_id            | d0ad534f-1bcd-43b0-aa0c-edee32520020                                                |</span><br><span class="line">| port_security_enabled | True                                                                                |</span><br><span class="line">| project_id            | 21c161dda51147fb9ff527aadfe1d81a                                                    |</span><br><span class="line">| revision_number       | 5                                                                                   |</span><br><span class="line">| security_groups       | abfba384-55f2-4eed-902a-712369be9604                                                |</span><br><span class="line">| status                | DOWN                                                                                |</span><br><span class="line">| tags                  |                                                                                     |</span><br><span class="line">| tenant_id             | 21c161dda51147fb9ff527aadfe1d81a                                                    |</span><br><span class="line">| updated_at            | 2017-11-28T02:35:18Z                                                                |</span><br><span class="line">+-----------------------+-------------------------------------------------------------------------------------+</span><br></pre></td></tr></table></figure><p><img src="https://ljw.howieli.cn/blog/2017-11-28/port-create.png"><br>4、执行命令<code>neutron port-list</code>查看端口，找到VIP的Port ID以及需要使用VIP的虚拟机的IP对应的Port id<br>比如两台虚拟机做HA绑定vip，那么需要查看两台虚拟机的port ID和这个vip的port ID</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> neutron port-list|grep 172.18.23.10</span></span><br><span class="line">| 7c7ccc26-9ac9-4ef7-8178-2b97218b1d63 |                                                 | 21c161dda51147fb9ff527aadfe1d81a | fa:16:3e:ea:81:a6 | &#123;&quot;subnet_id&quot;: &quot;9a7f07e5-e906-4622-8bc6-def64b3622ec&quot;, &quot;ip_address&quot;: &quot;172.18.23.10&quot;&#125;  |</span><br></pre></td></tr></table></figure><p>可以看出vip<code>172.18.23.10</code>的port id为<code>7c7ccc26-9ac9-4ef7-8178-2b97218b1d63</code>.<br>5、取消安全组对应端口的管理</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">neutron port-update --no-security-groups &lt;Port_id&gt;</span><br><span class="line">neutron port-update --port_security_enabled=false &lt;Port_id&gt;</span><br><span class="line">注：</span><br><span class="line">    替换Port_id为之前neutron port-list中找到的Port_id</span><br></pre></td></tr></table></figure><p>具有命令如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> neutron port-update --no-security-groups 7c7ccc26-9ac9-4ef7-8178-2b97218b1d63</span></span><br><span class="line">Updated port: 7c7ccc26-9ac9-4ef7-8178-2b97218b1d63</span><br><span class="line"><span class="meta">#</span><span class="bash"> neutron port-update --port_security_enabled=<span class="literal">false</span> 7c7ccc26-9ac9-4ef7-8178-2b97218b1d63</span></span><br><span class="line">Updated port: 7c7ccc26-9ac9-4ef7-8178-2b97218b1d63</span><br></pre></td></tr></table></figure><p>6、此时执行命令neutron port-show <Port_id><br><img src="https://ljw.howieli.cn/blog/2017-11-28/port-show.png"><br>可看到port_security_enabled的value为False，security_groups的value为空，即OK，这样两个端口就没有了安全组了。<br>7、意思就是对VIP和需要使用VIP的虚拟机都执行4、5、6步，比如配置HA，VIP+两台虚拟机，总共3个Port，都需要执行4、5、6步<br>然后就可以在这两台虚拟机上搭建keepalived集群使用<code>172.18.23.10</code>这个vip了。</p>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;在openstack上的虚拟机绑定vip&quot;&gt;&lt;a href=&quot;#在openstack上的虚拟机绑定vip&quot; class=&quot;headerlink&quot; title=&quot;在openstack上的虚拟机绑定vip&quot;&gt;&lt;/a&gt;在openstack上的虚拟机绑定vip&lt;/h3&gt;&lt;p&gt;有些情况下，客户想在openstack的虚拟机上配置vip搭建高可用集群，下面我就简单的说下在openstack上的虚拟机如何绑定vip&lt;/p&gt;</summary>
    
    
    
    <category term="openstack" scheme="https://www.lijiawang.org/categories/openstack/"/>
    
    
    <category term="openstack" scheme="https://www.lijiawang.org/tags/openstack/"/>
    
  </entry>
  
  <entry>
    <title>kolla Pike on CentOS 7.4</title>
    <link href="https://www.lijiawang.org/kolla-pike-on-centos"/>
    <id>https://www.lijiawang.org/kolla-pike-on-centos</id>
    <published>2017-11-22T03:42:06.000Z</published>
    <updated>2018-01-10T06:09:26.627Z</updated>
    
    <content type="html"><![CDATA[<h3 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h3><p>我这里用workstation 创建了一个虚拟机，安装centos7.4系统，这台虚拟机上有两张网卡，一张做openstack管理网，一张做为虚拟机的业务网卡。</p><a id="more"></a><h3 id="确认环境信息"><a href="#确认环境信息" class="headerlink" title="确认环境信息"></a>确认环境信息</h3><p>1.确认系统版本</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> cat /etc/redhat-release</span></span><br><span class="line">CentOS Linux release 7.4.1708 (Core)</span><br></pre></td></tr></table></figure><p>2.确认网卡个数和状态</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> ip a</span></span><br><span class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1</span><br><span class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">    inet 127.0.0.1/8 scope host lo</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 ::1/128 scope host</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">2: ens33: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000</span><br><span class="line">    link/ether 00:0c:29:a7:8c:91 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 192.168.10.139/24 brd 192.168.10.255 scope global dynamic ens33</span><br><span class="line">       valid_lft 1555sec preferred_lft 1555sec</span><br><span class="line">    inet6 fe80::5057:38c5:6b65:b5/64 scope link</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">3: ens34: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000</span><br><span class="line">    link/ether 00:0c:29:a7:8c:9b brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet6 fe80::20c:29ff:fea7:8c9b/64 scope link</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br></pre></td></tr></table></figure><p>上面可以看出有两张网卡<code>ens33</code>和<code>ens34</code>，这里我用<code>ens33</code>做管理网，<code>ens34</code>做业务网，这里不需要配置ip，把<code>ens34</code>网卡up起来就好。</p><h3 id="环境初始化"><a href="#环境初始化" class="headerlink" title="环境初始化"></a>环境初始化</h3><p>1.更改主机名</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> hostnamectl set-hostname pike</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> hostname pike</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> hostname</span></span><br><span class="line">pike</span><br></pre></td></tr></table></figure><p>2.关闭NetworkManager,firewalld,selinux</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> systemctl stop NetworkManager</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> systemctl <span class="built_in">disable</span> NetworkManager</span></span><br><span class="line">Removed symlink /etc/systemd/system/multi-user.target.wants/NetworkManager.service.</span><br><span class="line">Removed symlink /etc/systemd/system/dbus-org.freedesktop.NetworkManager.service.</span><br><span class="line">Removed symlink /etc/systemd/system/dbus-org.freedesktop.nm-dispatcher.service.</span><br><span class="line"><span class="meta">#</span><span class="bash"> systemctl stop firewalld</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> systemctl <span class="built_in">disable</span> firewalld</span></span><br><span class="line">Removed symlink /etc/systemd/system/multi-user.target.wants/firewalld.service.</span><br><span class="line">Removed symlink /etc/systemd/system/dbus-org.fedoraproject.FirewallD1.service.</span><br><span class="line"><span class="meta">#</span><span class="bash"> sed -i <span class="string">&quot;s/SELINUX=enforcing/SELINUX=disabled/&quot;</span> /etc/selinux/config</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> setenforce 0</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> getenforce</span></span><br><span class="line">Permissive</span><br></pre></td></tr></table></figure><p>3.查看是否开启了虚拟化</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> egrep <span class="string">&quot;vmx|svm&quot;</span> /proc/cpuinfo</span></span><br></pre></td></tr></table></figure><h3 id="安装基础包"><a href="#安装基础包" class="headerlink" title="安装基础包"></a>安装基础包</h3><p>1.配置epel源安装基础包</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># yum install epel-release</span><br><span class="line"># yum install axel vim git curl wget lrzsz gcc  python-devel yum* python-pip</span><br></pre></td></tr></table></figure><p>2.配置docker-ce的yum源安装docker-ce,这里我用的docker版本为docker-ce17.09</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> yum install -y docker-ce</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> docker -v</span></span><br><span class="line">Docker version 17.09.0-ce, build afdb6d4</span><br></pre></td></tr></table></figure><p>3.配置docker</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> mkdir /etc/systemd/system/docker.service.d</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> tee /etc/systemd/system/docker.service.d/kolla.conf &lt;&lt; <span class="string">&#x27;EOF&#x27;</span></span></span><br><span class="line">[Service]</span><br><span class="line">MountFlags=shared</span><br><span class="line">EOF</span><br><span class="line"><span class="meta">#</span><span class="bash"> vim /usr/lib/systemd/system/docker.service</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> ExecStart=/usr/bin/dockerd</span></span><br><span class="line">ExecStart=/usr/bin/dockerd --insecure-registry 192.168.10.139:4000</span><br></pre></td></tr></table></figure><p>4.启动docker</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> systemctl daemon-reload</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> systemctl restart docker</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> systemctl <span class="built_in">enable</span> docker</span></span><br><span class="line">Created symlink from /etc/systemd/system/multi-user.target.wants/docker.service to /usr/lib/systemd/system/docker.service.</span><br><span class="line"><span class="meta">#</span><span class="bash"> systemctl status docker</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> docker info</span></span><br></pre></td></tr></table></figure><h3 id="安装ansible"><a href="#安装ansible" class="headerlink" title="安装ansible"></a>安装ansible</h3><p>ansible版本必须在2.0以上</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> yum -y install ansible -y</span></span><br></pre></td></tr></table></figure><h3 id="搭建Registry"><a href="#搭建Registry" class="headerlink" title="搭建Registry"></a>搭建Registry</h3><p>默认docker的registry是使用5000端口，对于OpenStack来说，有端口冲突，所以我将端口改成了4000。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> docker run -d -v /opt/registry:/var/lib/registry -p 4000:5000 \</span></span><br><span class="line"><span class="bash">--restart=always --name registry registry:2</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> docker ps</span></span><br><span class="line">CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                    NAMES</span><br><span class="line">005883e5a115        registry:2          &quot;/entrypoint.sh /e...&quot;   24 seconds ago      Up 22 seconds       0.0.0.0:4000-&gt;5000/tcp   registry</span><br></pre></td></tr></table></figure><h3 id="下载kolla官方提供的Pile镜像"><a href="#下载kolla官方提供的Pile镜像" class="headerlink" title="下载kolla官方提供的Pile镜像"></a>下载kolla官方提供的Pile镜像</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> wget http://tarballs.openstack.org/kolla/images/centos-source-registry-pike.tar.gz</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> du -sh centos-source-registry-pike.tar.gz</span></span><br><span class="line">4.2Gcentos-source-registry-pike.tar.gz</span><br><span class="line"><span class="meta">#</span><span class="bash"> tar zxf centos-source-registry-pike.tar.gz -C /opt/registry/</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> curl http://192.168.10.139:4000/v2/_catalog</span></span><br></pre></td></tr></table></figure><h3 id="下载kolla-ansible，并配置"><a href="#下载kolla-ansible，并配置" class="headerlink" title="下载kolla-ansible，并配置"></a>下载kolla-ansible，并配置</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> git <span class="built_in">clone</span> https://github.com/openstack/kolla-ansible -b stable/pike</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">cd</span> kolla-ansible/</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> cp -r etc/kolla/ /etc/kolla/</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> pip install . -i https://pypi.tuna.tsinghua.edu.cn/simple</span></span><br></pre></td></tr></table></figure><h3 id="安装kolla"><a href="#安装kolla" class="headerlink" title="安装kolla"></a>安装kolla</h3><p>1.因为我是在虚拟机安装的kolla，希望可以启动虚拟机，那么需要把virt_type=qemu,默认是kvm</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> mkdir -p /etc/kolla/config/nova</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> cat &lt;&lt; <span class="string">EOF &gt; /etc/kolla/config/nova/nova-compute.conf</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> [libvirt]</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> virt_type=qemu</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> cpu_mode = none</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> EOF</span></span><br></pre></td></tr></table></figure><p>2.ceph打标签,下面两个命令分别用来快速删除 osd disk 的分区和快速给 disk 打 ceph 标签</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> lsblk</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> lsblk | grep <span class="string">&quot;^sd[^a]&quot;</span> | awk <span class="string">&#x27;&#123;print $1&#125;&#x27;</span> | <span class="keyword">while</span> <span class="built_in">read</span> line; <span class="keyword">do</span> \</span></span><br><span class="line"><span class="bash">parted /dev/<span class="variable">$line</span> -s -- mklabel gpt mkpart KOLLA_CEPH_OSD_BOOTSTRAP 1 -1; \</span></span><br><span class="line"><span class="bash"><span class="keyword">done</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> lsblk</span></span><br></pre></td></tr></table></figure><p>3.配置GLOBALS.YML文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> cat /etc/kolla/globals.yml |grep -Ev <span class="string">&#x27;^$|^#&#x27;</span></span></span><br><span class="line">---</span><br><span class="line">kolla_base_distro: &quot;centos&quot;</span><br><span class="line">kolla_install_type: &quot;source&quot;</span><br><span class="line">openstack_release: &quot;5.0.1&quot;</span><br><span class="line">kolla_internal_vip_address: &quot;192.168.10.139&quot;</span><br><span class="line">docker_registry: &quot;192.168.10.139:4000&quot;</span><br><span class="line">docker_namespace: &quot;lokolla&quot;</span><br><span class="line">network_interface: &quot;ens33&quot;</span><br><span class="line">neutron_external_interface: &quot;ens34&quot;</span><br><span class="line">keepalived_virtual_router_id: &quot;52&quot;</span><br><span class="line">enable_ceph: &quot;yes&quot;</span><br><span class="line">enable_chrony: &quot;yes&quot;</span><br><span class="line">enable_cinder: &quot;yes&quot;</span><br><span class="line">enable_haproxy: &quot;no&quot;</span><br><span class="line">enable_horizon: &quot;yes&quot;</span><br><span class="line">tempest_image_id:</span><br><span class="line">tempest_flavor_ref_id:</span><br><span class="line">tempest_public_network_id:</span><br><span class="line">tempest_floating_network_name:</span><br></pre></td></tr></table></figure><p>4.创建 /etc/kolla/config/ceph.conf</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[global]</span><br><span class="line">osd pool default size = 1</span><br><span class="line">osd pool default min size = 1</span><br></pre></td></tr></table></figure><h3 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h3><p>1.生成密码,编辑/etc/kolla/passwords.yml</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> kolla-genpwd</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> vim /etc/kolla/passwords.yml</span></span><br><span class="line">keystone_admin_password: lijiawang</span><br></pre></td></tr></table></figure><p>这是登录Dashboard，admin使用的密码，你可以根据自己需要进行修改<br>2.部署检查</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> kolla-ansible prechecks</span></span><br></pre></td></tr></table></figure><p>3.部署</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> kolla-ansible deploy</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> kolla-ansible post-deploy</span></span><br></pre></td></tr></table></figure><h3 id="安装openstack-client-端"><a href="#安装openstack-client-端" class="headerlink" title="安装openstack client 端"></a>安装openstack client 端</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> pip install python-openstackclient</span></span><br></pre></td></tr></table></figure><h3 id="创建网络"><a href="#创建网络" class="headerlink" title="创建网络"></a>创建网络</h3><p>1.编辑 /usr/share/kolla-ansible/init-runonce<br>网络需要根据实际情况修改,我都是用的nat模式</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">EXT_NET_CIDR=&#x27;192.168.10.0/24&#x27;</span><br><span class="line">EXT_NET_RANGE=&#x27;start=192.168.10.150,end=192.168.10.199&#x27;</span><br><span class="line">EXT_NET_GATEWAY=&#x27;192.168.10.2&#x27;</span><br></pre></td></tr></table></figure><p>2.运行脚本</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">source</span> /etc/kolla/admin-openrc.sh</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> bash /usr/share/kolla-ansible/init-runonce</span></span><br></pre></td></tr></table></figure><p>3.创建实例绑定浮动ip，测试网络</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@pike ~]# nova list</span><br><span class="line">+--------------------------------------+-------+--------+------------+-------------+------------------------------------+</span><br><span class="line">| ID                                   | Name  | Status | Task State | Power State | Networks                           |</span><br><span class="line">+--------------------------------------+-------+--------+------------+-------------+------------------------------------+</span><br><span class="line">| cbc40238-3bee-4df1-abc0-a10fc269e6e0 | demo1 | ACTIVE | -          | Running     | demo-net=10.0.0.10, 192.168.10.154 |</span><br><span class="line">+--------------------------------------+-------+--------+------------+-------------+------------------------------------+</span><br><span class="line">[root@pike ~]# ping 192.168.10.154</span><br><span class="line">PING 192.168.10.154 (192.168.10.154) 56(84) bytes of data.</span><br><span class="line">64 bytes from 192.168.10.154: icmp_seq=1 ttl=63 time=3.91 ms</span><br><span class="line">64 bytes from 192.168.10.154: icmp_seq=2 ttl=63 time=4.56 ms</span><br><span class="line">64 bytes from 192.168.10.154: icmp_seq=3 ttl=63 time=2.04 ms</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;环境准备&quot;&gt;&lt;a href=&quot;#环境准备&quot; class=&quot;headerlink&quot; title=&quot;环境准备&quot;&gt;&lt;/a&gt;环境准备&lt;/h3&gt;&lt;p&gt;我这里用workstation 创建了一个虚拟机，安装centos7.4系统，这台虚拟机上有两张网卡，一张做openstack管理网，一张做为虚拟机的业务网卡。&lt;/p&gt;</summary>
    
    
    
    <category term="openstack" scheme="https://www.lijiawang.org/categories/openstack/"/>
    
    
    <category term="openstack" scheme="https://www.lijiawang.org/tags/openstack/"/>
    
  </entry>
  
</feed>
